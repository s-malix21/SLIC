{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport numpy as np\nfrom torch.utils.data import DataLoader, Subset\nimport copy\nimport random\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport time\n\n# Reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\nNUM_CLASSES = 100\n\n# Hyperparameters\nBATCH_SIZE = 64\nLEARNING_RATE = 0.01\nLOCAL_EPOCHS = 5\nNUM_OF_CLIENTS = 10\nCOMM_ROUND = 50\nALPHA = 0.5\nFRAC = 0.1\nRHO = 0.05  # SAM perturbation radius (from paper)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"execution":{"iopub.status.busy":"2025-12-27T13:06:17.521626Z","iopub.execute_input":"2025-12-27T13:06:17.522338Z","iopub.status.idle":"2025-12-27T13:06:17.529443Z","shell.execute_reply.started":"2025-12-27T13:06:17.522306Z","shell.execute_reply":"2025-12-27T13:06:17.528669Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"class SimpleCNN(nn.Module):\n    def __init__(self, num_classes=NUM_CLASSES):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n        self.bn3 = nn.BatchNorm2d(128)\n        self.conv4 = nn.Conv2d(128, 128, 3, padding=1)\n        self.bn4 = nn.BatchNorm2d(128)\n        self.conv5 = nn.Conv2d(128, 256, 3, padding=1)\n        self.bn5 = nn.BatchNorm2d(256)\n        self.conv6 = nn.Conv2d(256, 256, 3, padding=1)\n        self.bn6 = nn.BatchNorm2d(256)\n        self.conv7 = nn.Conv2d(256, 512, 3, padding=1)\n        self.bn7 = nn.BatchNorm2d(512)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((2, 2))\n        self.fc1 = nn.Linear(512 * 2 * 2, 1024)\n        self.bn_fc1 = nn.BatchNorm1d(1024)\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(1024, 512)\n        self.bn_fc2 = nn.BatchNorm1d(512)\n        self.dropout2 = nn.Dropout(0.3)\n        self.fc3 = nn.Linear(512, num_classes)\n\n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = self.pool(x)\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = F.relu(self.bn4(self.conv4(x)))\n        x = self.pool(x)\n        x = F.relu(self.bn5(self.conv5(x)))\n        x = F.relu(self.bn6(self.conv6(x)))\n        x = self.pool(x)\n        x = F.relu(self.bn7(self.conv7(x)))\n        x = self.adaptive_pool(x)\n        x = x.view(x.size(0), -1)\n        f1 = F.relu(self.bn_fc1(self.fc1(x)))\n        f1 = self.dropout1(f1)\n        f2 = F.relu(self.bn_fc2(self.fc2(f1)))\n        f2 = self.dropout2(f2)\n        logits = self.fc3(f2)\n        return f2, logits  # features, logits\n\n\ndef load_and_partition_data(num_clients=NUM_OF_CLIENTS, alpha=ALPHA, batch_size=BATCH_SIZE, frac=FRAC, rand_seed=42):\n    \"\"\"\n    Load and partition CIFAR-100 dataset using Dirichlet distribution for non-IID split.\n    \n    Args:\n        num_clients: Number of federated clients\n        alpha: Dirichlet concentration parameter (lower = more non-IID)\n        batch_size: Batch size for data loaders\n        frac: Fraction of training data to use per client\n        rand_seed: Random seed for reproducibility\n    \n    Returns:\n        client_train_loaders: List of training DataLoaders for each client\n        client_val_loaders: List of validation DataLoaders for each client\n        test_loader: Test DataLoader\n        client_class_dist: List of class distributions for each client\n    \"\"\"\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n\n    torch.manual_seed(rand_seed)\n    np.random.seed(rand_seed)\n\n    # Load CIFAR-100 dataset\n    full_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n    test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n\n    y_train = np.array(full_dataset.targets)\n    y_test = np.array(test_dataset.targets)\n\n    net_dataidx_map = {}\n    net_dataidx_map_test = {}\n\n    # Partition data using Dirichlet distribution\n    min_size = 0\n    while min_size < 10:\n        idx_batch = [[] for _ in range(num_clients)]\n        idx_batch_test = [[] for _ in range(num_clients)]\n        \n        # For each of the 100 classes\n        for k in range(100):\n            idx_k = np.where(y_train == k)[0]\n            idx_k_test = np.where(y_test == k)[0]\n            np.random.shuffle(idx_k)\n            np.random.shuffle(idx_k_test)\n            \n            # Sample proportions from Dirichlet distribution\n            proportions = np.random.dirichlet(np.repeat(alpha, num_clients))\n            proportions_train = np.array([p * (len(idx_j) < len(full_dataset)/num_clients) for p, idx_j in zip(proportions, idx_batch)])\n            proportions_test = np.array([p * (len(idx_j) < len(test_dataset)/num_clients) for p, idx_j in zip(proportions, idx_batch_test)])\n            proportions_train /= proportions_train.sum()\n            proportions_test /= proportions_test.sum()\n            \n            # Split data according to proportions\n            split_train = (np.cumsum(proportions_train) * len(idx_k)).astype(int)[:-1]\n            split_test = (np.cumsum(proportions_test) * len(idx_k_test)).astype(int)[:-1]\n            idx_batch = [idx_j + idx.tolist() for idx_j, idx in zip(idx_batch, np.split(idx_k, split_train))]\n            idx_batch_test = [idx_j + idx.tolist() for idx_j, idx in zip(idx_batch_test, np.split(idx_k_test, split_test))]\n        \n        min_size = min(len(l) for l in idx_batch)\n\n    # Store indices for each client\n    for j in range(num_clients):\n        np.random.shuffle(idx_batch[j])\n        np.random.shuffle(idx_batch_test[j])\n        net_dataidx_map[j] = idx_batch[j]\n        net_dataidx_map_test[j] = idx_batch_test[j]\n\n    # Create data loaders for each client\n    client_train_loaders = []\n    client_val_loaders = []\n    client_class_dist = []\n\n    for i in range(num_clients):\n        np.random.seed(rand_seed + i)\n        \n        # Sample subset of data based on frac parameter\n        train_idx = np.random.choice(net_dataidx_map[i], int(frac * len(net_dataidx_map[i])), replace=False)\n        val_idx = np.random.choice(net_dataidx_map_test[i], int(min(2*frac, 1.0)*len(net_dataidx_map_test[i])), replace=False)\n\n        # Calculate class distribution for this client (FIXED: range(100) for CIFAR-100)\n        client_labels = [y_train[k] for k in train_idx]\n        dist = {c: client_labels.count(c)/len(client_labels) if client_labels else 0 for c in range(100)}\n        client_class_dist.append(dist)\n\n        # Create data loaders\n        train_loader = DataLoader(Subset(full_dataset, train_idx), batch_size=batch_size,\n                                  shuffle=True, generator=torch.Generator().manual_seed(rand_seed+i), drop_last=True)\n        val_loader = DataLoader(Subset(test_dataset, val_idx), batch_size=batch_size,\n                                shuffle=True, generator=torch.Generator().manual_seed(rand_seed+i+num_clients), drop_last=True)\n        client_train_loaders.append(train_loader)\n        client_val_loaders.append(val_loader)\n\n    # Create global test loader\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n                             generator=torch.Generator().manual_seed(rand_seed + 2*num_clients + 1))\n\n    # Print data partitioning statistics (FIXED: better printing for 100 classes)\n    print(\"Data partitioning complete.\")\n    print(\"=\"*80)\n    for i, d in enumerate(client_class_dist):\n        num_classes_present = len([c for c in range(100) if d.get(c, 0) > 0])\n        top_classes = sorted([(c, d[c]) for c in range(100) if d.get(c, 0) > 0], \n                            key=lambda x: x[1], reverse=True)[:5]\n        print(f\"Client {i}: {num_classes_present} classes present | \"\n              f\"Samples: {len(net_dataidx_map[i])} train, {len(net_dataidx_map_test[i])} test\")\n        print(f\"  Top 5 classes: {[(f'Class {c}', f'{prob:.3f}') for c, prob in top_classes]}\")\n    print(\"=\"*80)\n\n    return client_train_loaders, client_val_loaders, test_loader, client_class_dist\n\n# train_loaders, val_loaders, test_loader = load_and_partition_data()\n\ntrain_loaders, val_loaders, test_loader, client_class_dist = load_and_partition_data()\n\n# ========================================\n# 3. TEMPNET\n# ========================================\nclass TempNet(nn.Module):\n    def __init__(self, feature_dim=512, hidden_dim=128, tau_min=0.05, tau_max=2.0):\n        super().__init__()\n        self.fc1 = nn.Linear(feature_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, 1)\n        self.tau_min = tau_min\n        self.tau_max = tau_max\n\n    def forward(self, x):\n        h = F.relu(self.fc1(x))\n        raw = self.fc2(h)\n        tau = torch.sigmoid(raw)\n        tau = tau * (self.tau_max - self.tau_min) + self.tau_min\n        return tau.mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T13:06:20.048783Z","iopub.execute_input":"2025-12-27T13:06:20.049092Z","iopub.status.idle":"2025-12-27T13:06:39.331389Z","shell.execute_reply.started":"2025-12-27T13:06:20.049053Z","shell.execute_reply":"2025-12-27T13:06:39.330643Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 169M/169M [00:15<00:00, 11.2MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Data partitioning complete.\n================================================================================\nClient 0: 62 classes present | Samples: 5003 train, 1010 test\n  Top 5 classes: [('Class 41', '0.058'), ('Class 83', '0.052'), ('Class 34', '0.050'), ('Class 31', '0.044'), ('Class 63', '0.042')]\nClient 1: 78 classes present | Samples: 5190 train, 1019 test\n  Top 5 classes: [('Class 99', '0.087'), ('Class 89', '0.064'), ('Class 43', '0.042'), ('Class 15', '0.037'), ('Class 24', '0.035')]\nClient 2: 67 classes present | Samples: 5053 train, 1023 test\n  Top 5 classes: [('Class 94', '0.063'), ('Class 58', '0.051'), ('Class 76', '0.051'), ('Class 98', '0.051'), ('Class 21', '0.042')]\nClient 3: 72 classes present | Samples: 5002 train, 1003 test\n  Top 5 classes: [('Class 3', '0.058'), ('Class 95', '0.052'), ('Class 44', '0.042'), ('Class 54', '0.042'), ('Class 78', '0.042')]\nClient 4: 83 classes present | Samples: 4875 train, 961 test\n  Top 5 classes: [('Class 96', '0.057'), ('Class 4', '0.045'), ('Class 87', '0.045'), ('Class 85', '0.039'), ('Class 81', '0.037')]\nClient 5: 62 classes present | Samples: 5013 train, 1003 test\n  Top 5 classes: [('Class 9', '0.052'), ('Class 59', '0.048'), ('Class 61', '0.046'), ('Class 68', '0.044'), ('Class 13', '0.040')]\nClient 6: 62 classes present | Samples: 5022 train, 1002 test\n  Top 5 classes: [('Class 37', '0.070'), ('Class 64', '0.058'), ('Class 40', '0.056'), ('Class 59', '0.052'), ('Class 18', '0.046')]\nClient 7: 74 classes present | Samples: 4646 train, 948 test\n  Top 5 classes: [('Class 97', '0.078'), ('Class 20', '0.043'), ('Class 54', '0.039'), ('Class 25', '0.034'), ('Class 70', '0.034')]\nClient 8: 62 classes present | Samples: 5117 train, 1021 test\n  Top 5 classes: [('Class 23', '0.072'), ('Class 12', '0.061'), ('Class 25', '0.047'), ('Class 73', '0.047'), ('Class 62', '0.045')]\nClient 9: 74 classes present | Samples: 5079 train, 1010 test\n  Top 5 classes: [('Class 91', '0.069'), ('Class 69', '0.065'), ('Class 38', '0.045'), ('Class 13', '0.039'), ('Class 92', '0.032')]\n================================================================================\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"class BaseClient:\n    \"\"\"Base class for federated learning clients\"\"\"\n    \n    def __init__(self, client_id, model, train_loader, val_loader, device, \n                 learning_rate=0.01, local_epochs=5, momentum=0.9):\n        self.client_id = client_id\n        self.model = model\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.device = device\n        self.learning_rate = learning_rate\n        self.local_epochs = local_epochs\n        self.momentum = momentum\n        self.loss_fn = nn.CrossEntropyLoss()\n        self.train_samples = 0\n    \n    def train(self):\n        raise NotImplementedError\n    \n    def set_parameters(self, model_state_dict):\n        \"\"\"Load model parameters from server\"\"\"\n        self.model.load_state_dict(model_state_dict)\n    \n    def get_parameters(self):\n        \"\"\"Get model parameters\"\"\"\n        return self.model.state_dict()\n    \n    def get_train_samples(self):\n        \"\"\"Get number of training samples\"\"\"\n        try:\n            return len(self.train_loader.dataset)\n        except:\n            return len(self.train_loader) * BATCH_SIZE\n\n\nclass BaseServer:\n    \"\"\"Base class for federated learning server\"\"\"\n    \n    def __init__(self, global_model, test_loader, num_clients, learning_rate=0.01, \n                 lr_decay=0.998, device='cuda'):\n        self.global_model = copy.deepcopy(global_model)\n        self.test_loader = test_loader\n        self.num_clients = num_clients\n        self.learning_rate = learning_rate\n        self.lr_decay = lr_decay\n        self.device = device\n        self.clients = []\n        self.loss_fn = nn.CrossEntropyLoss()\n    \n    def send_models(self, clients):\n        \"\"\"Send global model to clients\"\"\"\n        model_state = self.global_model.state_dict()\n        for client in clients:\n            client.set_parameters(model_state)\n    \n    def receive_models(self, clients):\n        \"\"\"Receive trained models from clients and aggregate\"\"\"\n        self.client_models = [client.get_parameters() for client in clients]\n        self.client_samples = [client.get_train_samples() for client in clients]\n    \n    def aggregate_parameters(self):\n        \"\"\"Aggregate client models using weighted averaging\n        \n        IMPORTANT: This method handles mixed dtypes in model state dict.\n        Batch normalization layers have both float parameters AND integer buffers \n        (like num_batches_tracked). We must convert to float for weighted averaging,\n        then convert back to original dtype to avoid RuntimeError.\n        \"\"\"\n        total_samples = sum(self.client_samples)\n        avg_state = {}\n        \n        # Get first model as reference\n        for key in self.client_models[0].keys():\n            # Get original dtype (may be float32, float64, or int64)\n            first_tensor = self.client_models[0][key]\n            original_dtype = first_tensor.dtype\n            \n            # Initialize accumulator with zeros, using float32 for safe averaging\n            avg_state[key] = torch.zeros_like(first_tensor, dtype=torch.float32)\n            \n            # Weighted average: convert each tensor to float before summing\n            for model_state, num_samples in zip(self.client_models, self.client_samples):\n                weight = num_samples / total_samples\n                # .float() converts int64 buffers to float32, allows weighted addition\n                avg_state[key] += weight * model_state[key].float()\n            \n            # Convert result back to original dtype\n            # Float params stay float, int buffers become int again\n            avg_state[key] = avg_state[key].to(original_dtype)\n        \n        self.global_model.load_state_dict(avg_state)\n    \n    def evaluate(self):\n        \"\"\"Evaluate global model on test data\"\"\"\n        self.global_model.eval()\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for X, y in self.test_loader:\n                X, y = X.to(self.device), y.to(self.device)\n                output = self.model_forward(X)\n                if isinstance(output, tuple):\n                    _, logits = output\n                else:\n                    logits = output\n                _, predicted = torch.max(logits, 1)\n                total += y.size(0)\n                correct += (predicted == y).sum().item()\n        \n        accuracy = correct / total\n        return accuracy\n    \n    def model_forward(self, X):\n        \"\"\"Forward pass through model\"\"\"\n        return self.global_model(X)\n\n\n# ========================================\n# FEDAVG IMPLEMENTATION\n# ========================================\n\nclass ClientFedAvg(BaseClient):\n    \"\"\"Standard FedAvg client using SGD\"\"\"\n    \n    def train(self):\n        self.model.train()\n        optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate)\n        \n        for epoch in range(self.local_epochs):\n            for X, y in self.train_loader:\n                X, y = X.to(self.device), y.to(self.device)\n                \n                optimizer.zero_grad()\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    _, logits = output\n                else:\n                    logits = output\n                loss = self.loss_fn(logits, y)\n                loss.backward()\n                optimizer.step()\n\n\nclass FedAvgServer(BaseServer):\n    \"\"\"Standard FedAvg server\"\"\"\n    \n    def __init__(self, global_model, test_loader, num_clients, client_loaders, \n                 learning_rate=0.01, lr_decay=0.998, device='cuda', local_epochs=5):\n        super().__init__(global_model, test_loader, num_clients, learning_rate, \n                        lr_decay, device)\n        self.local_epochs = local_epochs\n        \n        for i, (train_loader, val_loader) in enumerate(client_loaders):\n            client = ClientFedAvg(i, copy.deepcopy(global_model), train_loader, \n                                 val_loader, device, learning_rate=learning_rate, \n                                 local_epochs=local_epochs)\n            client.train_samples = client.get_train_samples()\n            self.clients.append(client)\n    \n    def train_round(self):\n        \"\"\"Execute one communication round\"\"\"\n        self.send_models(self.clients)\n        \n        for client in self.clients:\n            client.train()\n        \n        self.receive_models(self.clients)\n        self.aggregate_parameters()\n        \n        acc = self.evaluate()\n        self.learning_rate *= self.lr_decay\n        \n        return acc\n\n\n# ========================================\n# FEDAVG + TEMPNET IMPLEMENTATION\n# ========================================\n\nclass ClientFedAvgTemp(BaseClient):\n    \"\"\"FedAvg + Temperature: Standard SGD with learned temperature scaling\"\"\"\n    \n    def __init__(self, client_id, model, train_loader, val_loader, device, \n                 learning_rate=0.01, local_epochs=5, momentum=0.9, train_samples=None):\n        super().__init__(client_id, model, train_loader, val_loader, device, \n                        learning_rate, local_epochs, momentum)\n        self.train_samples = train_samples or 0\n        self.tempnet = TempNet(feature_dim=512, hidden_dim=128).to(device)\n        self.temp_optimizer = optim.SGD(self.tempnet.parameters(), \n                                       lr=learning_rate)\n    \n    def train(self):\n        self.model.train()\n        self.tempnet.train()\n        optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate)\n        \n        for epoch in range(self.local_epochs):\n            for X, y in self.train_loader:\n                X, y = X.to(self.device), y.to(self.device)\n                \n                optimizer.zero_grad()\n                self.temp_optimizer.zero_grad()\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    features, logits = output\n                else:\n                    features = None\n                    logits = output\n                \n                tau = self.tempnet(features.detach()) if features is not None else torch.tensor(1.0, device=self.device)\n                scaled_logits = logits / tau\n                loss = self.loss_fn(scaled_logits, y)\n                loss.backward()\n                \n                optimizer.step()\n                self.temp_optimizer.step()\n    \n    def get_temperature(self):\n        \"\"\"Get current temperature value\"\"\"\n        self.model.eval()\n        self.tempnet.eval()\n        try:\n            with torch.no_grad():\n                X, _ = next(iter(self.train_loader))\n                X = X.to(self.device)\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    features, _ = output\n                    tau = self.tempnet(features)\n                    return tau.item()\n                else:\n                    return 1.0\n        except:\n            return 1.0\n\n\nclass FedAvgTempServer(BaseServer):\n    \"\"\"FedAvg + Temperature Server\"\"\"\n    \n    def __init__(self, global_model, test_loader, num_clients, client_loaders, \n                 learning_rate=0.01, lr_decay=0.998, device='cuda', local_epochs=5):\n        super().__init__(global_model, test_loader, num_clients, learning_rate, \n                        lr_decay, device)\n        self.local_epochs = local_epochs\n        self.tau_history = []\n        \n        for i, (train_loader, val_loader) in enumerate(client_loaders):\n            client = ClientFedAvgTemp(i, copy.deepcopy(global_model), train_loader, \n                                     val_loader, device, learning_rate=learning_rate, \n                                     local_epochs=local_epochs)\n            client.train_samples = client.get_train_samples()\n            self.clients.append(client)\n    \n    def train_round(self):\n        self.send_models(self.clients)\n        \n        local_taus = []\n        for client in self.clients:\n            client.train()\n            tau = client.get_temperature()\n            local_taus.append(tau)\n        \n        self.receive_models(self.clients)\n        self.aggregate_parameters()\n        \n        acc = self.evaluate()\n        avg_tau = sum(local_taus) / len(local_taus) if local_taus else 1.0\n        self.tau_history.append(avg_tau)\n        \n        self.learning_rate *= self.lr_decay\n        \n        return acc, avg_tau\n\n# ========================================\n# SAM OPTIMIZER (Sharpness Aware Minimization)\n# ========================================\n\nclass SAMOptimizer(optim.Optimizer):\n    \"\"\"SAM optimizer - Sharpness Aware Minimization\"\"\"\n    \n    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n        assert rho >= 0.0, f\"Invalid rho: {rho}\"\n        defaults = dict(rho=rho, adaptive=adaptive)\n        super().__init__(params, defaults)\n        \n        self.base_optimizer = base_optimizer\n        self.param_groups = self.base_optimizer.param_groups\n        for group in self.param_groups:\n            group[\"rho\"] = rho\n            group[\"adaptive\"] = adaptive\n    \n    @torch.no_grad()\n    def first_step(self):\n        \"\"\"Perturbation step: climb to local maximum\"\"\"\n        grad_norm = self._grad_norm()\n        \n        for group in self.param_groups:\n            scale = group[\"rho\"] / (grad_norm + 1e-7)\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n                p.add_(e_w)  # Move to perturbed point θ + ε\n                self.state[p][\"e_w\"] = e_w\n    \n    @torch.no_grad()\n    def second_step(self):\n        \"\"\"Restore to original point (do this BEFORE applying gradients)\"\"\"\n        for group in self.param_groups:\n            for p in group[\"params\"]:\n                if \"e_w\" in self.state[p]:\n                    p.sub_(self.state[p][\"e_w\"])  # Return to original θ\n                    self.state[p][\"e_w\"] = 0\n    \n    def _grad_norm(self):\n        \"\"\"Compute gradient norm across all parameters\"\"\"\n        norm = torch.norm(torch.stack([\n            ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(p=2)\n            for group in self.param_groups for p in group[\"params\"]\n            if p.grad is not None\n        ]), p=2)\n        return norm\n\n\n# ========================================\n# FEDSAM IMPLEMENTATION\n# ========================================\n\nclass ClientFedSAM(BaseClient):\n    \"\"\"FedSAM client using SAM optimizer\"\"\"\n    \n    def __init__(self, client_id, model, train_loader, val_loader, device, \n                 learning_rate=0.01, local_epochs=5, momentum=0.9, rho=0.05, \n                 train_samples=None):\n        super().__init__(client_id, model, train_loader, val_loader, device, \n                        learning_rate, local_epochs, momentum)\n        self.rho = rho\n        self.train_samples = train_samples or 0\n    \n    def train(self):\n        self.model.train()\n        base_optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate)\n        sam_optimizer = SAMOptimizer(self.model.parameters(), base_optimizer, rho=self.rho)\n        \n        for epoch in range(self.local_epochs):\n            for X, y in self.train_loader:\n                X, y = X.to(self.device), y.to(self.device)\n                \n                # First forward-backward: compute gradient at θ\n                base_optimizer.zero_grad()\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    _, logits = output\n                else:\n                    logits = output\n                loss = self.loss_fn(logits, y)\n                loss.backward()\n                \n                # SAM first step: ascent to θ + ε\n                sam_optimizer.first_step()\n                \n                # Second forward-backward: compute gradient at perturbed point θ + ε\n                base_optimizer.zero_grad()\n                output_pert = self.model(X)\n                if isinstance(output_pert, tuple):\n                    _, logits_pert = output_pert\n                else:\n                    logits_pert = output_pert\n                loss_pert = self.loss_fn(logits_pert, y)\n                loss_pert.backward()\n                \n                # SAM second step: restore to original θ (BEFORE gradient descent)\n                sam_optimizer.second_step()\n                \n                # Apply gradient from perturbed point to original θ\n                base_optimizer.step()\n\n\nclass FedSAMServer(BaseServer):\n    \"\"\"FedSAM server\"\"\"\n    \n    def __init__(self, global_model, test_loader, num_clients, client_loaders, \n                 learning_rate=0.01, lr_decay=0.998, device='cuda', local_epochs=5, \n                 rho=0.05):\n        super().__init__(global_model, test_loader, num_clients, learning_rate, \n                        lr_decay, device)\n        self.local_epochs = local_epochs\n        self.rho = rho\n        \n        for i, (train_loader, val_loader) in enumerate(client_loaders):\n            client = ClientFedSAM(i, copy.deepcopy(global_model), train_loader, \n                                 val_loader, device, learning_rate=learning_rate, \n                                 local_epochs=local_epochs, rho=rho)\n            client.train_samples = client.get_train_samples()\n            self.clients.append(client)\n    \n    def train_round(self):\n        self.send_models(self.clients)\n        \n        for client in self.clients:\n            client.train()\n        \n        self.receive_models(self.clients)\n        self.aggregate_parameters()\n        \n        acc = self.evaluate()\n        self.learning_rate *= self.lr_decay\n        \n        return acc\n\n\n# ========================================\n# FEDSAM + TEMPNET IMPLEMENTATION\n# ========================================\n\nclass ClientFedSAMTemp(BaseClient):\n    \"\"\"FedSAM + Temperature: SAM with learned temperature scaling\"\"\"\n    \n    def __init__(self, client_id, model, train_loader, val_loader, device, \n                 learning_rate=0.01, local_epochs=5, momentum=0.9, rho=0.05, \n                 train_samples=None):\n        super().__init__(client_id, model, train_loader, val_loader, device, \n                        learning_rate, local_epochs, momentum)\n        self.rho = rho\n        self.train_samples = train_samples or 0\n        self.tempnet = TempNet(feature_dim=512, hidden_dim=128).to(device)\n        self.temp_optimizer = optim.SGD(self.tempnet.parameters(), \n                                       lr=learning_rate)\n    \n    def train(self):\n        self.model.train()\n        self.tempnet.train()\n        base_optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate)\n        sam_optimizer = SAMOptimizer(self.model.parameters(), base_optimizer, rho=self.rho)\n        \n        for epoch in range(self.local_epochs):\n            for X, y in self.train_loader:\n                X, y = X.to(self.device), y.to(self.device)\n                \n                # First forward-backward: compute gradient at θ with temperature\n                base_optimizer.zero_grad()\n                self.temp_optimizer.zero_grad()\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    features, logits = output\n                else:\n                    features = None\n                    logits = output\n                \n                tau = self.tempnet(features.detach()) if features is not None else torch.tensor(1.0, device=self.device)\n                scaled_logits = logits / tau\n                loss = self.loss_fn(scaled_logits, y)\n                loss.backward()\n                \n                # SAM first step: ascent to θ + ε (model only)\n                sam_optimizer.first_step()\n                \n                # Second forward-backward: at perturbed point θ + ε\n                base_optimizer.zero_grad()\n                self.temp_optimizer.zero_grad()\n                output_pert = self.model(X)\n                if isinstance(output_pert, tuple):\n                    features_pert, logits_pert = output_pert\n                else:\n                    features_pert = None\n                    logits_pert = output_pert\n                \n                tau_pert = self.tempnet(features_pert.detach()) if features_pert is not None else torch.tensor(1.0, device=self.device)\n                scaled_logits_pert = logits_pert / tau_pert\n                loss_pert = self.loss_fn(scaled_logits_pert, y)\n                loss_pert.backward()\n                \n                # SAM second step: restore model to original θ (BEFORE gradient descent)\n                sam_optimizer.second_step()\n                \n                # Apply gradients: model gets gradient from perturbed point, tempnet standard update\n                base_optimizer.step()\n                self.temp_optimizer.step()\n    \n    def get_temperature(self):\n        \"\"\"Get current temperature value\"\"\"\n        self.model.eval()\n        self.tempnet.eval()\n        try:\n            with torch.no_grad():\n                X, _ = next(iter(self.train_loader))\n                X = X.to(self.device)\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    features, _ = output\n                    tau = self.tempnet(features)\n                    return tau.item()\n                else:\n                    return 1.0\n        except:\n            return 1.0\n\n\nclass FedSAMTempServer(BaseServer):\n    \"\"\"FedSAM + Temperature Server\"\"\"\n    \n    def __init__(self, global_model, test_loader, num_clients, client_loaders, \n                 learning_rate=0.01, lr_decay=0.998, device='cuda', local_epochs=5, \n                 rho=0.05):\n        super().__init__(global_model, test_loader, num_clients, learning_rate, \n                        lr_decay, device)\n        self.local_epochs = local_epochs\n        self.rho = rho\n        self.tau_history = []\n        \n        for i, (train_loader, val_loader) in enumerate(client_loaders):\n            client = ClientFedSAMTemp(i, copy.deepcopy(global_model), train_loader, \n                                     val_loader, device, learning_rate=learning_rate, \n                                     local_epochs=local_epochs, rho=rho)\n            client.train_samples = client.get_train_samples()\n            self.clients.append(client)\n    \n    def train_round(self):\n        self.send_models(self.clients)\n        \n        local_taus = []\n        for client in self.clients:\n            client.train()\n            tau = client.get_temperature()\n            local_taus.append(tau)\n        \n        self.receive_models(self.clients)\n        self.aggregate_parameters()\n        \n        acc = self.evaluate()\n        avg_tau = sum(local_taus) / len(local_taus) if local_taus else 1.0\n        self.tau_history.append(avg_tau)\n        \n        self.learning_rate *= self.lr_decay\n        \n        return acc, avg_tau\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T13:06:42.314799Z","iopub.execute_input":"2025-12-27T13:06:42.315149Z","iopub.status.idle":"2025-12-27T13:06:42.369791Z","shell.execute_reply.started":"2025-12-27T13:06:42.315117Z","shell.execute_reply":"2025-12-27T13:06:42.368901Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# ========================================\n# FEDLESAM IMPLEMENTATION\n# ========================================\nclass LESAMOptimizer(optim.Optimizer):\n    \n    def __init__(self, params, base_optimizer, rho=0.05, **kwargs):\n        assert rho >= 0.0, f\"Invalid rho: {rho}\"\n        defaults = dict(rho=rho)\n        super().__init__(params, defaults)\n        \n        self.base_optimizer = base_optimizer\n        self.param_groups = self.base_optimizer.param_groups\n        for group in self.param_groups:\n            group[\"rho\"] = rho\n    \n    @torch.no_grad()\n    def first_step(self, g_update=None):\n        \"\"\"Perturbation step using gradient update\"\"\"\n        if g_update is None:\n            g_update = [p.grad.clone() if p.grad is not None else None \n                       for p in self.param_groups[0][\"params\"]]\n        \n        grad_norm = torch.norm(torch.stack([\n            g.norm(p=2) for g in g_update if g is not None\n        ]), p=2)\n        \n        for group in self.param_groups:\n            scale = group[\"rho\"] / (grad_norm + 1e-7)\n            for idx, p in enumerate(group[\"params\"]):\n                if g_update[idx] is None:\n                    continue\n                e_w = g_update[idx] * scale.to(p)\n                p.add_(e_w)\n                self.state[p][\"e_w\"] = e_w\n    \n    @torch.no_grad()\n    def second_step(self):\n        \"\"\"Restore step\"\"\"\n        for group in self.param_groups:\n            for p in group[\"params\"]:\n                if p.grad is None or not self.state[p]:\n                    continue\n                p.sub_(self.state[p][\"e_w\"])\n                self.state[p][\"e_w\"] = 0\n\n\nclass ClientFedLESAM(BaseClient):\n    \"\"\"FedLESAM Client - SAM with momentum\"\"\"\n    \n    def __init__(self, client_id, model, train_loader, val_loader, device, \n                 learning_rate=0.01, local_epochs=5, momentum=0.9, rho=0.05, \n                 train_samples=None):\n        super().__init__(client_id, model, train_loader, val_loader, device, \n                        learning_rate, local_epochs, momentum)\n        self.rho = rho\n        self.train_samples = train_samples or 0\n        self.grad_momentum = None\n    \n    def train(self):\n        self.model.train()\n        base_optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate)\n        lesam_optimizer = LESAMOptimizer(self.model.parameters(), base_optimizer, rho=self.rho)\n        \n        for epoch in range(self.local_epochs):\n            for X, y in self.train_loader:\n                X, y = X.to(self.device), y.to(self.device)\n                \n                # First forward-backward: compute gradient\n                base_optimizer.zero_grad()\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    _, logits = output\n                else:\n                    logits = output\n                loss = self.loss_fn(logits, y)\n                loss.backward()\n                \n                # Initialize gradient momentum\n                if self.grad_momentum is None:\n                    self.grad_momentum = [p.grad.clone() if p.grad is not None else None \n                                         for p in self.model.parameters()]\n                \n                # Apply momentum to gradient\n                for i, p in enumerate(self.model.parameters()):\n                    if p.grad is not None and self.grad_momentum[i] is not None:\n                        p.grad.data = p.grad.data + self.momentum * self.grad_momentum[i]\n                        self.grad_momentum[i] = p.grad.clone()\n                \n                # LESAM first step using momentum-updated gradient\n                lesam_optimizer.first_step(self.grad_momentum)\n                \n                # Second forward-backward: at perturbed point\n                base_optimizer.zero_grad()\n                output_pert = self.model(X)\n                if isinstance(output_pert, tuple):\n                    _, logits_pert = output_pert\n                else:\n                    logits_pert = output_pert\n                loss_pert = self.loss_fn(logits_pert, y)\n                loss_pert.backward()\n                \n                # LESAM second step: descent\n                lesam_optimizer.second_step()\n                \n                # SGD step\n                base_optimizer.step()\n\n\nclass FedLESAMServer(BaseServer):\n    \"\"\"FedLESAM Server\"\"\"\n    \n    def __init__(self, global_model, test_loader, num_clients, client_loaders, \n                 learning_rate=0.01, lr_decay=0.998, device='cuda', local_epochs=5, \n                 rho=0.05):\n        super().__init__(global_model, test_loader, num_clients, learning_rate, \n                        lr_decay, device)\n        self.local_epochs = local_epochs\n        self.rho = rho\n        \n        for i, (train_loader, val_loader) in enumerate(client_loaders):\n            client = ClientFedLESAM(i, copy.deepcopy(global_model), train_loader, \n                                   val_loader, device, learning_rate=learning_rate, \n                                   local_epochs=local_epochs, rho=rho)\n            client.train_samples = client.get_train_samples()\n            self.clients.append(client)\n    \n    def train_round(self):\n        self.send_models(self.clients)\n        \n        for client in self.clients:\n            client.train()\n        \n        self.receive_models(self.clients)\n        self.aggregate_parameters()\n        \n        acc = self.evaluate()\n        self.learning_rate *= self.lr_decay\n        \n        return acc\n\n\nclass ClientFedLESAMTemp(BaseClient):\n    \"\"\"FedLESAM + Temperature Client\"\"\"\n    \n    def __init__(self, client_id, model, train_loader, val_loader, device, \n                 learning_rate=0.01, local_epochs=5, momentum=0.9, rho=0.05, \n                 train_samples=None):\n        super().__init__(client_id, model, train_loader, val_loader, device, \n                        learning_rate, local_epochs, momentum)\n        self.rho = rho\n        self.train_samples = train_samples or 0\n        self.tempnet = TempNet(feature_dim=512, hidden_dim=128).to(device)\n        self.temp_optimizer = optim.SGD(self.tempnet.parameters(), \n                                       lr=learning_rate)\n        self.grad_momentum = None\n    \n    def train(self):\n        self.model.train()\n        self.tempnet.train()\n        base_optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate)\n        lesam_optimizer = LESAMOptimizer(self.model.parameters(), base_optimizer, rho=self.rho)\n        \n        for epoch in range(self.local_epochs):\n            for X, y in self.train_loader:\n                X, y = X.to(self.device), y.to(self.device)\n                \n                # First forward-backward\n                base_optimizer.zero_grad()\n                self.temp_optimizer.zero_grad()\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    features, logits = output\n                else:\n                    features = None\n                    logits = output\n                \n                tau = self.tempnet(features.detach()) if features is not None else torch.tensor(1.0, device=self.device)\n                scaled_logits = logits / tau\n                loss = self.loss_fn(scaled_logits, y)\n                loss.backward()\n                \n                # Initialize gradient momentum\n                if self.grad_momentum is None:\n                    self.grad_momentum = [p.grad.clone() if p.grad is not None else None \n                                         for p in self.model.parameters()]\n                \n                # Apply momentum\n                for i, p in enumerate(self.model.parameters()):\n                    if p.grad is not None and self.grad_momentum[i] is not None:\n                        p.grad.data = p.grad.data + self.momentum * self.grad_momentum[i]\n                        self.grad_momentum[i] = p.grad.clone()\n                \n                # LESAM first step\n                lesam_optimizer.first_step(self.grad_momentum)\n                \n                # Second forward-backward at perturbed point\n                base_optimizer.zero_grad()\n                self.temp_optimizer.zero_grad()\n                output_pert = self.model(X)\n                if isinstance(output_pert, tuple):\n                    features_pert, logits_pert = output_pert\n                else:\n                    features_pert = None\n                    logits_pert = output_pert\n                \n                tau_pert = self.tempnet(features_pert.detach()) if features_pert is not None else torch.tensor(1.0, device=self.device)\n                scaled_logits_pert = logits_pert / tau_pert\n                loss_pert = self.loss_fn(scaled_logits_pert, y)\n                loss_pert.backward()\n                \n                # LESAM second step\n                lesam_optimizer.second_step()\n                \n                # SGD step\n                base_optimizer.step()\n                self.temp_optimizer.step()\n    \n    def get_temperature(self):\n        \"\"\"Get current temperature value\"\"\"\n        self.model.eval()\n        self.tempnet.eval()\n        try:\n            with torch.no_grad():\n                X, _ = next(iter(self.train_loader))\n                X = X.to(self.device)\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    features, _ = output\n                    tau = self.tempnet(features)\n                    return tau.item()\n                else:\n                    return 1.0\n        except:\n            return 1.0\n\n\nclass FedLESAMTempServer(BaseServer):\n    \"\"\"FedLESAM + Temperature Server\"\"\"\n    \n    def __init__(self, global_model, test_loader, num_clients, client_loaders, \n                 learning_rate=0.01, lr_decay=0.998, device='cuda', local_epochs=5, \n                 rho=0.05):\n        super().__init__(global_model, test_loader, num_clients, learning_rate, \n                        lr_decay, device)\n        self.local_epochs = local_epochs\n        self.rho = rho\n        self.tau_history = []\n        \n        for i, (train_loader, val_loader) in enumerate(client_loaders):\n            client = ClientFedLESAMTemp(i, copy.deepcopy(global_model), train_loader, \n                                       val_loader, device, learning_rate=learning_rate, \n                                       local_epochs=local_epochs, rho=rho)\n            client.train_samples = client.get_train_samples()\n            self.clients.append(client)\n    \n    def train_round(self):\n        self.send_models(self.clients)\n        \n        local_taus = []\n        for client in self.clients:\n            client.train()\n            tau = client.get_temperature()\n            local_taus.append(tau)\n        \n        self.receive_models(self.clients)\n        self.aggregate_parameters()\n        \n        acc = self.evaluate()\n        avg_tau = sum(local_taus) / len(local_taus) if local_taus else 1.0\n        self.tau_history.append(avg_tau)\n        \n        self.learning_rate *= self.lr_decay\n        \n        return acc, avg_tau\n\n\n# ========================================\n# FEDSMOO IMPLEMENTATION\n# ========================================\n\nclass SMOOOptimizer:\n    \"\"\"Optimizer for FedSMOO - combines SAM-like perturbations with smoothness regularization\"\"\"\n    \n    def __init__(self, params, base_optimizer, rho=0.05):\n        self.params = list(params)\n        self.base_optimizer = base_optimizer\n        self.rho = rho\n        self.state = {}\n        \n    def zero_grad(self):\n        self.base_optimizer.zero_grad()\n    \n    @torch.no_grad()\n    def first_step(self):\n        \"\"\"Perturb parameters in gradient direction (SAM first step)\"\"\"\n        grad_norm = self._grad_norm()\n        scale = self.rho / (grad_norm + 1e-7)\n        \n        perturbations = []\n        for p in self.params:\n            if p.grad is None:\n                continue\n            # Compute perturbation\n            e_w = p.grad * scale\n            # Store perturbation\n            self.state[p] = {\"e_w\": e_w.clone()}\n            # Apply perturbation\n            p.add_(e_w)\n            perturbations.append(e_w.reshape(-1))\n        \n        return torch.cat(perturbations) if perturbations else torch.zeros(1, device=self.params[0].device)\n    \n    @torch.no_grad()\n    def second_step(self):\n        \"\"\"Remove perturbation (SAM second step)\"\"\"\n        for p in self.params:\n            if p in self.state and \"e_w\" in self.state[p]:\n                # Remove perturbation\n                p.sub_(self.state[p][\"e_w\"])\n    \n    def _grad_norm(self):\n        \"\"\"Compute gradient norm\"\"\"\n        shared_device = self.params[0].device\n        norm = torch.norm(\n            torch.stack([\n                p.grad.norm(p=2).to(shared_device) for p in self.params \n                if p.grad is not None\n            ]),\n            p=2\n        )\n        return norm\n\n\ndef param_to_vector(model):\n    \"\"\"Convert model parameters to a single vector\"\"\"\n    vec = []\n    for param in model.parameters():\n        vec.append(param.data.reshape(-1))\n    return torch.cat(vec)\n\n\ndef vector_to_param(vector, model):\n    \"\"\"Assign vector values back to model parameters\"\"\"\n    index = 0\n    for param in model.parameters():\n        param_size = param.numel()\n        param.data.copy_(vector[index:index + param_size].view(param.shape))\n        index += param_size\n\n\nclass ClientFedSMOO(BaseClient):\n    \"\"\"FedSMOO Client - Smoothness Optimized Federated Learning\"\"\"\n    \n    def __init__(self, client_id, model, train_loader, val_loader, device, \n                 learning_rate=0.01, local_epochs=5, momentum=0.9, rho=0.05, \n                 beta=0.1, train_samples=None):\n        super().__init__(client_id, model, train_loader, val_loader, device, \n                        learning_rate, local_epochs, momentum)\n        self.rho = rho\n        self.beta = beta\n        self.train_samples = train_samples or 0\n        \n        # FedSMOO specific variables\n        self.dual_variable = None\n        self.global_s = None\n        self.mu_i = None\n        self.local_update = None\n        self.local_s_i = None\n    \n    def initialize_variables(self):\n        \"\"\"Initialize dual variables and momentum terms\"\"\"\n        with torch.no_grad():\n            param_vector = param_to_vector(self.model)\n            if self.mu_i is None:\n                self.mu_i = torch.zeros_like(param_vector, device=self.device)\n            if self.dual_variable is None:\n                self.dual_variable = torch.zeros_like(param_vector, device=self.device)\n            if self.global_s is None:\n                self.global_s = torch.zeros_like(param_vector, device=self.device)\n    \n    def train(self):\n        self.model.train()\n        self.initialize_variables()\n        \n        # Create optimizers\n        base_optimizer = optim.SGD(\n            self.model.parameters(), \n            lr=self.learning_rate, \n        )\n        smoo_optimizer = SMOOOptimizer(\n            self.model.parameters(), \n            base_optimizer, \n            rho=self.rho\n        )\n        \n        # Store initial parameters for dual variable update\n        with torch.no_grad():\n            initial_params = param_to_vector(self.model).clone()\n        \n        s_i_k_last = None\n        \n        for epoch in range(self.local_epochs):\n            for batch_idx, (X, y) in enumerate(self.train_loader):\n                X, y = X.to(self.device), y.to(self.device)\n                \n                # === Part 1: Standard gradient computation ===\n                base_optimizer.zero_grad()\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    _, logits = output\n                else:\n                    logits = output\n                loss = self.loss_fn(logits, y)\n                loss.backward()\n                \n                # Store original gradients before SAM\n                original_grads = []\n                for p in self.model.parameters():\n                    if p.grad is not None:\n                        original_grads.append(p.grad.clone())\n                \n                # === Part 2: Apply smoothness adjustment to gradients ===\n                with torch.no_grad():\n                    idx = 0\n                    for p in self.model.parameters():\n                        if p.grad is not None:\n                            numel = p.grad.numel()\n                            # Subtract smoothness terms from gradient\n                            grad_flat = p.grad.reshape(-1)\n                            adjustment = self.mu_i[idx:idx+numel] + self.global_s[idx:idx+numel]\n                            grad_flat -= adjustment\n                            idx += numel\n                \n                # === Part 3: SAM first step (perturb in gradient direction) ===\n                s_i_k = smoo_optimizer.first_step()\n                \n                # === Part 4: Compute gradient at perturbed point ===\n                base_optimizer.zero_grad()\n                output_pert = self.model(X)\n                if isinstance(output_pert, tuple):\n                    _, logits_pert = output_pert\n                else:\n                    logits_pert = output_pert\n                loss_pert = self.loss_fn(logits_pert, y)\n                loss_pert.backward()\n                \n                # === Part 5: SAM second step (return to original point) ===\n                smoo_optimizer.second_step()\n                \n                # === Part 6: Update momentum term ===\n                with torch.no_grad():\n                    self.mu_i += (s_i_k - self.global_s)\n                \n                # === Part 7: Apply optimizer step with proximal term ===\n                # Add proximal regularization gradient\n                with torch.no_grad():\n                    current_params = param_to_vector(self.model)\n                    # Proximal gradient: beta * (current_params - initial_params - dual_variable)\n                    prox_grad = self.beta * (current_params - initial_params + self.dual_variable)\n                    \n                    idx = 0\n                    for p in self.model.parameters():\n                        if p.grad is not None:\n                            numel = p.grad.numel()\n                            # Add proximal gradient term\n                            p.grad += prox_grad[idx:idx+numel].view(p.grad.shape)\n                            idx += numel\n                \n                # Apply optimizer step\n                base_optimizer.step()\n                \n                s_i_k_last = s_i_k\n        \n        # Store local update for server aggregation\n        with torch.no_grad():\n            final_params = param_to_vector(self.model)\n            self.local_update = final_params - initial_params\n            if s_i_k_last is not None:\n                self.local_s_i = self.mu_i - s_i_k_last\n            else:\n                self.local_s_i = self.mu_i.clone()\n\n\nclass FedSMOOServer(BaseServer):\n    \"\"\"FedSMOO Server - Coordinates smoothness-optimized federated learning\"\"\"\n    \n    def __init__(self, global_model, test_loader, num_clients, client_loaders, \n                 learning_rate=0.01, lr_decay=0.998, device='cuda', local_epochs=5, \n                 rho=0.05, beta=0.1):\n        super().__init__(global_model, test_loader, num_clients, learning_rate, \n                        lr_decay, device)\n        self.local_epochs = local_epochs\n        self.rho = rho\n        self.beta = beta\n        \n        # Initialize FedSMOO specific variables\n        with torch.no_grad():\n            init_params = param_to_vector(global_model)\n            self.global_s = torch.zeros_like(init_params, device=device)\n            self.dual_variable_list = torch.zeros(\n                (num_clients, init_params.shape[0]),\n                device=device\n            )\n        \n        # Create clients\n        for i, (train_loader, val_loader) in enumerate(client_loaders):\n            client = ClientFedSMOO(\n                i, copy.deepcopy(global_model), train_loader, val_loader, device,\n                learning_rate=learning_rate, local_epochs=local_epochs, \n                rho=rho, beta=beta\n            )\n            client.train_samples = client.get_train_samples()\n            self.clients.append(client)\n    \n    def train_round(self):\n        # Send models and variables to clients\n        self.send_models_with_variables(self.clients)\n        \n        # Reset global smoothness term\n        self.global_s.zero_()\n        \n        # Train clients\n        for client in self.clients:\n            client.train()\n            \n            # Update dual variables\n            with torch.no_grad():\n                self.dual_variable_list[client.client_id] += client.local_update.to(self.device)\n            \n            # Accumulate smoothness terms\n            with torch.no_grad():\n                self.global_s += client.local_s_i.to(self.device) / len(self.clients)\n        \n        # Normalize global smoothness term\n        with torch.no_grad():\n            global_s_norm = torch.norm(self.global_s)\n            if global_s_norm > 1e-7:\n                self.global_s = (self.rho * self.global_s) / global_s_norm\n        \n        # Aggregate models\n        self.receive_models(self.clients)\n        self.aggregate_parameters()\n        \n        # Add dual variable correction to global model\n        with torch.no_grad():\n            global_params = param_to_vector(self.global_model)\n            dual_mean = torch.mean(self.dual_variable_list, dim=0)\n            global_params = global_params + dual_mean\n            vector_to_param(global_params, self.global_model)\n        \n        # Evaluate and update learning rate\n        acc = self.evaluate()\n        self.learning_rate *= self.lr_decay\n        \n        return acc\n    \n    def send_models_with_variables(self, clients):\n        \"\"\"Send global model and FedSMOO variables to clients\"\"\"\n        model_state = self.global_model.state_dict()\n        for client in clients:\n            client.set_parameters(model_state)\n            with torch.no_grad():\n                client.global_s = self.global_s.clone()\n                client.dual_variable = self.dual_variable_list[client.client_id].clone()\n\n\nclass ClientFedSMOOTemp(ClientFedSMOO):\n    \"\"\"FedSMOO + Temperature Client\"\"\"\n    \n    def __init__(self, client_id, model, train_loader, val_loader, device, \n                 learning_rate=0.01, local_epochs=5, momentum=0.9, rho=0.05, \n                 beta=0.1, train_samples=None, feature_dim=512):\n        super().__init__(client_id, model, train_loader, val_loader, device, \n                        learning_rate, local_epochs, momentum, rho, beta, train_samples)\n        # Temperature network\n        self.tempnet = TempNet(feature_dim=feature_dim, hidden_dim=128).to(device)\n        self.temp_optimizer = optim.SGD(\n            self.tempnet.parameters(), \n            lr=learning_rate\n        )\n    \n    def train(self):\n        self.model.train()\n        self.tempnet.train()\n        self.initialize_variables()\n        \n        # Create optimizers\n        base_optimizer = optim.SGD(\n            self.model.parameters(), \n            lr=self.learning_rate, \n        )\n        smoo_optimizer = SMOOOptimizer(\n            self.model.parameters(), \n            base_optimizer, \n            rho=self.rho\n        )\n        \n        # Store initial parameters\n        with torch.no_grad():\n            initial_params = param_to_vector(self.model).clone()\n        \n        s_i_k_last = None\n        \n        for epoch in range(self.local_epochs):\n            for X, y in self.train_loader:\n                X, y = X.to(self.device), y.to(self.device)\n                \n                # === Standard forward with temperature scaling ===\n                base_optimizer.zero_grad()\n                self.temp_optimizer.zero_grad()\n                \n                output = self.model(X)\n                if isinstance(output, tuple):\n                    features, logits = output\n                    tau = self.tempnet(features.detach())\n                    scaled_logits = logits / tau\n                else:\n                    logits = output\n                    scaled_logits = logits\n                \n                loss = self.loss_fn(scaled_logits, y)\n                loss.backward()\n                \n                # === Apply smoothness adjustment ===\n                with torch.no_grad():\n                    idx = 0\n                    for p in self.model.parameters():\n                        if p.grad is not None:\n                            numel = p.grad.numel()\n                            grad_flat = p.grad.reshape(-1)\n                            adjustment = (self.mu_i[idx:idx+numel] + self.global_s[idx:idx+numel])/tau\n                            grad_flat -= adjustment\n                            idx += numel\n                \n                # === SAM first step ===\n                s_i_k = smoo_optimizer.first_step()\n                \n                # === Forward at perturbed point ===\n                base_optimizer.zero_grad()\n                self.temp_optimizer.zero_grad()\n                \n                output_pert = self.model(X)\n                if isinstance(output_pert, tuple):\n                    features_pert, logits_pert = output_pert\n                    tau_pert = self.tempnet(features_pert.detach())\n                    scaled_logits_pert = logits_pert / tau_pert\n                else:\n                    logits_pert = output_pert\n                    scaled_logits_pert = logits_pert\n                \n                loss_pert = self.loss_fn(scaled_logits_pert, y)\n                loss_pert.backward()\n                \n                # === SAM second step ===\n                smoo_optimizer.second_step()\n                \n                # === Update momentum ===\n                with torch.no_grad():\n                    self.mu_i += (s_i_k - self.global_s)\n                \n                # === Add proximal term and step ===\n                with torch.no_grad():\n                    current_params = param_to_vector(self.model)\n                    prox_grad = self.beta * (current_params - initial_params + self.dual_variable)\n\n                    prox_grad /= tau_pert\n                    \n                    idx = 0\n                    for p in self.model.parameters():\n                        if p.grad is not None:\n                            numel = p.grad.numel()\n                            p.grad += prox_grad[idx:idx+numel].view(p.grad.shape)\n                            idx += numel\n                \n                base_optimizer.step()\n                self.temp_optimizer.step()\n                \n                s_i_k_last = s_i_k\n        \n        # Store updates\n        with torch.no_grad():\n            final_params = param_to_vector(self.model)\n            self.local_update = final_params - initial_params\n            if s_i_k_last is not None:\n                self.local_s_i = self.mu_i - s_i_k_last\n            else:\n                self.local_s_i = self.mu_i.clone()\n    \n    def get_temperature(self):\n        \"\"\"Get current temperature value\"\"\"\n        self.model.eval()\n        self.tempnet.eval()\n        try:\n            with torch.no_grad():\n                X, _ = next(iter(self.train_loader))\n                X = X.to(self.device)\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    features, _ = output\n                    tau = self.tempnet(features)\n                    return tau.item()\n                else:\n                    return 1.0\n        except:\n            return 1.0\n\n\nclass FedSMOOTempServer(FedSMOOServer):\n    \"\"\"FedSMOO + Temperature Server\"\"\"\n    \n    def __init__(self, global_model, test_loader, num_clients, client_loaders, \n                 learning_rate=0.01, lr_decay=0.998, device='cuda', local_epochs=5, \n                 rho=0.05, beta=0.1, feature_dim=512):\n        # Initialize parent without creating clients\n        BaseServer.__init__(self, global_model, test_loader, num_clients, learning_rate, \n                           lr_decay, device)\n        self.local_epochs = local_epochs\n        self.rho = rho\n        self.beta = beta\n        \n        # Initialize FedSMOO specific variables\n        with torch.no_grad():\n            init_params = param_to_vector(global_model)\n            self.global_s = torch.zeros_like(init_params, device=device)\n            self.dual_variable_list = torch.zeros(\n                (num_clients, init_params.shape[0]),\n                device=device\n            )\n        \n        # Create temperature-aware clients\n        for i, (train_loader, val_loader) in enumerate(client_loaders):\n            client = ClientFedSMOOTemp(\n                i, copy.deepcopy(global_model), train_loader, val_loader, device,\n                learning_rate=learning_rate, local_epochs=local_epochs, \n                rho=rho, beta=beta, feature_dim=feature_dim\n            )\n            client.train_samples = client.get_train_samples()\n            self.clients.append(client)\n        \n        self.tau_history = []\n    \n    def train_round(self):\n        # Send models and variables to clients\n        self.send_models_with_variables(self.clients)\n        \n        # Reset global smoothness term\n        self.global_s.zero_()\n        \n        # Train clients and collect temperatures\n        local_taus = []\n        for client in self.clients:\n            client.train()\n            tau = client.get_temperature()\n            local_taus.append(tau)\n            \n            # Update dual variables\n            with torch.no_grad():\n                self.dual_variable_list[client.client_id] += client.local_update.to(self.device)\n            \n            # Accumulate smoothness terms\n            with torch.no_grad():\n                self.global_s += client.local_s_i.to(self.device) / len(self.clients)\n        \n        # Normalize global smoothness\n        with torch.no_grad():\n            global_s_norm = torch.norm(self.global_s)\n            if global_s_norm > 1e-7:\n                self.global_s = (self.rho * self.global_s) / global_s_norm\n        \n        # Aggregate models\n        self.receive_models(self.clients)\n        self.aggregate_parameters()\n        \n        # Add dual variable correction\n        with torch.no_grad():\n            global_params = param_to_vector(self.global_model)\n            dual_mean = torch.mean(self.dual_variable_list, dim=0)\n            global_params = global_params + dual_mean\n            vector_to_param(global_params, self.global_model)\n        \n        # Evaluate and update learning rate\n        acc = self.evaluate()\n        avg_tau = sum(local_taus) / len(local_taus) if local_taus else 1.0\n        self.tau_history.append(avg_tau)\n        \n        self.learning_rate *= self.lr_decay\n        \n        return acc, avg_tau\n\n# ========================================\n# FEDGMT IMPLEMENTATION \n# ========================================\n\n\ndef param_to_vector(model):\n    \"\"\"Convert model parameters to a single vector\"\"\"\n    vec = []\n    for param in model.parameters():\n        vec.append(param.data.reshape(-1))\n    return torch.cat(vec)\n\n\ndef vector_to_param(vector, model):\n    \"\"\"Assign vector values back to model parameters\"\"\"\n    index = 0\n    for param in model.parameters():\n        param_size = param.numel()\n        param.data.copy_(vector[index:index + param_size].view(param.shape))\n        index += param_size\n\n\nclass ClientFedGMT(BaseClient):\n    \"\"\"FedGMT Client - Global Model Teaching with dual variables\"\"\"\n    \n    def __init__(self, client_id, model, train_loader, val_loader, device, \n                 learning_rate=0.01, local_epochs=5, momentum=0.9, tau=3.0, \n                 gamma=1.0, beta=0.1, train_samples=None):\n        super().__init__(client_id, model, train_loader, val_loader, device, \n                        learning_rate, local_epochs, momentum)\n        self.tau = tau  # Temperature for KL divergence\n        self.gamma = gamma  # Weight for KL divergence loss\n        self.beta = beta  # Proximal term coefficient\n        self.train_samples = train_samples or 0\n        \n        # FedGMT specific variables\n        self.EMA_model = None  # Received from server (global EMA)\n        self.dual_variable = None  # Received from server\n        self.local_update = None  # To send back to server\n    \n    def train(self):\n        \"\"\"Train with KL divergence to global EMA model + dual variable correction\"\"\"\n        self.model.train()\n        if self.EMA_model is not None:\n            self.EMA_model.eval()\n        \n        optimizer = optim.SGD(\n            self.model.parameters(), \n            lr=self.learning_rate,\n        )\n        kl_loss_fn = nn.KLDivLoss(reduction=\"batchmean\")\n        \n        # Store initial parameters for dual variable update\n        with torch.no_grad():\n            initial_params = param_to_vector(self.model).clone()\n        \n        for epoch in range(self.local_epochs):\n            for X, y in self.train_loader:\n                X, y = X.to(self.device), y.to(self.device)\n                \n                optimizer.zero_grad()\n                \n                # === Forward pass on current model ===\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    _, logits = output\n                else:\n                    logits = output\n                \n                # Cross-entropy loss\n                ce_loss = self.loss_fn(logits, y)\n                total_loss = ce_loss\n                \n                # === KL divergence to global EMA model ===\n                if self.EMA_model is not None:\n                    with torch.no_grad():\n                        output_ema = self.EMA_model(X)\n                        if isinstance(output_ema, tuple):\n                            _, logits_ema = output_ema\n                        else:\n                            logits_ema = output_ema\n                    \n                    # KL(model || EMA_model)\n                    pred_log_prob = F.log_softmax(logits / self.tau, dim=1)\n                    target_prob = F.softmax(logits_ema / self.tau, dim=1)\n                    kl_div_loss = kl_loss_fn(pred_log_prob, target_prob)\n                    \n                    # Add KL term with temperature scaling\n                    total_loss = total_loss + self.gamma * (self.tau ** 2) * kl_div_loss\n                \n                # Backward pass\n                total_loss.backward()\n                \n                # === Add proximal term for dual variable ===\n                if self.dual_variable is not None:\n                    with torch.no_grad():\n                        current_params = param_to_vector(self.model)\n                        # Proximal gradient: beta * (current - initial + dual)\n                        prox_grad = self.beta * (current_params - initial_params + self.dual_variable)\n                        \n                        idx = 0\n                        for p in self.model.parameters():\n                            if p.grad is not None:\n                                numel = p.grad.numel()\n                                p.grad += prox_grad[idx:idx+numel].view(p.grad.shape)\n                                idx += numel\n                \n                optimizer.step()\n        \n        # Store local update for server\n        with torch.no_grad():\n            final_params = param_to_vector(self.model)\n            self.local_update = final_params - initial_params\n\n\nclass FedGMTServer(BaseServer):\n    \"\"\"FedGMT Server - Maintains global EMA model and dual variables\"\"\"\n    \n    def __init__(self, global_model, test_loader, num_clients, client_loaders, \n                 learning_rate=0.01, lr_decay=0.998, device='cuda', local_epochs=5, \n                 tau=3.0, gamma=1.0, beta=0.1, alpha=0.95):\n        super().__init__(global_model, test_loader, num_clients, learning_rate, \n                        lr_decay, device)\n        self.local_epochs = local_epochs\n        self.tau = tau\n        self.gamma = gamma\n        self.beta = beta\n        self.alpha = alpha  # EMA momentum for global EMA model\n        \n        # Global EMA model (key difference from original FedAvg)\n        self.EMA_model = copy.deepcopy(global_model)\n        self.EMA_model.to(device)\n        \n        # Dual variables for each client\n        with torch.no_grad():\n            init_params = param_to_vector(global_model)\n            self.dual_variable_list = torch.zeros(\n                (num_clients, init_params.shape[0]),\n                device=device\n            )\n        \n        # Create clients\n        for i, (train_loader, val_loader) in enumerate(client_loaders):\n            client = ClientFedGMT(\n                i, copy.deepcopy(global_model), train_loader, val_loader, device,\n                learning_rate=learning_rate, local_epochs=local_epochs, \n                tau=tau, gamma=gamma, beta=beta\n            )\n            client.train_samples = client.get_train_samples()\n            self.clients.append(client)\n    \n    def train_round(self):\n        \"\"\"Train one round with global EMA teaching\"\"\"\n        # Send global model, EMA model, and dual variables to clients\n        self.send_models_with_ema(self.clients)\n        \n        # Train clients\n        for client in self.clients:\n            client.train()\n            \n            # Update dual variables with local updates\n            with torch.no_grad():\n                self.dual_variable_list[client.client_id] += client.local_update.to(self.device)\n        \n        # Aggregate client models\n        self.receive_models(self.clients)\n        self.aggregate_parameters()\n        \n        # Add dual variable correction to global model\n        with torch.no_grad():\n            global_params = param_to_vector(self.global_model)\n            dual_mean = torch.mean(self.dual_variable_list, dim=0)\n            global_params = global_params + dual_mean\n            vector_to_param(global_params, self.global_model)\n        \n        # Update global EMA model\n        with torch.no_grad():\n            ema_params = param_to_vector(self.EMA_model)\n            global_params = param_to_vector(self.global_model)\n            \n            # EMA update: EMA = alpha * EMA + (1 - alpha) * global\n            ema_params = self.alpha * ema_params + (1 - self.alpha) * global_params\n            vector_to_param(ema_params, self.EMA_model)\n        \n        # Evaluate and update learning rate\n        acc = self.evaluate()\n        self.learning_rate *= self.lr_decay\n        \n        return acc\n    \n    def send_models_with_ema(self, clients):\n        \"\"\"Send global model, EMA model, and dual variables to clients\"\"\"\n        global_state = self.global_model.state_dict()\n        \n        for client in clients:\n            # Send global model\n            client.set_parameters(global_state)\n            \n            # Send EMA model (deep copy to avoid sharing)\n            client.EMA_model = copy.deepcopy(self.EMA_model)\n            client.EMA_model.eval()\n            \n            # Send dual variable\n            with torch.no_grad():\n                client.dual_variable = self.dual_variable_list[client.client_id].clone()\n\n\nclass ClientFedGMTTemp(ClientFedGMT):\n    \n    def __init__(self, client_id, model, train_loader, val_loader, device, \n                 learning_rate=0.01, local_epochs=5, momentum=0.9, tau=3.0, \n                 gamma=1.0, beta=0.1, train_samples=None, feature_dim=512):\n        super().__init__(client_id, model, train_loader, val_loader, device, \n                        learning_rate, local_epochs, momentum, tau, gamma, beta, train_samples)\n        \n        # Learnable temperature network\n        self.tempnet = TempNet(feature_dim=feature_dim, hidden_dim=128).to(device)\n        self.temp_optimizer = optim.SGD(self.tempnet.parameters(), lr=learning_rate)\n    \n    def train(self):\n        \"\"\"Train with adaptive temperature scaling\"\"\"\n        self.model.train()\n        self.tempnet.train()\n        if self.EMA_model is not None:\n            self.EMA_model.eval()\n        \n        optimizer = optim.SGD(\n            self.model.parameters(), \n            lr=self.learning_rate,\n        )\n        kl_loss_fn = nn.KLDivLoss(reduction=\"batchmean\")\n        \n        with torch.no_grad():\n            initial_params = param_to_vector(self.model).clone()\n        \n        for epoch in range(self.local_epochs):\n            for X, y in self.train_loader:\n                X, y = X.to(self.device), y.to(self.device)\n                \n                optimizer.zero_grad()\n                self.temp_optimizer.zero_grad()\n                \n                # Forward pass\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    features, logits = output\n                else:\n                    features = None\n                    logits = output\n                \n                # Compute adaptive temperature\n                tau_adaptive = self.tempnet(features.detach())\n                \n                # Cross-entropy loss\n                ce_loss = self.loss_fn(logits/tau_adaptive, y)\n                total_loss = ce_loss\n                \n                # KL divergence with adaptive temperature\n                if self.EMA_model is not None:\n                    with torch.no_grad():\n                        output_ema = self.EMA_model(X)\n                        if isinstance(output_ema, tuple):\n                            _, logits_ema = output_ema\n                        else:\n                            logits_ema = output_ema\n                    \n                    pred_log_prob = F.log_softmax(logits / self.tau, dim=1)\n                    target_prob = F.softmax(logits_ema / self.tau, dim=1)\n                    kl_div_loss = kl_loss_fn(pred_log_prob, target_prob)\n                    \n                    total_loss = total_loss + self.gamma * (self.tau ** 2) * kl_div_loss\n                \n                total_loss.backward()\n                \n                # Proximal term\n                if self.dual_variable is not None:\n                    with torch.no_grad():\n                        current_params = param_to_vector(self.model)\n                        prox_grad = self.beta * (current_params - initial_params + self.dual_variable)\n\n                        prox_grad /= tau_adaptive\n                        \n                        idx = 0\n                        for p in self.model.parameters():\n                            if p.grad is not None:\n                                numel = p.grad.numel()\n                                p.grad += prox_grad[idx:idx+numel].view(p.grad.shape)\n                                idx += numel\n                \n                optimizer.step()\n                self.temp_optimizer.step()\n        \n        with torch.no_grad():\n            final_params = param_to_vector(self.model)\n            self.local_update = final_params - initial_params\n    \n    def get_temperature(self):\n        \"\"\"Get current average temperature\"\"\"\n        self.model.eval()\n        self.tempnet.eval()\n        try:\n            with torch.no_grad():\n                X, _ = next(iter(self.train_loader))\n                X = X.to(self.device)\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    features, _ = output\n                    tau = self.tempnet(features)\n                    return tau.item()\n                else:\n                    return self.tau\n        except:\n            return self.tau\n\n\nclass FedGMTTempServer(FedGMTServer):\n    \"\"\"FedGMT + Temperature Server\"\"\"\n    \n    def __init__(self, global_model, test_loader, num_clients, client_loaders, \n                 learning_rate=0.01, lr_decay=0.998, device='cuda', local_epochs=5, \n                 tau=3.0, gamma=1.0, beta=0.1, alpha=0.95, feature_dim=512):\n        # Initialize parent without creating clients\n        BaseServer.__init__(self, global_model, test_loader, num_clients, \n                           learning_rate, lr_decay, device)\n        self.local_epochs = local_epochs\n        self.tau = tau\n        self.gamma = gamma\n        self.beta = beta\n        self.alpha = alpha\n        \n        # Global EMA model\n        self.EMA_model = copy.deepcopy(global_model)\n        self.EMA_model.to(device)\n        \n        # Dual variables\n        with torch.no_grad():\n            init_params = param_to_vector(global_model)\n            self.dual_variable_list = torch.zeros(\n                (num_clients, init_params.shape[0]),\n                device=device\n            )\n        \n        # Create temperature-aware clients\n        for i, (train_loader, val_loader) in enumerate(client_loaders):\n            client = ClientFedGMTTemp(\n                i, copy.deepcopy(global_model), train_loader, val_loader, device,\n                learning_rate=learning_rate, local_epochs=local_epochs, \n                tau=tau, gamma=gamma, beta=beta, feature_dim=feature_dim\n            )\n            client.train_samples = client.get_train_samples()\n            self.clients.append(client)\n        \n        self.tau_history = []\n    \n    def train_round(self):\n        \"\"\"Train round with temperature tracking\"\"\"\n        self.send_models_with_ema(self.clients)\n        \n        local_taus = []\n        for client in self.clients:\n            client.train()\n            tau = client.get_temperature()\n            local_taus.append(tau)\n            \n            with torch.no_grad():\n                self.dual_variable_list[client.client_id] += client.local_update.to(self.device)\n        \n        self.receive_models(self.clients)\n        self.aggregate_parameters()\n        \n        # Apply dual variable correction\n        with torch.no_grad():\n            global_params = param_to_vector(self.global_model)\n            dual_mean = torch.mean(self.dual_variable_list, dim=0)\n            global_params = global_params + dual_mean\n            vector_to_param(global_params, self.global_model)\n        \n        # Update global EMA\n        with torch.no_grad():\n            ema_params = param_to_vector(self.EMA_model)\n            global_params = param_to_vector(self.global_model)\n            ema_params = self.alpha * ema_params + (1 - self.alpha) * global_params\n            vector_to_param(ema_params, self.EMA_model)\n        \n        acc = self.evaluate()\n        avg_tau = sum(local_taus) / len(local_taus) if local_taus else self.tau\n        self.tau_history.append(avg_tau)\n        \n        self.learning_rate *= self.lr_decay\n        \n        return acc, avg_tau\n    \n\n# ========================================\n# FEDPROX IMPLEMENTATION\n# ========================================\n\nclass ClientFedProx(BaseClient):\n    \"\"\"FedProx client - FedAvg with proximal term\"\"\"\n    \n    def __init__(self, client_id, model, train_loader, val_loader, device, \n                 learning_rate=0.01, local_epochs=5, momentum=0.9, mu=0.01, \n                 train_samples=None):\n        super().__init__(client_id, model, train_loader, val_loader, device, \n                        learning_rate, local_epochs, momentum)\n        self.mu = mu  # Proximal term coefficient\n        self.train_samples = train_samples or 0\n        self.global_model = None\n    \n    def set_global_model(self, global_state_dict):\n        \"\"\"Store global model state for proximal term\"\"\"\n        self.global_model = copy.deepcopy(self.model)\n        self.global_model.load_state_dict(global_state_dict)\n        self.global_model.eval()\n    \n    def train(self):\n        self.model.train()\n        optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate)\n        \n        for epoch in range(self.local_epochs):\n            for X, y in self.train_loader:\n                X, y = X.to(self.device), y.to(self.device)\n                \n                optimizer.zero_grad()\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    _, logits = output\n                else:\n                    logits = output\n                \n                # Standard cross-entropy loss\n                loss = self.loss_fn(logits, y)\n                \n                # Add proximal term: (mu/2) * ||w - w_global||^2\n                if self.global_model is not None:\n                    proximal_term = 0.0\n                    for param, global_param in zip(self.model.parameters(), \n                                                   self.global_model.parameters()):\n                        proximal_term += torch.norm(param - global_param.detach()) ** 2\n                    loss += (self.mu / 2) * proximal_term\n                \n                loss.backward()\n                optimizer.step()\n\n\nclass FedProxServer(BaseServer):\n    \"\"\"FedProx server\"\"\"\n    \n    def __init__(self, global_model, test_loader, num_clients, client_loaders, \n                 learning_rate=0.01, lr_decay=0.998, device='cuda', local_epochs=5, \n                 mu=0.01):\n        super().__init__(global_model, test_loader, num_clients, learning_rate, \n                        lr_decay, device)\n        self.local_epochs = local_epochs\n        self.mu = mu\n        \n        for i, (train_loader, val_loader) in enumerate(client_loaders):\n            client = ClientFedProx(i, copy.deepcopy(global_model), train_loader, \n                                  val_loader, device, learning_rate=learning_rate, \n                                  local_epochs=local_epochs, mu=mu)\n            client.train_samples = client.get_train_samples()\n            self.clients.append(client)\n    \n    def train_round(self):\n        self.send_models(self.clients)\n        \n        # Set global model for proximal term\n        global_state = self.global_model.state_dict()\n        for client in self.clients:\n            client.set_global_model(global_state)\n            client.train()\n        \n        self.receive_models(self.clients)\n        self.aggregate_parameters()\n        \n        acc = self.evaluate()\n        self.learning_rate *= self.lr_decay\n        \n        return acc\n\n\n# ========================================\n# FEDPROX + TEMPNET IMPLEMENTATION\n# ========================================\n\nclass ClientFedProxTemp(BaseClient):\n    \"\"\"FedProx + Temperature: FedProx with learned temperature scaling\"\"\"\n    \n    def __init__(self, client_id, model, train_loader, val_loader, device, \n                 learning_rate=0.01, local_epochs=5, momentum=0.9, mu=0.01, \n                 train_samples=None):\n        super().__init__(client_id, model, train_loader, val_loader, device, \n                        learning_rate, local_epochs, momentum)\n        self.mu = mu\n        self.train_samples = train_samples or 0\n        self.global_model = None\n        self.tempnet = TempNet(feature_dim=512, hidden_dim=128).to(device)\n        self.temp_optimizer = optim.SGD(self.tempnet.parameters(), \n                                       lr=learning_rate)\n    \n    def set_global_model(self, global_state_dict):\n        \"\"\"Store global model state for proximal term\"\"\"\n        self.global_model = copy.deepcopy(self.model)\n        self.global_model.load_state_dict(global_state_dict)\n        self.global_model.eval()\n    \n    def train(self):\n        self.model.train()\n        self.tempnet.train()\n        optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate)\n        \n        for epoch in range(self.local_epochs):\n            for X, y in self.train_loader:\n                X, y = X.to(self.device), y.to(self.device)\n                \n                optimizer.zero_grad()\n                self.temp_optimizer.zero_grad()\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    features, logits = output\n                else:\n                    features = None\n                    logits = output\n                \n                # Get temperature\n                tau = self.tempnet(features.detach()) if features is not None else torch.tensor(1.0, device=self.device)\n                \n                # Standard cross-entropy loss with temperature\n                scaled_logits = logits / tau\n                loss = self.loss_fn(scaled_logits, y)\n                \n                # Add proximal term\n                if self.global_model is not None:\n                    proximal_term = 0.0\n                    for param, global_param in zip(self.model.parameters(), \n                                                   self.global_model.parameters()):\n                        proximal_term += torch.norm(param - global_param.detach()) ** 2\n                    loss += (self.mu / 2) * proximal_term\n                \n                loss.backward()\n                optimizer.step()\n                self.temp_optimizer.step()\n    \n    def get_temperature(self):\n        \"\"\"Get current temperature value\"\"\"\n        self.model.eval()\n        self.tempnet.eval()\n        try:\n            with torch.no_grad():\n                X, _ = next(iter(self.train_loader))\n                X = X.to(self.device)\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    features, _ = output\n                    tau = self.tempnet(features)\n                    return tau.item()\n                else:\n                    return 1.0\n        except:\n            return 1.0\n\n\nclass FedProxTempServer(BaseServer):\n    \"\"\"FedProx + Temperature Server\"\"\"\n    \n    def __init__(self, global_model, test_loader, num_clients, client_loaders, \n                 learning_rate=0.01, lr_decay=0.998, device='cuda', local_epochs=5, \n                 mu=0.01):\n        super().__init__(global_model, test_loader, num_clients, learning_rate, \n                        lr_decay, device)\n        self.local_epochs = local_epochs\n        self.mu = mu\n        self.tau_history = []\n        \n        for i, (train_loader, val_loader) in enumerate(client_loaders):\n            client = ClientFedProxTemp(i, copy.deepcopy(global_model), train_loader, \n                                      val_loader, device, learning_rate=learning_rate, \n                                      local_epochs=local_epochs, mu=mu)\n            client.train_samples = client.get_train_samples()\n            self.clients.append(client)\n    \n    def train_round(self):\n        self.send_models(self.clients)\n        \n        # Set global model for proximal term\n        global_state = self.global_model.state_dict()\n        for client in self.clients:\n            client.set_global_model(global_state)\n        \n        local_taus = []\n        for client in self.clients:\n            client.train()\n            tau = client.get_temperature()\n            local_taus.append(tau)\n        \n        self.receive_models(self.clients)\n        self.aggregate_parameters()\n        \n        acc = self.evaluate()\n        avg_tau = sum(local_taus) / len(local_taus) if local_taus else 1.0\n        self.tau_history.append(avg_tau)\n        \n        self.learning_rate *= self.lr_decay\n        \n        return acc, avg_tau","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T13:06:50.666643Z","iopub.execute_input":"2025-12-27T13:06:50.666943Z","iopub.status.idle":"2025-12-27T13:06:50.915501Z","shell.execute_reply.started":"2025-12-27T13:06:50.666916Z","shell.execute_reply":"2025-12-27T13:06:50.914854Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# ========================================\n# TRAINING FUNCTIONS FOR ALL ALGORITHMS\n# ========================================\ndef run_fedavg():\n    \"\"\"Run FedAvg training\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STARTING FedAvg TRAINING\")\n    print(\"=\"*60)\n    \n    client_loaders = [(train_loaders[i], val_loaders[i]) for i in range(NUM_OF_CLIENTS)]\n    \n    server = FedAvgServer(\n        global_model=SimpleCNN().to(device),\n        test_loader=test_loader,\n        num_clients=NUM_OF_CLIENTS,\n        client_loaders=client_loaders,\n        learning_rate=LEARNING_RATE,\n        lr_decay=1,\n        device=device,\n        local_epochs=LOCAL_EPOCHS\n    )\n    \n    test_accs = []\n    for rnd in range(COMM_ROUND):\n        round_start = time.time()\n        acc = server.train_round()\n        test_accs.append(acc)\n        \n        print(f\"[FedAvg] R{rnd+1}/{COMM_ROUND} Test Acc: {acc:.4f} | Time: {time.time()-round_start:.2f}s\")\n    \n    return test_accs, server.global_model\n\n\ndef run_fedavg_temp():\n    \"\"\"Run FedAvg + Temperature training\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STARTING FedAvg + TEMP TRAINING\")\n    print(\"=\"*60)\n    \n    client_loaders = [(train_loaders[i], val_loaders[i]) for i in range(NUM_OF_CLIENTS)]\n    \n    server = FedAvgTempServer(\n        global_model=SimpleCNN().to(device),\n        test_loader=test_loader,\n        num_clients=NUM_OF_CLIENTS,\n        client_loaders=client_loaders,\n        learning_rate=LEARNING_RATE,\n        lr_decay=1,\n        device=device,\n        local_epochs=LOCAL_EPOCHS\n    )\n    \n    test_accs = []\n    tau_history = []\n    for rnd in range(COMM_ROUND):\n        round_start = time.time()\n        acc, avg_tau = server.train_round()\n        test_accs.append(acc)\n        tau_history.append(avg_tau)\n        \n        print(f\"[FedAvg+Temp] R{rnd+1}/{COMM_ROUND} Test Acc: {acc:.4f} | Avg τ: {avg_tau:.4f} | Time: {time.time()-round_start:.2f}s\")\n    \n    return test_accs, tau_history, server.global_model\n\n\ndef run_fedsam():\n    \"\"\"Run FedSAM training\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STARTING FedSAM TRAINING\")\n    print(\"=\"*60)\n    \n    client_loaders = [(train_loaders[i], val_loaders[i]) for i in range(NUM_OF_CLIENTS)]\n    \n    server = FedSAMServer(\n        global_model=SimpleCNN().to(device),\n        test_loader=test_loader,\n        num_clients=NUM_OF_CLIENTS,\n        client_loaders=client_loaders,\n        learning_rate=LEARNING_RATE,\n        lr_decay=1,\n        device=device,\n        local_epochs=LOCAL_EPOCHS,\n        rho=RHO\n    )\n    \n    test_accs = []\n    for rnd in range(COMM_ROUND):\n        round_start = time.time()\n        acc = server.train_round()\n        test_accs.append(acc)\n        \n        print(f\"[FedSAM] R{rnd+1}/{COMM_ROUND} Test Acc: {acc:.4f} | Time: {time.time()-round_start:.2f}s\")\n    \n    return test_accs, server.global_model\n\n\ndef run_fedsam_temp():\n    \"\"\"Run FedSAM + Temperature training\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STARTING FedSAM + TEMP TRAINING\")\n    print(\"=\"*60)\n    \n    client_loaders = [(train_loaders[i], val_loaders[i]) for i in range(NUM_OF_CLIENTS)]\n    \n    server = FedSAMTempServer(\n        global_model=SimpleCNN().to(device),\n        test_loader=test_loader,\n        num_clients=NUM_OF_CLIENTS,\n        client_loaders=client_loaders,\n        learning_rate=LEARNING_RATE,\n        lr_decay=1,\n        device=device,\n        local_epochs=LOCAL_EPOCHS,\n        rho=RHO\n    )\n    \n    test_accs = []\n    tau_history = []\n    for rnd in range(COMM_ROUND):\n        round_start = time.time()\n        acc, avg_tau = server.train_round()\n        test_accs.append(acc)\n        tau_history.append(avg_tau)\n        \n        print(f\"[FedSAM+Temp] R{rnd+1}/{COMM_ROUND} Test Acc: {acc:.4f} | Avg τ: {avg_tau:.4f} | Time: {time.time()-round_start:.2f}s\")\n    \n    return test_accs, tau_history, server.global_model\n\n\ndef run_fedlesam():\n    \"\"\"Run FedLESAM training\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STARTING FedLESAM TRAINING\")\n    print(\"=\"*60)\n    \n    client_loaders = [(train_loaders[i], val_loaders[i]) for i in range(NUM_OF_CLIENTS)]\n    \n    server = FedLESAMServer(\n        global_model=SimpleCNN().to(device),\n        test_loader=test_loader,\n        num_clients=NUM_OF_CLIENTS,\n        client_loaders=client_loaders,\n        learning_rate=LEARNING_RATE,\n        lr_decay=1,\n        device=device,\n        local_epochs=LOCAL_EPOCHS,\n        rho=RHO\n    )\n    \n    test_accs = []\n    for rnd in range(COMM_ROUND):\n        round_start = time.time()\n        acc = server.train_round()\n        test_accs.append(acc)\n        \n        print(f\"[FedLESAM] R{rnd+1}/{COMM_ROUND} Test Acc: {acc:.4f} | Time: {time.time()-round_start:.2f}s\")\n    \n    return test_accs, server.global_model\n\n\ndef run_fedlesam_temp():\n    \"\"\"Run FedLESAM + Temperature training\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STARTING FedLESAM + TEMP TRAINING\")\n    print(\"=\"*60)\n    \n    client_loaders = [(train_loaders[i], val_loaders[i]) for i in range(NUM_OF_CLIENTS)]\n    \n    server = FedLESAMTempServer(\n        global_model=SimpleCNN().to(device),\n        test_loader=test_loader,\n        num_clients=NUM_OF_CLIENTS,\n        client_loaders=client_loaders,\n        learning_rate=LEARNING_RATE,\n        lr_decay=1,\n        device=device,\n        local_epochs=LOCAL_EPOCHS,\n        rho=RHO\n    )\n    \n    test_accs = []\n    tau_history = []\n    for rnd in range(COMM_ROUND):\n        round_start = time.time()\n        acc, avg_tau = server.train_round()\n        test_accs.append(acc)\n        tau_history.append(avg_tau)\n        \n        print(f\"[FedLESAM+Temp] R{rnd+1}/{COMM_ROUND} Test Acc: {acc:.4f} | Avg τ: {avg_tau:.4f} | Time: {time.time()-round_start:.2f}s\")\n    \n    return test_accs, tau_history, server.global_model\n\n\ndef run_fedsmoo():\n    \"\"\"Run FedSMOO training\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STARTING FedSMOO TRAINING\")\n    print(\"=\"*60)\n    \n    client_loaders = [(train_loaders[i], val_loaders[i]) for i in range(NUM_OF_CLIENTS)]\n    \n    server = FedSMOOServer(\n        global_model=SimpleCNN().to(device),\n        test_loader=test_loader,\n        num_clients=NUM_OF_CLIENTS,\n        client_loaders=client_loaders,\n        learning_rate=LEARNING_RATE,\n        lr_decay=1,\n        device=device,\n        local_epochs=LOCAL_EPOCHS,\n        rho=RHO\n    )\n    \n    test_accs = []\n    for rnd in range(COMM_ROUND):\n        round_start = time.time()\n        acc = server.train_round()\n        test_accs.append(acc)\n        \n        print(f\"[FedSMOO] R{rnd+1}/{COMM_ROUND} Test Acc: {acc:.4f} | Time: {time.time()-round_start:.2f}s\")\n    \n    return test_accs, server.global_model\n\n\ndef run_fedsmoo_temp():\n    \"\"\"Run FedSMOO + Temperature training\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STARTING FedSMOO + TEMP TRAINING\")\n    print(\"=\"*60)\n    \n    client_loaders = [(train_loaders[i], val_loaders[i]) for i in range(NUM_OF_CLIENTS)]\n    \n    server = FedSMOOTempServer(\n        global_model=SimpleCNN().to(device),\n        test_loader=test_loader,\n        num_clients=NUM_OF_CLIENTS,\n        client_loaders=client_loaders,\n        learning_rate=LEARNING_RATE,\n        lr_decay=1,\n        device=device,\n        local_epochs=LOCAL_EPOCHS,\n        rho=RHO\n    )\n    \n    test_accs = []\n    tau_history = []\n    for rnd in range(COMM_ROUND):\n        round_start = time.time()\n        acc, avg_tau = server.train_round()\n        test_accs.append(acc)\n        tau_history.append(avg_tau)\n        \n        print(f\"[FedSMOO+Temp] R{rnd+1}/{COMM_ROUND} Test Acc: {acc:.4f} | Avg τ: {avg_tau:.4f} | Time: {time.time()-round_start:.2f}s\")\n    \n    return test_accs, tau_history, server.global_model\n\n\ndef run_fedgmt():\n    \"\"\"Run FedGMT training\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STARTING FedGMT TRAINING\")\n    print(\"=\"*60)\n    \n    client_loaders = [(train_loaders[i], val_loaders[i]) for i in range(NUM_OF_CLIENTS)]\n    \n    server = FedGMTServer(\n        global_model=SimpleCNN().to(device),\n        test_loader=test_loader,\n        num_clients=NUM_OF_CLIENTS,\n        client_loaders=client_loaders,\n        learning_rate=LEARNING_RATE,\n        lr_decay=1,\n        device=device,\n        local_epochs=LOCAL_EPOCHS,\n        tau=3.0,\n        gamma=1.0\n    )\n    \n    test_accs = []\n    for rnd in range(COMM_ROUND):\n        round_start = time.time()\n        acc = server.train_round()\n        test_accs.append(acc)\n        \n        print(f\"[FedGMT] R{rnd+1}/{COMM_ROUND} Test Acc: {acc:.4f} | Time: {time.time()-round_start:.2f}s\")\n    \n    return test_accs, server.global_model\n\n\ndef run_fedgmt_temp():\n    \"\"\"Run FedGMT + Temperature training\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STARTING FedGMT + TEMP TRAINING\")\n    print(\"=\"*60)\n    \n    client_loaders = [(train_loaders[i], val_loaders[i]) for i in range(NUM_OF_CLIENTS)]\n    \n    server = FedGMTTempServer(\n        global_model=SimpleCNN().to(device),\n        test_loader=test_loader,\n        num_clients=NUM_OF_CLIENTS,\n        client_loaders=client_loaders,\n        learning_rate=LEARNING_RATE,\n        lr_decay=1,\n        device=device,\n        local_epochs=LOCAL_EPOCHS,\n        tau=3.0,\n        gamma=1.0\n    )\n    \n    test_accs = []\n    tau_history = []\n    for rnd in range(COMM_ROUND):\n        round_start = time.time()\n        acc, avg_tau = server.train_round()\n        test_accs.append(acc)\n        tau_history.append(avg_tau)\n        \n        print(f\"[FedGMT+Temp] R{rnd+1}/{COMM_ROUND} Test Acc: {acc:.4f} | Avg τ: {avg_tau:.4f} | Time: {time.time()-round_start:.2f}s\")\n    \n    return test_accs, tau_history, server.global_model\n\ndef run_fedprox():\n    \"\"\"Run FedProx training\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STARTING FedProx TRAINING\")\n    print(\"=\"*60)\n    \n    client_loaders = [(train_loaders[i], val_loaders[i]) for i in range(NUM_OF_CLIENTS)]\n    \n    server = FedProxServer(\n        global_model=SimpleCNN().to(device),\n        test_loader=test_loader,\n        num_clients=NUM_OF_CLIENTS,\n        client_loaders=client_loaders,\n        learning_rate=LEARNING_RATE,\n        lr_decay=1,\n        device=device,\n        local_epochs=LOCAL_EPOCHS,\n        mu=0.01\n    )\n    \n    test_accs = []\n    for rnd in range(COMM_ROUND):\n        round_start = time.time()\n        acc = server.train_round()\n        test_accs.append(acc)\n        \n        print(f\"[FedProx] R{rnd+1}/{COMM_ROUND} Test Acc: {acc:.4f} | Time: {time.time()-round_start:.2f}s\")\n    \n    return test_accs, server.global_model\n\n\ndef run_fedprox_temp():\n    \"\"\"Run FedProx + Temperature training\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STARTING FedProx + TEMP TRAINING\")\n    print(\"=\"*60)\n    \n    client_loaders = [(train_loaders[i], val_loaders[i]) for i in range(NUM_OF_CLIENTS)]\n    \n    server = FedProxTempServer(\n        global_model=SimpleCNN().to(device),\n        test_loader=test_loader,\n        num_clients=NUM_OF_CLIENTS,\n        client_loaders=client_loaders,\n        learning_rate=LEARNING_RATE,\n        lr_decay=1,\n        device=device,\n        local_epochs=LOCAL_EPOCHS,\n        mu=0.01\n    )\n    \n    test_accs = []\n    tau_history = []\n    for rnd in range(COMM_ROUND):\n        round_start = time.time()\n        acc, avg_tau = server.train_round()\n        test_accs.append(acc)\n        tau_history.append(avg_tau)\n        \n        print(f\"[FedProx+Temp] R{rnd+1}/{COMM_ROUND} Test Acc: {acc:.4f} | Avg τ: {avg_tau:.4f} | Time: {time.time()-round_start:.2f}s\")\n    \n    return test_accs, tau_history, server.global_model\n\n# ========================================\n# RUN ALL EXPERIMENTS\n# ========================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"FEDERATED LEARNING EXPERIMENTS - COMPLETE FL-SAM SUITE\")\nprint(\"=\"*80)\nprint(f\"Configuration:\")\nprint(f\"  - Num Clients: {NUM_OF_CLIENTS}\")\nprint(f\"  - Comm Rounds: {COMM_ROUND}\")\nprint(f\"  - Local Epochs: {LOCAL_EPOCHS}\")\nprint(f\"  - Batch Size: {BATCH_SIZE}\")\nprint(f\"  - Learning Rate: {LEARNING_RATE}\")\nprint(f\"  - RHO (SAM): {RHO}\")\nprint(f\"  - Alpha (Dirichlet): {ALPHA}\")\nprint(\"=\"*80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T13:07:00.950806Z","iopub.execute_input":"2025-12-27T13:07:00.951102Z","iopub.status.idle":"2025-12-27T13:07:00.982537Z","shell.execute_reply.started":"2025-12-27T13:07:00.951075Z","shell.execute_reply":"2025-12-27T13:07:00.981881Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nFEDERATED LEARNING EXPERIMENTS - COMPLETE FL-SAM SUITE\n================================================================================\nConfiguration:\n  - Num Clients: 10\n  - Comm Rounds: 50\n  - Local Epochs: 5\n  - Batch Size: 64\n  - Learning Rate: 0.01\n  - RHO (SAM): 0.05\n  - Alpha (Dirichlet): 0.5\n================================================================================\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"print(\"\\n[1/12] FedSMOO...\")\ntest_accs_fedsmoo, model_fedsmoo = run_fedsmoo()\n\nprint(\"\\n[2/12] FedSMOO+Temp...\")\ntest_accs_fedsmoo_temp, tau_fedsmoo_temp, model_fedsmoo_temp = run_fedsmoo_temp()\n\nprint(\"\\n[3/12] FedGMT...\")\ntest_accs_fedgmt, model_fedgmt = run_fedgmt()\n\nprint(\"\\n[4/12] FedGMT+Temp...\")\ntest_accs_fedgmt_temp, tau_fedgmt_temp, model_fedgmt_temp = run_fedgmt_temp()\n\n# print(\"\\n[5/12] FedAvg...\")\n# test_accs_fedavg, model_fedavg = run_fedavg()\n\n# print(\"\\n[6/12] FedAvg+Temp...\")\n# test_accs_fedavg_temp, tau_fedavg_temp, model_fedavg_temp = run_fedavg_temp()\n\n# print(\"\\n[7/12] FedSAM...\")\n# test_accs_fedsam, model_fedsam = run_fedsam()\n\n# print(\"\\n[8/12] FedSAM+Temp...\")\n# test_accs_fedsam_temp, tau_fedsam_temp, model_fedsam_temp = run_fedsam_temp()\n\n# print(\"\\n[9/12] FedLESAM...\")\n# test_accs_fedlesam, model_fedlesam = run_fedlesam()\n\n# print(\"\\n[10/12] FedLESAM+Temp...\")\n# test_accs_fedlesam_temp, tau_fedlesam_temp, model_fedlesam_temp = run_fedlesam_temp()\n\n# print(\"\\n[11/12] FedProx...\")\n# test_accs_fedprox, model_fedprox = run_fedprox()\n\n# print(\"\\n[12/12] FedProx+Temp...\")\n# test_accs_fedprox_temp, tau_fedprox_temp, model_fedprox_temp = run_fedprox_temp()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# ========================================\n# PLOTTING ALL ALGORITHMS\n# ========================================\n\nplt.figure(figsize=(14, 8))\n\n# Define colors and line styles for better distinction\ncolors = {\n    'FedAvg': '#1f77b4',\n    'FedAvg+Temp': '#aec7e8',\n    'FedSAM': '#ff7f0e',\n    'FedSAM+Temp': '#ffbb78',\n    'FedLESAM': '#2ca02c',\n    'FedLESAM+Temp': '#98df8a',\n    'FedSMOO': '#d62728',\n    'FedSMOO+Temp': '#ff9896',\n    'FedGMT': '#9467bd',\n    'FedGMT+Temp': '#c5b0d5',\n    'FedProx': '#8c564b',\n    'FedProx+Temp': '#c49c94'\n}\n\nline_styles = {\n    'FedAvg': '-',\n    'FedAvg+Temp': '--',\n    'FedSAM': '-',\n    'FedSAM+Temp': '--',\n    'FedLESAM': '-',\n    'FedLESAM+Temp': '--',\n    'FedSMOO': '-',\n    'FedSMOO+Temp': '--',\n    'FedGMT': '-',\n    'FedGMT+Temp': '--',\n    'FedProx': '-',\n    'FedProx+Temp': '--'\n}\n\n# Communication rounds (x-axis)\nrounds = np.arange(1, len(test_accs_fedavg) + 1)\n\n# Plot all algorithms\nplt.plot(rounds, test_accs_fedavg, \n         color=colors['FedAvg'], linestyle=line_styles['FedAvg'], \n         linewidth=2, label='FedAvg', marker='o', markersize=4, markevery=5)\n\nplt.plot(rounds, test_accs_fedavg_temp, \n         color=colors['FedAvg+Temp'], linestyle=line_styles['FedAvg+Temp'], \n         linewidth=2, label='FedAvg+Temp', marker='s', markersize=4, markevery=5)\n\nplt.plot(rounds, test_accs_fedsam, \n         color=colors['FedSAM'], linestyle=line_styles['FedSAM'], \n         linewidth=2, label='FedSAM', marker='o', markersize=4, markevery=5)\n\nplt.plot(rounds, test_accs_fedsam_temp, \n         color=colors['FedSAM+Temp'], linestyle=line_styles['FedSAM+Temp'], \n         linewidth=2, label='FedSAM+Temp', marker='s', markersize=4, markevery=5)\n\nplt.plot(rounds, test_accs_fedlesam, \n         color=colors['FedLESAM'], linestyle=line_styles['FedLESAM'], \n         linewidth=2, label='FedLESAM', marker='o', markersize=4, markevery=5)\n\nplt.plot(rounds, test_accs_fedlesam_temp, \n         color=colors['FedLESAM+Temp'], linestyle=line_styles['FedLESAM+Temp'], \n         linewidth=2, label='FedLESAM+Temp', marker='s', markersize=4, markevery=5)\n\nplt.plot(rounds, test_accs_fedsmoo, \n         color=colors['FedSMOO'], linestyle=line_styles['FedSMOO'], \n         linewidth=2, label='FedSMOO', marker='o', markersize=4, markevery=5)\n\nplt.plot(rounds, test_accs_fedsmoo_temp, \n         color=colors['FedSMOO+Temp'], linestyle=line_styles['FedSMOO+Temp'], \n         linewidth=2, label='FedSMOO+Temp', marker='s', markersize=4, markevery=5)\n\nplt.plot(rounds, test_accs_fedgmt, \n         color=colors['FedGMT'], linestyle=line_styles['FedGMT'], \n         linewidth=2, label='FedGMT', marker='o', markersize=4, markevery=5)\n\nplt.plot(rounds, test_accs_fedgmt_temp, \n         color=colors['FedGMT+Temp'], linestyle=line_styles['FedGMT+Temp'], \n         linewidth=2, label='FedGMT+Temp', marker='s', markersize=4, markevery=5)\n\nplt.plot(rounds, test_accs_fedprox, \n         color=colors['FedProx'], linestyle=line_styles['FedProx'], \n         linewidth=2, label='FedProx', marker='o', markersize=4, markevery=5)\n\nplt.plot(rounds, test_accs_fedprox_temp, \n         color=colors['FedProx+Temp'], linestyle=line_styles['FedProx+Temp'], \n         linewidth=2, label='FedProx+Temp', marker='s', markersize=4, markevery=5)\n\n# Formatting\nplt.xlabel('Communication Rounds', fontsize=14, fontweight='bold')\nplt.ylabel('Test Accuracy', fontsize=14, fontweight='bold')\nplt.title('Federated Learning Algorithm Comparison\\nTest Accuracy vs Communication Rounds', \n          fontsize=16, fontweight='bold', pad=20)\nplt.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)\nplt.legend(loc='lower right', fontsize=10, ncol=2, framealpha=0.95)\n\n# Set reasonable axis limits\nplt.xlim(1, len(test_accs_fedavg))\n\nplt.tight_layout()\nplt.show()\n\n# ========================================\n# PRINT FINAL ACCURACIES\n# ========================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"FINAL TEST ACCURACIES (Last Round)\")\nprint(\"=\"*80)\nprint(f\"FedAvg:          {test_accs_fedavg[-1]:.4f}\")\nprint(f\"FedAvg+Temp:     {test_accs_fedavg_temp[-1]:.4f}\")\nprint(f\"FedSAM:          {test_accs_fedsam[-1]:.4f}\")\nprint(f\"FedSAM+Temp:     {test_accs_fedsam_temp[-1]:.4f}\")\nprint(f\"FedLESAM:        {test_accs_fedlesam[-1]:.4f}\")\nprint(f\"FedLESAM+Temp:   {test_accs_fedlesam_temp[-1]:.4f}\")\nprint(f\"FedSMOO:         {test_accs_fedsmoo[-1]:.4f}\")\nprint(f\"FedSMOO+Temp:    {test_accs_fedsmoo_temp[-1]:.4f}\")\nprint(f\"FedGMT:          {test_accs_fedgmt[-1]:.4f}\")\nprint(f\"FedGMT+Temp:     {test_accs_fedgmt_temp[-1]:.4f}\")\nprint(f\"FedProx:         {test_accs_fedprox[-1]:.4f}\")\nprint(f\"FedProx+Temp:    {test_accs_fedprox_temp[-1]:.4f}\")\nprint(\"=\"*80)\n\n# Find best performing algorithm\nall_results = {\n    'FedAvg': test_accs_fedavg[-1],\n    'FedAvg+Temp': test_accs_fedavg_temp[-1],\n    'FedSAM': test_accs_fedsam[-1],\n    'FedSAM+Temp': test_accs_fedsam_temp[-1],\n    'FedLESAM': test_accs_fedlesam[-1],\n    'FedLESAM+Temp': test_accs_fedlesam_temp[-1],\n    'FedSMOO': test_accs_fedsmoo[-1],\n    'FedSMOO+Temp': test_accs_fedsmoo_temp[-1],\n    'FedGMT': test_accs_fedgmt[-1],vader\n    'FedGMT+Temp': test_accs_fedgmt_temp[-1],\n    'FedProx': test_accs_fedprox[-1],\n    'FedProx+Temp': test_accs_fedprox_temp[-1]\n}\n\nbest_algo = max(all_results, key=all_results.get)\nprint(f\"\\n🏆 BEST PERFORMING ALGORITHM: {best_algo} with {all_results[best_algo]:.4f} accuracy\")\nprint(\"=\"*80)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for alph in [0.05, 0.5, 5]:\n    print(\"#\"*80)\n    print(\"RUNNING EXPERIMENTS WITH ALPHA: \", alph)\n    print(\"#\"*80)\n    \n    train_loaders, val_loaders, test_loader, client_class_dist = load_and_partition_data(alpha=alph)\n\n    print(\"\\n[1/12] FedSMOO...\")\n    test_accs_fedsmoo, model_fedsmoo = run_fedsmoo()\n    \n    print(\"\\n[2/12] FedSMOO+Temp...\")\n    test_accs_fedsmoo_temp, tau_fedsmoo_temp, model_fedsmoo_temp = run_fedsmoo_temp()\n    \n    print(\"\\n[3/12] FedGMT...\")\n    test_accs_fedgmt, model_fedgmt = run_fedgmt()\n    \n    print(\"\\n[4/12] FedGMT+Temp...\")\n    test_accs_fedgmt_temp, tau_fedgmt_temp, model_fedgmt_temp = run_fedgmt_temp()\n    \n    continue\n    \n    import matplotlib.pyplot as plt\n    import numpy as np\n    \n    # ========================================\n    # PLOTTING ALL ALGORITHMS\n    # ========================================\n    \n    plt.figure(figsize=(14, 8))\n    \n    # Define colors and line styles for better distinction\n    colors = {\n        'FedAvg': '#1f77b4',\n        'FedAvg+Temp': '#aec7e8',\n        'FedSAM': '#ff7f0e',\n        'FedSAM+Temp': '#ffbb78',\n        'FedLESAM': '#2ca02c',\n        'FedLESAM+Temp': '#98df8a',\n        'FedSMOO': '#d62728',\n        'FedSMOO+Temp': '#ff9896',\n        'FedGMT': '#9467bd',\n        'FedGMT+Temp': '#c5b0d5',\n        'FedProx': '#8c564b',\n        'FedProx+Temp': '#c49c94'\n    }\n    \n    line_styles = {\n        'FedAvg': '-',\n        'FedAvg+Temp': '--',\n        'FedSAM': '-',\n        'FedSAM+Temp': '--',\n        'FedLESAM': '-',\n        'FedLESAM+Temp': '--',\n        'FedSMOO': '-',\n        'FedSMOO+Temp': '--',\n        'FedGMT': '-',\n        'FedGMT+Temp': '--',\n        'FedProx': '-',\n        'FedProx+Temp': '--'\n    }\n    \n    # Communication rounds (x-axis)\n    rounds = np.arange(1, len(test_accs_fedavg) + 1)\n    \n    # Plot all algorithms\n    plt.plot(rounds, test_accs_fedavg, \n             color=colors['FedAvg'], linestyle=line_styles['FedAvg'], \n             linewidth=2, label='FedAvg', marker='o', markersize=4, markevery=5)\n    \n    plt.plot(rounds, test_accs_fedavg_temp, \n             color=colors['FedAvg+Temp'], linestyle=line_styles['FedAvg+Temp'], \n             linewidth=2, label='FedAvg+Temp', marker='s', markersize=4, markevery=5)\n    \n    plt.plot(rounds, test_accs_fedsam, \n             color=colors['FedSAM'], linestyle=line_styles['FedSAM'], \n             linewidth=2, label='FedSAM', marker='o', markersize=4, markevery=5)\n    \n    plt.plot(rounds, test_accs_fedsam_temp, \n             color=colors['FedSAM+Temp'], linestyle=line_styles['FedSAM+Temp'], \n             linewidth=2, label='FedSAM+Temp', marker='s', markersize=4, markevery=5)\n    \n    plt.plot(rounds, test_accs_fedlesam, \n             color=colors['FedLESAM'], linestyle=line_styles['FedLESAM'], \n             linewidth=2, label='FedLESAM', marker='o', markersize=4, markevery=5)\n    \n    plt.plot(rounds, test_accs_fedlesam_temp, \n             color=colors['FedLESAM+Temp'], linestyle=line_styles['FedLESAM+Temp'], \n             linewidth=2, label='FedLESAM+Temp', marker='s', markersize=4, markevery=5)\n    \n    # plt.plot(rounds, test_accs_fedsmoo, \n    #          color=colors['FedSMOO'], linestyle=line_styles['FedSMOO'], \n    #          linewidth=2, label='FedSMOO', marker='o', markersize=4, markevery=5)\n    \n    # plt.plot(rounds, test_accs_fedsmoo_temp, \n    #          color=colors['FedSMOO+Temp'], linestyle=line_styles['FedSMOO+Temp'], \n    #          linewidth=2, label='FedSMOO+Temp', marker='s', markersize=4, markevery=5)\n    \n    # plt.plot(rounds, test_accs_fedgmt, \n    #          color=colors['FedGMT'], linestyle=line_styles['FedGMT'], \n    #          linewidth=2, label='FedGMT', marker='o', markersize=4, markevery=5)\n    \n    # plt.plot(rounds, test_accs_fedgmt_temp, \n    #          color=colors['FedGMT+Temp'], linestyle=line_styles['FedGMT+Temp'], \n    #          linewidth=2, label='FedGMT+Temp', marker='s', markersize=4, markevery=5)\n    \n    plt.plot(rounds, test_accs_fedprox, \n             color=colors['FedProx'], linestyle=line_styles['FedProx'], \n             linewidth=2, label='FedProx', marker='o', markersize=4, markevery=5)\n    \n    plt.plot(rounds, test_accs_fedprox_temp, \n             color=colors['FedProx+Temp'], linestyle=line_styles['FedProx+Temp'], \n             linewidth=2, label='FedProx+Temp', marker='s', markersize=4, markevery=5)\n    \n    # Formatting\n    plt.xlabel('Communication Rounds', fontsize=14, fontweight='bold')\n    plt.ylabel('Test Accuracy', fontsize=14, fontweight='bold')\n    plt.title('Federated Learning Algorithm Comparison\\nTest Accuracy vs Communication Rounds', \n              fontsize=16, fontweight='bold', pad=20)\n    plt.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)\n    plt.legend(loc='lower right', fontsize=10, ncol=2, framealpha=0.95)\n    \n    # Set reasonable axis limits\n    plt.xlim(1, len(test_accs_fedavg))\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # ========================================\n    # PRINT FINAL ACCURACIES\n    # ========================================\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"FINAL TEST ACCURACIES (Last Round)\")\n    print(\"=\"*80)\n    print(f\"FedAvg:          {test_accs_fedavg[-1]:.4f}\")\n    print(f\"FedAvg+Temp:     {test_accs_fedavg_temp[-1]:.4f}\")\n    print(f\"FedSAM:          {test_accs_fedsam[-1]:.4f}\")\n    print(f\"FedSAM+Temp:     {test_accs_fedsam_temp[-1]:.4f}\")\n    print(f\"FedLESAM:        {test_accs_fedlesam[-1]:.4f}\")\n    print(f\"FedLESAM+Temp:   {test_accs_fedlesam_temp[-1]:.4f}\")\n    # print(f\"FedSMOO:         {test_accs_fedsmoo[-1]:.4f}\")\n    # print(f\"FedSMOO+Temp:    {test_accs_fedsmoo_temp[-1]:.4f}\")\n    # print(f\"FedGMT:          {test_accs_fedgmt[-1]:.4f}\")\n    # print(f\"FedGMT+Temp:     {test_accs_fedgmt_temp[-1]:.4f}\")\n    print(f\"FedProx:         {test_accs_fedprox[-1]:.4f}\")\n    print(f\"FedProx+Temp:    {test_accs_fedprox_temp[-1]:.4f}\")\n    print(\"=\"*80)\n    \n    # Find best performing algorithm\n    all_results = {\n        'FedAvg': test_accs_fedavg[-1],\n        'FedAvg+Temp': test_accs_fedavg_temp[-1],\n        'FedSAM': test_accs_fedsam[-1],\n        'FedSAM+Temp': test_accs_fedsam_temp[-1],\n        'FedLESAM': test_accs_fedlesam[-1],\n        'FedLESAM+Temp': test_accs_fedlesam_temp[-1],\n        # 'FedSMOO': test_accs_fedsmoo[-1],\n        # 'FedSMOO+Temp': test_accs_fedsmoo_temp[-1],\n        # 'FedGMT': test_accs_fedgmt[-1],\n        # 'FedGMT+Temp': test_accs_fedgmt_temp[-1],\n        'FedProx': test_accs_fedprox[-1],\n        'FedProx+Temp': test_accs_fedprox_temp[-1]\n    }\n    \n    best_algo = max(all_results, key=all_results.get)\n    print(f\"\\n🏆 BEST PERFORMING ALGORITHM: {best_algo} with {all_results[best_algo]:.4f} accuracy\")\n    print(\"=\"*80)\n    ","metadata":{"execution":{"iopub.status.busy":"2025-12-27T13:07:54.887396Z","iopub.execute_input":"2025-12-27T13:07:54.888101Z","iopub.status.idle":"2025-12-27T15:25:50.319865Z","shell.execute_reply.started":"2025-12-27T13:07:54.888071Z","shell.execute_reply":"2025-12-27T15:25:50.319078Z"},"trusted":true},"outputs":[{"name":"stdout","text":"################################################################################\nRUNNING EXPERIMENTS WITH ALPHA:  0.05\n################################################################################\nData partitioning complete.\n================================================================================\nClient 0: 24 classes present | Samples: 3949 train, 775 test\n  Top 5 classes: [('Class 83', '0.137'), ('Class 97', '0.124'), ('Class 99', '0.122'), ('Class 67', '0.107'), ('Class 89', '0.081')]\nClient 1: 27 classes present | Samples: 5123 train, 1016 test\n  Top 5 classes: [('Class 32', '0.100'), ('Class 93', '0.100'), ('Class 71', '0.096'), ('Class 50', '0.092'), ('Class 86', '0.082')]\nClient 2: 26 classes present | Samples: 5052 train, 1005 test\n  Top 5 classes: [('Class 21', '0.109'), ('Class 72', '0.103'), ('Class 47', '0.097'), ('Class 59', '0.093'), ('Class 73', '0.089')]\nClient 3: 25 classes present | Samples: 5281 train, 1050 test\n  Top 5 classes: [('Class 43', '0.106'), ('Class 3', '0.095'), ('Class 64', '0.087'), ('Class 69', '0.076'), ('Class 58', '0.066')]\nClient 4: 24 classes present | Samples: 4511 train, 896 test\n  Top 5 classes: [('Class 6', '0.113'), ('Class 90', '0.104'), ('Class 4', '0.100'), ('Class 75', '0.100'), ('Class 52', '0.095')]\nClient 5: 21 classes present | Samples: 5240 train, 1041 test\n  Top 5 classes: [('Class 81', '0.116'), ('Class 9', '0.107'), ('Class 13', '0.094'), ('Class 42', '0.092'), ('Class 14', '0.086')]\nClient 6: 24 classes present | Samples: 5434 train, 1086 test\n  Top 5 classes: [('Class 53', '0.116'), ('Class 98', '0.105'), ('Class 77', '0.099'), ('Class 91', '0.096'), ('Class 44', '0.083')]\nClient 7: 28 classes present | Samples: 5107 train, 1030 test\n  Top 5 classes: [('Class 2', '0.114'), ('Class 94', '0.102'), ('Class 96', '0.092'), ('Class 66', '0.088'), ('Class 56', '0.082')]\nClient 8: 21 classes present | Samples: 5218 train, 1041 test\n  Top 5 classes: [('Class 12', '0.113'), ('Class 36', '0.098'), ('Class 23', '0.096'), ('Class 37', '0.094'), ('Class 49', '0.090')]\nClient 9: 27 classes present | Samples: 5085 train, 1060 test\n  Top 5 classes: [('Class 39', '0.104'), ('Class 10', '0.094'), ('Class 20', '0.093'), ('Class 51', '0.091'), ('Class 1', '0.083')]\n================================================================================\n\n[1/12] FedSMOO...\n\n============================================================\nSTARTING FedSMOO TRAINING\n============================================================\n[FedSMOO] R1/50 Test Acc: 0.0492 | Time: 16.26s\n[FedSMOO] R2/50 Test Acc: 0.0872 | Time: 16.53s\n[FedSMOO] R3/50 Test Acc: 0.1051 | Time: 16.59s\n[FedSMOO] R4/50 Test Acc: 0.1417 | Time: 16.26s\n[FedSMOO] R5/50 Test Acc: 0.1612 | Time: 16.51s\n[FedSMOO] R6/50 Test Acc: 0.1794 | Time: 16.34s\n[FedSMOO] R7/50 Test Acc: 0.1871 | Time: 16.31s\n[FedSMOO] R8/50 Test Acc: 0.2114 | Time: 16.33s\n[FedSMOO] R9/50 Test Acc: 0.2146 | Time: 16.53s\n[FedSMOO] R10/50 Test Acc: 0.2239 | Time: 16.45s\n[FedSMOO] R11/50 Test Acc: 0.2276 | Time: 16.27s\n[FedSMOO] R12/50 Test Acc: 0.2335 | Time: 16.45s\n[FedSMOO] R13/50 Test Acc: 0.2317 | Time: 16.34s\n[FedSMOO] R14/50 Test Acc: 0.2330 | Time: 16.35s\n[FedSMOO] R15/50 Test Acc: 0.2380 | Time: 16.24s\n[FedSMOO] R16/50 Test Acc: 0.2456 | Time: 16.25s\n[FedSMOO] R17/50 Test Acc: 0.2421 | Time: 16.44s\n[FedSMOO] R18/50 Test Acc: 0.2485 | Time: 16.28s\n[FedSMOO] R19/50 Test Acc: 0.2468 | Time: 16.40s\n[FedSMOO] R20/50 Test Acc: 0.2521 | Time: 16.29s\n[FedSMOO] R21/50 Test Acc: 0.2517 | Time: 16.36s\n[FedSMOO] R22/50 Test Acc: 0.2550 | Time: 16.37s\n[FedSMOO] R23/50 Test Acc: 0.2510 | Time: 16.41s\n[FedSMOO] R24/50 Test Acc: 0.2602 | Time: 16.32s\n[FedSMOO] R25/50 Test Acc: 0.2614 | Time: 16.45s\n[FedSMOO] R26/50 Test Acc: 0.2600 | Time: 16.34s\n[FedSMOO] R27/50 Test Acc: 0.2609 | Time: 16.21s\n[FedSMOO] R28/50 Test Acc: 0.2637 | Time: 16.24s\n[FedSMOO] R29/50 Test Acc: 0.2641 | Time: 16.25s\n[FedSMOO] R30/50 Test Acc: 0.2650 | Time: 16.21s\n[FedSMOO] R31/50 Test Acc: 0.2648 | Time: 16.46s\n[FedSMOO] R32/50 Test Acc: 0.2727 | Time: 16.26s\n[FedSMOO] R33/50 Test Acc: 0.2695 | Time: 16.25s\n[FedSMOO] R34/50 Test Acc: 0.2738 | Time: 16.35s\n[FedSMOO] R35/50 Test Acc: 0.2761 | Time: 16.31s\n[FedSMOO] R36/50 Test Acc: 0.2749 | Time: 16.38s\n[FedSMOO] R37/50 Test Acc: 0.2707 | Time: 16.41s\n[FedSMOO] R38/50 Test Acc: 0.2731 | Time: 16.33s\n[FedSMOO] R39/50 Test Acc: 0.2809 | Time: 16.38s\n[FedSMOO] R40/50 Test Acc: 0.2761 | Time: 16.33s\n[FedSMOO] R41/50 Test Acc: 0.2770 | Time: 16.51s\n[FedSMOO] R42/50 Test Acc: 0.2800 | Time: 16.35s\n[FedSMOO] R43/50 Test Acc: 0.2769 | Time: 16.31s\n[FedSMOO] R44/50 Test Acc: 0.2843 | Time: 16.43s\n[FedSMOO] R45/50 Test Acc: 0.2815 | Time: 16.31s\n[FedSMOO] R46/50 Test Acc: 0.2817 | Time: 16.32s\n[FedSMOO] R47/50 Test Acc: 0.2803 | Time: 16.20s\n[FedSMOO] R48/50 Test Acc: 0.2844 | Time: 16.31s\n[FedSMOO] R49/50 Test Acc: 0.2858 | Time: 16.35s\n[FedSMOO] R50/50 Test Acc: 0.2813 | Time: 16.33s\n\n[2/12] FedSMOO+Temp...\n\n============================================================\nSTARTING FedSMOO + TEMP TRAINING\n============================================================\n[FedSMOO+Temp] R1/50 Test Acc: 0.0512 | Avg τ: 0.5592 | Time: 16.61s\n[FedSMOO+Temp] R2/50 Test Acc: 0.0721 | Avg τ: 0.4828 | Time: 16.62s\n[FedSMOO+Temp] R3/50 Test Acc: 0.0979 | Avg τ: 0.4509 | Time: 16.66s\n[FedSMOO+Temp] R4/50 Test Acc: 0.1405 | Avg τ: 0.3978 | Time: 16.59s\n[FedSMOO+Temp] R5/50 Test Acc: 0.1672 | Avg τ: 0.3778 | Time: 16.62s\n[FedSMOO+Temp] R6/50 Test Acc: 0.1863 | Avg τ: 0.3353 | Time: 16.60s\n[FedSMOO+Temp] R7/50 Test Acc: 0.2017 | Avg τ: 0.3241 | Time: 16.72s\n[FedSMOO+Temp] R8/50 Test Acc: 0.2076 | Avg τ: 0.3032 | Time: 16.70s\n[FedSMOO+Temp] R9/50 Test Acc: 0.2317 | Avg τ: 0.2938 | Time: 16.75s\n[FedSMOO+Temp] R10/50 Test Acc: 0.2266 | Avg τ: 0.2775 | Time: 16.65s\n[FedSMOO+Temp] R11/50 Test Acc: 0.2341 | Avg τ: 0.2855 | Time: 16.76s\n[FedSMOO+Temp] R12/50 Test Acc: 0.2484 | Avg τ: 0.2737 | Time: 16.58s\n[FedSMOO+Temp] R13/50 Test Acc: 0.2499 | Avg τ: 0.2715 | Time: 16.70s\n[FedSMOO+Temp] R14/50 Test Acc: 0.2581 | Avg τ: 0.2697 | Time: 16.63s\n[FedSMOO+Temp] R15/50 Test Acc: 0.2608 | Avg τ: 0.2712 | Time: 16.58s\n[FedSMOO+Temp] R16/50 Test Acc: 0.2520 | Avg τ: 0.2687 | Time: 16.56s\n[FedSMOO+Temp] R17/50 Test Acc: 0.2581 | Avg τ: 0.2616 | Time: 16.71s\n[FedSMOO+Temp] R18/50 Test Acc: 0.2681 | Avg τ: 0.2540 | Time: 16.57s\n[FedSMOO+Temp] R19/50 Test Acc: 0.2632 | Avg τ: 0.2584 | Time: 16.81s\n[FedSMOO+Temp] R20/50 Test Acc: 0.2608 | Avg τ: 0.2501 | Time: 16.72s\n[FedSMOO+Temp] R21/50 Test Acc: 0.2695 | Avg τ: 0.2498 | Time: 16.65s\n[FedSMOO+Temp] R22/50 Test Acc: 0.2746 | Avg τ: 0.2552 | Time: 16.56s\n[FedSMOO+Temp] R23/50 Test Acc: 0.2702 | Avg τ: 0.2434 | Time: 16.68s\n[FedSMOO+Temp] R24/50 Test Acc: 0.2688 | Avg τ: 0.2565 | Time: 16.61s\n[FedSMOO+Temp] R25/50 Test Acc: 0.2672 | Avg τ: 0.2578 | Time: 16.84s\n[FedSMOO+Temp] R26/50 Test Acc: 0.2695 | Avg τ: 0.2597 | Time: 16.64s\n[FedSMOO+Temp] R27/50 Test Acc: 0.2720 | Avg τ: 0.2501 | Time: 16.56s\n[FedSMOO+Temp] R28/50 Test Acc: 0.2758 | Avg τ: 0.2593 | Time: 16.54s\n[FedSMOO+Temp] R29/50 Test Acc: 0.2758 | Avg τ: 0.2622 | Time: 16.79s\n[FedSMOO+Temp] R30/50 Test Acc: 0.2767 | Avg τ: 0.2640 | Time: 16.75s\n[FedSMOO+Temp] R31/50 Test Acc: 0.2785 | Avg τ: 0.2623 | Time: 16.57s\n[FedSMOO+Temp] R32/50 Test Acc: 0.2844 | Avg τ: 0.2645 | Time: 16.74s\n[FedSMOO+Temp] R33/50 Test Acc: 0.2777 | Avg τ: 0.2731 | Time: 16.85s\n[FedSMOO+Temp] R34/50 Test Acc: 0.2791 | Avg τ: 0.2652 | Time: 16.75s\n[FedSMOO+Temp] R35/50 Test Acc: 0.2774 | Avg τ: 0.2674 | Time: 16.71s\n[FedSMOO+Temp] R36/50 Test Acc: 0.2801 | Avg τ: 0.2641 | Time: 16.56s\n[FedSMOO+Temp] R37/50 Test Acc: 0.2860 | Avg τ: 0.2672 | Time: 16.71s\n[FedSMOO+Temp] R38/50 Test Acc: 0.2838 | Avg τ: 0.2642 | Time: 16.63s\n[FedSMOO+Temp] R39/50 Test Acc: 0.2815 | Avg τ: 0.2681 | Time: 16.63s\n[FedSMOO+Temp] R40/50 Test Acc: 0.2844 | Avg τ: 0.2752 | Time: 16.69s\n[FedSMOO+Temp] R41/50 Test Acc: 0.2796 | Avg τ: 0.2704 | Time: 16.86s\n[FedSMOO+Temp] R42/50 Test Acc: 0.2816 | Avg τ: 0.2729 | Time: 16.58s\n[FedSMOO+Temp] R43/50 Test Acc: 0.2770 | Avg τ: 0.2745 | Time: 16.61s\n[FedSMOO+Temp] R44/50 Test Acc: 0.2735 | Avg τ: 0.2744 | Time: 16.62s\n[FedSMOO+Temp] R45/50 Test Acc: 0.2717 | Avg τ: 0.2670 | Time: 16.57s\n[FedSMOO+Temp] R46/50 Test Acc: 0.2799 | Avg τ: 0.2678 | Time: 16.73s\n[FedSMOO+Temp] R47/50 Test Acc: 0.2802 | Avg τ: 0.2674 | Time: 16.74s\n[FedSMOO+Temp] R48/50 Test Acc: 0.2807 | Avg τ: 0.2687 | Time: 16.82s\n[FedSMOO+Temp] R49/50 Test Acc: 0.2770 | Avg τ: 0.2680 | Time: 16.79s\n[FedSMOO+Temp] R50/50 Test Acc: 0.2769 | Avg τ: 0.2670 | Time: 16.58s\n\n[3/12] FedGMT...\n\n============================================================\nSTARTING FedGMT TRAINING\n============================================================\n[FedGMT] R1/50 Test Acc: 0.0168 | Time: 11.50s\n[FedGMT] R2/50 Test Acc: 0.0829 | Time: 11.49s\n[FedGMT] R3/50 Test Acc: 0.1258 | Time: 11.56s\n[FedGMT] R4/50 Test Acc: 0.1414 | Time: 11.47s\n[FedGMT] R5/50 Test Acc: 0.1684 | Time: 11.71s\n[FedGMT] R6/50 Test Acc: 0.1754 | Time: 11.67s\n[FedGMT] R7/50 Test Acc: 0.1968 | Time: 11.84s\n[FedGMT] R8/50 Test Acc: 0.2015 | Time: 11.84s\n[FedGMT] R9/50 Test Acc: 0.2060 | Time: 11.96s\n[FedGMT] R10/50 Test Acc: 0.1978 | Time: 11.94s\n[FedGMT] R11/50 Test Acc: 0.2056 | Time: 11.82s\n[FedGMT] R12/50 Test Acc: 0.2015 | Time: 11.39s\n[FedGMT] R13/50 Test Acc: 0.1997 | Time: 11.71s\n[FedGMT] R14/50 Test Acc: 0.1958 | Time: 11.61s\n[FedGMT] R15/50 Test Acc: 0.2030 | Time: 11.57s\n[FedGMT] R16/50 Test Acc: 0.1937 | Time: 11.68s\n[FedGMT] R17/50 Test Acc: 0.1987 | Time: 11.74s\n[FedGMT] R18/50 Test Acc: 0.1961 | Time: 11.50s\n[FedGMT] R19/50 Test Acc: 0.2131 | Time: 11.65s\n[FedGMT] R20/50 Test Acc: 0.2238 | Time: 11.59s\n[FedGMT] R21/50 Test Acc: 0.2224 | Time: 11.76s\n[FedGMT] R22/50 Test Acc: 0.2254 | Time: 11.78s\n[FedGMT] R23/50 Test Acc: 0.2336 | Time: 11.77s\n[FedGMT] R24/50 Test Acc: 0.2310 | Time: 11.57s\n[FedGMT] R25/50 Test Acc: 0.2385 | Time: 11.44s\n[FedGMT] R26/50 Test Acc: 0.2395 | Time: 11.46s\n[FedGMT] R27/50 Test Acc: 0.2407 | Time: 11.71s\n[FedGMT] R28/50 Test Acc: 0.2401 | Time: 11.83s\n[FedGMT] R29/50 Test Acc: 0.2432 | Time: 11.49s\n[FedGMT] R30/50 Test Acc: 0.2448 | Time: 11.72s\n[FedGMT] R31/50 Test Acc: 0.2465 | Time: 11.69s\n[FedGMT] R32/50 Test Acc: 0.2465 | Time: 11.66s\n[FedGMT] R33/50 Test Acc: 0.2470 | Time: 11.48s\n[FedGMT] R34/50 Test Acc: 0.2494 | Time: 11.88s\n[FedGMT] R35/50 Test Acc: 0.2505 | Time: 11.61s\n[FedGMT] R36/50 Test Acc: 0.2482 | Time: 11.49s\n[FedGMT] R37/50 Test Acc: 0.2495 | Time: 11.52s\n[FedGMT] R38/50 Test Acc: 0.2499 | Time: 11.39s\n[FedGMT] R39/50 Test Acc: 0.2502 | Time: 11.73s\n[FedGMT] R40/50 Test Acc: 0.2451 | Time: 11.60s\n[FedGMT] R41/50 Test Acc: 0.2474 | Time: 11.50s\n[FedGMT] R42/50 Test Acc: 0.2499 | Time: 11.54s\n[FedGMT] R43/50 Test Acc: 0.2510 | Time: 11.43s\n[FedGMT] R44/50 Test Acc: 0.2539 | Time: 11.60s\n[FedGMT] R45/50 Test Acc: 0.2517 | Time: 11.65s\n[FedGMT] R46/50 Test Acc: 0.2538 | Time: 11.65s\n[FedGMT] R47/50 Test Acc: 0.2531 | Time: 11.61s\n[FedGMT] R48/50 Test Acc: 0.2530 | Time: 11.92s\n[FedGMT] R49/50 Test Acc: 0.2508 | Time: 11.66s\n[FedGMT] R50/50 Test Acc: 0.2522 | Time: 11.51s\n\n[4/12] FedGMT+Temp...\n\n============================================================\nSTARTING FedGMT + TEMP TRAINING\n============================================================\n[FedGMT+Temp] R1/50 Test Acc: 0.0243 | Avg τ: 0.6049 | Time: 12.16s\n[FedGMT+Temp] R2/50 Test Acc: 0.0895 | Avg τ: 0.4154 | Time: 12.19s\n[FedGMT+Temp] R3/50 Test Acc: 0.1139 | Avg τ: 0.3771 | Time: 12.19s\n[FedGMT+Temp] R4/50 Test Acc: 0.1407 | Avg τ: 0.3382 | Time: 11.84s\n[FedGMT+Temp] R5/50 Test Acc: 0.1562 | Avg τ: 0.3113 | Time: 12.00s\n[FedGMT+Temp] R6/50 Test Acc: 0.1816 | Avg τ: 0.2777 | Time: 11.95s\n[FedGMT+Temp] R7/50 Test Acc: 0.1875 | Avg τ: 0.2609 | Time: 11.88s\n[FedGMT+Temp] R8/50 Test Acc: 0.1932 | Avg τ: 0.2518 | Time: 12.18s\n[FedGMT+Temp] R9/50 Test Acc: 0.2106 | Avg τ: 0.2481 | Time: 12.14s\n[FedGMT+Temp] R10/50 Test Acc: 0.2263 | Avg τ: 0.2319 | Time: 11.83s\n[FedGMT+Temp] R11/50 Test Acc: 0.2157 | Avg τ: 0.2281 | Time: 12.07s\n[FedGMT+Temp] R12/50 Test Acc: 0.2280 | Avg τ: 0.2185 | Time: 12.19s\n[FedGMT+Temp] R13/50 Test Acc: 0.2231 | Avg τ: 0.2123 | Time: 12.02s\n[FedGMT+Temp] R14/50 Test Acc: 0.2237 | Avg τ: 0.2116 | Time: 11.88s\n[FedGMT+Temp] R15/50 Test Acc: 0.2314 | Avg τ: 0.2012 | Time: 11.82s\n[FedGMT+Temp] R16/50 Test Acc: 0.2312 | Avg τ: 0.1922 | Time: 12.09s\n[FedGMT+Temp] R17/50 Test Acc: 0.2346 | Avg τ: 0.1834 | Time: 11.99s\n[FedGMT+Temp] R18/50 Test Acc: 0.2358 | Avg τ: 0.1892 | Time: 11.92s\n[FedGMT+Temp] R19/50 Test Acc: 0.2430 | Avg τ: 0.1802 | Time: 12.02s\n[FedGMT+Temp] R20/50 Test Acc: 0.2346 | Avg τ: 0.1709 | Time: 11.84s\n[FedGMT+Temp] R21/50 Test Acc: 0.2439 | Avg τ: 0.1713 | Time: 11.82s\n[FedGMT+Temp] R22/50 Test Acc: 0.2433 | Avg τ: 0.1731 | Time: 11.99s\n[FedGMT+Temp] R23/50 Test Acc: 0.2421 | Avg τ: 0.1664 | Time: 11.83s\n[FedGMT+Temp] R24/50 Test Acc: 0.2301 | Avg τ: 0.1660 | Time: 11.99s\n[FedGMT+Temp] R25/50 Test Acc: 0.2476 | Avg τ: 0.1665 | Time: 11.92s\n[FedGMT+Temp] R26/50 Test Acc: 0.2431 | Avg τ: 0.1574 | Time: 12.15s\n[FedGMT+Temp] R27/50 Test Acc: 0.2451 | Avg τ: 0.1549 | Time: 11.80s\n[FedGMT+Temp] R28/50 Test Acc: 0.2465 | Avg τ: 0.1593 | Time: 11.92s\n[FedGMT+Temp] R29/50 Test Acc: 0.2446 | Avg τ: 0.1539 | Time: 11.76s\n[FedGMT+Temp] R30/50 Test Acc: 0.2411 | Avg τ: 0.1568 | Time: 11.98s\n[FedGMT+Temp] R31/50 Test Acc: 0.2327 | Avg τ: 0.1499 | Time: 12.10s\n[FedGMT+Temp] R32/50 Test Acc: 0.2328 | Avg τ: 0.1561 | Time: 12.09s\n[FedGMT+Temp] R33/50 Test Acc: 0.2408 | Avg τ: 0.1496 | Time: 12.12s\n[FedGMT+Temp] R34/50 Test Acc: 0.2474 | Avg τ: 0.1503 | Time: 12.06s\n[FedGMT+Temp] R35/50 Test Acc: 0.2511 | Avg τ: 0.1529 | Time: 11.94s\n[FedGMT+Temp] R36/50 Test Acc: 0.2478 | Avg τ: 0.1414 | Time: 11.85s\n[FedGMT+Temp] R37/50 Test Acc: 0.2474 | Avg τ: 0.1446 | Time: 11.88s\n[FedGMT+Temp] R38/50 Test Acc: 0.2387 | Avg τ: 0.1445 | Time: 11.91s\n[FedGMT+Temp] R39/50 Test Acc: 0.2425 | Avg τ: 0.1425 | Time: 12.00s\n[FedGMT+Temp] R40/50 Test Acc: 0.2446 | Avg τ: 0.1407 | Time: 11.79s\n[FedGMT+Temp] R41/50 Test Acc: 0.2471 | Avg τ: 0.1373 | Time: 11.91s\n[FedGMT+Temp] R42/50 Test Acc: 0.2440 | Avg τ: 0.1359 | Time: 11.77s\n[FedGMT+Temp] R43/50 Test Acc: 0.2451 | Avg τ: 0.1293 | Time: 11.85s\n[FedGMT+Temp] R44/50 Test Acc: 0.2440 | Avg τ: 0.1343 | Time: 11.86s\n[FedGMT+Temp] R45/50 Test Acc: 0.2353 | Avg τ: 0.1260 | Time: 11.83s\n[FedGMT+Temp] R46/50 Test Acc: 0.2422 | Avg τ: 0.1205 | Time: 12.06s\n[FedGMT+Temp] R47/50 Test Acc: 0.2431 | Avg τ: 0.1273 | Time: 11.78s\n[FedGMT+Temp] R48/50 Test Acc: 0.2463 | Avg τ: 0.1217 | Time: 11.99s\n[FedGMT+Temp] R49/50 Test Acc: 0.2450 | Avg τ: 0.1261 | Time: 11.95s\n[FedGMT+Temp] R50/50 Test Acc: 0.2383 | Avg τ: 0.1244 | Time: 12.02s\n################################################################################\nRUNNING EXPERIMENTS WITH ALPHA:  0.5\n################################################################################\nData partitioning complete.\n================================================================================\nClient 0: 62 classes present | Samples: 5003 train, 1010 test\n  Top 5 classes: [('Class 41', '0.058'), ('Class 83', '0.052'), ('Class 34', '0.050'), ('Class 31', '0.044'), ('Class 63', '0.042')]\nClient 1: 78 classes present | Samples: 5190 train, 1019 test\n  Top 5 classes: [('Class 99', '0.087'), ('Class 89', '0.064'), ('Class 43', '0.042'), ('Class 15', '0.037'), ('Class 24', '0.035')]\nClient 2: 67 classes present | Samples: 5053 train, 1023 test\n  Top 5 classes: [('Class 94', '0.063'), ('Class 58', '0.051'), ('Class 76', '0.051'), ('Class 98', '0.051'), ('Class 21', '0.042')]\nClient 3: 72 classes present | Samples: 5002 train, 1003 test\n  Top 5 classes: [('Class 3', '0.058'), ('Class 95', '0.052'), ('Class 44', '0.042'), ('Class 54', '0.042'), ('Class 78', '0.042')]\nClient 4: 83 classes present | Samples: 4875 train, 961 test\n  Top 5 classes: [('Class 96', '0.057'), ('Class 4', '0.045'), ('Class 87', '0.045'), ('Class 85', '0.039'), ('Class 81', '0.037')]\nClient 5: 62 classes present | Samples: 5013 train, 1003 test\n  Top 5 classes: [('Class 9', '0.052'), ('Class 59', '0.048'), ('Class 61', '0.046'), ('Class 68', '0.044'), ('Class 13', '0.040')]\nClient 6: 62 classes present | Samples: 5022 train, 1002 test\n  Top 5 classes: [('Class 37', '0.070'), ('Class 64', '0.058'), ('Class 40', '0.056'), ('Class 59', '0.052'), ('Class 18', '0.046')]\nClient 7: 74 classes present | Samples: 4646 train, 948 test\n  Top 5 classes: [('Class 97', '0.078'), ('Class 20', '0.043'), ('Class 54', '0.039'), ('Class 25', '0.034'), ('Class 70', '0.034')]\nClient 8: 62 classes present | Samples: 5117 train, 1021 test\n  Top 5 classes: [('Class 23', '0.072'), ('Class 12', '0.061'), ('Class 25', '0.047'), ('Class 73', '0.047'), ('Class 62', '0.045')]\nClient 9: 74 classes present | Samples: 5079 train, 1010 test\n  Top 5 classes: [('Class 91', '0.069'), ('Class 69', '0.065'), ('Class 38', '0.045'), ('Class 13', '0.039'), ('Class 92', '0.032')]\n================================================================================\n\n[1/12] FedSMOO...\n\n============================================================\nSTARTING FedSMOO TRAINING\n============================================================\n[FedSMOO] R1/50 Test Acc: 0.0656 | Time: 15.86s\n[FedSMOO] R2/50 Test Acc: 0.0969 | Time: 15.86s\n[FedSMOO] R3/50 Test Acc: 0.1191 | Time: 15.83s\n[FedSMOO] R4/50 Test Acc: 0.1439 | Time: 15.75s\n[FedSMOO] R5/50 Test Acc: 0.1650 | Time: 15.74s\n[FedSMOO] R6/50 Test Acc: 0.1897 | Time: 15.76s\n[FedSMOO] R7/50 Test Acc: 0.2031 | Time: 15.90s\n[FedSMOO] R8/50 Test Acc: 0.2245 | Time: 15.80s\n[FedSMOO] R9/50 Test Acc: 0.2533 | Time: 15.77s\n[FedSMOO] R10/50 Test Acc: 0.2571 | Time: 15.79s\n[FedSMOO] R11/50 Test Acc: 0.2630 | Time: 15.89s\n[FedSMOO] R12/50 Test Acc: 0.2633 | Time: 15.79s\n[FedSMOO] R13/50 Test Acc: 0.2694 | Time: 15.67s\n[FedSMOO] R14/50 Test Acc: 0.2658 | Time: 15.70s\n[FedSMOO] R15/50 Test Acc: 0.2712 | Time: 15.75s\n[FedSMOO] R16/50 Test Acc: 0.2765 | Time: 15.86s\n[FedSMOO] R17/50 Test Acc: 0.2729 | Time: 15.68s\n[FedSMOO] R18/50 Test Acc: 0.2711 | Time: 15.93s\n[FedSMOO] R19/50 Test Acc: 0.2762 | Time: 15.73s\n[FedSMOO] R20/50 Test Acc: 0.2753 | Time: 15.77s\n[FedSMOO] R21/50 Test Acc: 0.2767 | Time: 15.91s\n[FedSMOO] R22/50 Test Acc: 0.2723 | Time: 15.84s\n[FedSMOO] R23/50 Test Acc: 0.2798 | Time: 15.93s\n[FedSMOO] R24/50 Test Acc: 0.2770 | Time: 15.93s\n[FedSMOO] R25/50 Test Acc: 0.2698 | Time: 15.77s\n[FedSMOO] R26/50 Test Acc: 0.2744 | Time: 15.77s\n[FedSMOO] R27/50 Test Acc: 0.2815 | Time: 15.87s\n[FedSMOO] R28/50 Test Acc: 0.2831 | Time: 15.84s\n[FedSMOO] R29/50 Test Acc: 0.2849 | Time: 15.72s\n[FedSMOO] R30/50 Test Acc: 0.2821 | Time: 15.82s\n[FedSMOO] R31/50 Test Acc: 0.2830 | Time: 15.82s\n[FedSMOO] R32/50 Test Acc: 0.2850 | Time: 15.75s\n[FedSMOO] R33/50 Test Acc: 0.2785 | Time: 15.83s\n[FedSMOO] R34/50 Test Acc: 0.2892 | Time: 15.82s\n[FedSMOO] R35/50 Test Acc: 0.2852 | Time: 15.95s\n[FedSMOO] R36/50 Test Acc: 0.2889 | Time: 15.76s\n[FedSMOO] R37/50 Test Acc: 0.2902 | Time: 15.62s\n[FedSMOO] R38/50 Test Acc: 0.2900 | Time: 15.80s\n[FedSMOO] R39/50 Test Acc: 0.2912 | Time: 15.76s\n[FedSMOO] R40/50 Test Acc: 0.2904 | Time: 16.00s\n[FedSMOO] R41/50 Test Acc: 0.2896 | Time: 15.74s\n[FedSMOO] R42/50 Test Acc: 0.2910 | Time: 15.66s\n[FedSMOO] R43/50 Test Acc: 0.2912 | Time: 15.90s\n[FedSMOO] R44/50 Test Acc: 0.2956 | Time: 15.83s\n[FedSMOO] R45/50 Test Acc: 0.2942 | Time: 15.68s\n[FedSMOO] R46/50 Test Acc: 0.2906 | Time: 15.83s\n[FedSMOO] R47/50 Test Acc: 0.2917 | Time: 15.90s\n[FedSMOO] R48/50 Test Acc: 0.2953 | Time: 15.81s\n[FedSMOO] R49/50 Test Acc: 0.2962 | Time: 15.74s\n[FedSMOO] R50/50 Test Acc: 0.2985 | Time: 15.74s\n\n[2/12] FedSMOO+Temp...\n\n============================================================\nSTARTING FedSMOO + TEMP TRAINING\n============================================================\n[FedSMOO+Temp] R1/50 Test Acc: 0.0594 | Avg τ: 0.8763 | Time: 16.24s\n[FedSMOO+Temp] R2/50 Test Acc: 0.0981 | Avg τ: 0.5526 | Time: 16.29s\n[FedSMOO+Temp] R3/50 Test Acc: 0.1387 | Avg τ: 0.4722 | Time: 16.15s\n[FedSMOO+Temp] R4/50 Test Acc: 0.1694 | Avg τ: 0.4430 | Time: 16.04s\n[FedSMOO+Temp] R5/50 Test Acc: 0.1880 | Avg τ: 0.3794 | Time: 16.02s\n[FedSMOO+Temp] R6/50 Test Acc: 0.1991 | Avg τ: 0.3493 | Time: 16.27s\n[FedSMOO+Temp] R7/50 Test Acc: 0.2143 | Avg τ: 0.3093 | Time: 16.21s\n[FedSMOO+Temp] R8/50 Test Acc: 0.2443 | Avg τ: 0.2966 | Time: 16.28s\n[FedSMOO+Temp] R9/50 Test Acc: 0.2518 | Avg τ: 0.2917 | Time: 16.28s\n[FedSMOO+Temp] R10/50 Test Acc: 0.2713 | Avg τ: 0.2859 | Time: 15.98s\n[FedSMOO+Temp] R11/50 Test Acc: 0.2663 | Avg τ: 0.2846 | Time: 16.19s\n[FedSMOO+Temp] R12/50 Test Acc: 0.2660 | Avg τ: 0.2882 | Time: 16.20s\n[FedSMOO+Temp] R13/50 Test Acc: 0.2770 | Avg τ: 0.2988 | Time: 16.06s\n[FedSMOO+Temp] R14/50 Test Acc: 0.2738 | Avg τ: 0.2875 | Time: 15.97s\n[FedSMOO+Temp] R15/50 Test Acc: 0.2873 | Avg τ: 0.2872 | Time: 15.95s\n[FedSMOO+Temp] R16/50 Test Acc: 0.2833 | Avg τ: 0.2754 | Time: 16.01s\n[FedSMOO+Temp] R17/50 Test Acc: 0.2893 | Avg τ: 0.2793 | Time: 16.14s\n[FedSMOO+Temp] R18/50 Test Acc: 0.2852 | Avg τ: 0.2802 | Time: 16.08s\n[FedSMOO+Temp] R19/50 Test Acc: 0.2831 | Avg τ: 0.2725 | Time: 16.12s\n[FedSMOO+Temp] R20/50 Test Acc: 0.2876 | Avg τ: 0.2670 | Time: 16.09s\n[FedSMOO+Temp] R21/50 Test Acc: 0.2875 | Avg τ: 0.2776 | Time: 16.26s\n[FedSMOO+Temp] R22/50 Test Acc: 0.2866 | Avg τ: 0.2763 | Time: 16.22s\n[FedSMOO+Temp] R23/50 Test Acc: 0.2880 | Avg τ: 0.2781 | Time: 16.20s\n[FedSMOO+Temp] R24/50 Test Acc: 0.2922 | Avg τ: 0.2795 | Time: 16.21s\n[FedSMOO+Temp] R25/50 Test Acc: 0.2912 | Avg τ: 0.2730 | Time: 16.10s\n[FedSMOO+Temp] R26/50 Test Acc: 0.2922 | Avg τ: 0.2715 | Time: 16.00s\n[FedSMOO+Temp] R27/50 Test Acc: 0.2948 | Avg τ: 0.2759 | Time: 16.11s\n[FedSMOO+Temp] R28/50 Test Acc: 0.2945 | Avg τ: 0.2709 | Time: 16.36s\n[FedSMOO+Temp] R29/50 Test Acc: 0.2893 | Avg τ: 0.2686 | Time: 16.00s\n[FedSMOO+Temp] R30/50 Test Acc: 0.2962 | Avg τ: 0.2685 | Time: 16.04s\n[FedSMOO+Temp] R31/50 Test Acc: 0.2964 | Avg τ: 0.2724 | Time: 16.25s\n[FedSMOO+Temp] R32/50 Test Acc: 0.2955 | Avg τ: 0.2662 | Time: 16.17s\n[FedSMOO+Temp] R33/50 Test Acc: 0.2925 | Avg τ: 0.2691 | Time: 16.08s\n[FedSMOO+Temp] R34/50 Test Acc: 0.2917 | Avg τ: 0.2623 | Time: 16.03s\n[FedSMOO+Temp] R35/50 Test Acc: 0.2971 | Avg τ: 0.2673 | Time: 16.00s\n[FedSMOO+Temp] R36/50 Test Acc: 0.2920 | Avg τ: 0.2633 | Time: 16.21s\n[FedSMOO+Temp] R37/50 Test Acc: 0.2924 | Avg τ: 0.2571 | Time: 16.12s\n[FedSMOO+Temp] R38/50 Test Acc: 0.2952 | Avg τ: 0.2618 | Time: 16.10s\n[FedSMOO+Temp] R39/50 Test Acc: 0.2946 | Avg τ: 0.2620 | Time: 16.16s\n[FedSMOO+Temp] R40/50 Test Acc: 0.2943 | Avg τ: 0.2551 | Time: 16.01s\n[FedSMOO+Temp] R41/50 Test Acc: 0.2950 | Avg τ: 0.2557 | Time: 15.98s\n[FedSMOO+Temp] R42/50 Test Acc: 0.3011 | Avg τ: 0.2551 | Time: 16.18s\n[FedSMOO+Temp] R43/50 Test Acc: 0.3007 | Avg τ: 0.2512 | Time: 16.25s\n[FedSMOO+Temp] R44/50 Test Acc: 0.2987 | Avg τ: 0.2463 | Time: 16.06s\n[FedSMOO+Temp] R45/50 Test Acc: 0.2981 | Avg τ: 0.2436 | Time: 16.03s\n[FedSMOO+Temp] R46/50 Test Acc: 0.2988 | Avg τ: 0.2470 | Time: 16.13s\n[FedSMOO+Temp] R47/50 Test Acc: 0.2969 | Avg τ: 0.2432 | Time: 16.12s\n[FedSMOO+Temp] R48/50 Test Acc: 0.2979 | Avg τ: 0.2449 | Time: 16.22s\n[FedSMOO+Temp] R49/50 Test Acc: 0.2982 | Avg τ: 0.2451 | Time: 16.27s\n[FedSMOO+Temp] R50/50 Test Acc: 0.2987 | Avg τ: 0.2479 | Time: 16.10s\n\n[3/12] FedGMT...\n\n============================================================\nSTARTING FedGMT TRAINING\n============================================================\n[FedGMT] R1/50 Test Acc: 0.0302 | Time: 11.44s\n[FedGMT] R2/50 Test Acc: 0.0954 | Time: 11.16s\n[FedGMT] R3/50 Test Acc: 0.1286 | Time: 11.20s\n[FedGMT] R4/50 Test Acc: 0.1548 | Time: 11.15s\n[FedGMT] R5/50 Test Acc: 0.1752 | Time: 11.31s\n[FedGMT] R6/50 Test Acc: 0.1875 | Time: 11.35s\n[FedGMT] R7/50 Test Acc: 0.2148 | Time: 11.39s\n[FedGMT] R8/50 Test Acc: 0.2284 | Time: 11.27s\n[FedGMT] R9/50 Test Acc: 0.2383 | Time: 10.97s\n[FedGMT] R10/50 Test Acc: 0.2306 | Time: 11.33s\n[FedGMT] R11/50 Test Acc: 0.2342 | Time: 11.19s\n[FedGMT] R12/50 Test Acc: 0.2303 | Time: 11.35s\n[FedGMT] R13/50 Test Acc: 0.2311 | Time: 11.20s\n[FedGMT] R14/50 Test Acc: 0.2179 | Time: 11.16s\n[FedGMT] R15/50 Test Acc: 0.2262 | Time: 11.34s\n[FedGMT] R16/50 Test Acc: 0.2233 | Time: 11.30s\n[FedGMT] R17/50 Test Acc: 0.2259 | Time: 11.19s\n[FedGMT] R18/50 Test Acc: 0.2251 | Time: 11.20s\n[FedGMT] R19/50 Test Acc: 0.2273 | Time: 11.38s\n[FedGMT] R20/50 Test Acc: 0.2289 | Time: 11.10s\n[FedGMT] R21/50 Test Acc: 0.2324 | Time: 11.30s\n[FedGMT] R22/50 Test Acc: 0.2383 | Time: 11.28s\n[FedGMT] R23/50 Test Acc: 0.2361 | Time: 11.35s\n[FedGMT] R24/50 Test Acc: 0.2420 | Time: 11.12s\n[FedGMT] R25/50 Test Acc: 0.2519 | Time: 11.29s\n[FedGMT] R26/50 Test Acc: 0.2543 | Time: 11.37s\n[FedGMT] R27/50 Test Acc: 0.2568 | Time: 11.23s\n[FedGMT] R28/50 Test Acc: 0.2614 | Time: 11.38s\n[FedGMT] R29/50 Test Acc: 0.2575 | Time: 11.15s\n[FedGMT] R30/50 Test Acc: 0.2593 | Time: 11.10s\n[FedGMT] R31/50 Test Acc: 0.2599 | Time: 11.13s\n[FedGMT] R32/50 Test Acc: 0.2603 | Time: 11.24s\n[FedGMT] R33/50 Test Acc: 0.2614 | Time: 11.13s\n[FedGMT] R34/50 Test Acc: 0.2607 | Time: 11.38s\n[FedGMT] R35/50 Test Acc: 0.2619 | Time: 11.30s\n[FedGMT] R36/50 Test Acc: 0.2602 | Time: 11.16s\n[FedGMT] R37/50 Test Acc: 0.2623 | Time: 11.36s\n[FedGMT] R38/50 Test Acc: 0.2604 | Time: 11.19s\n[FedGMT] R39/50 Test Acc: 0.2601 | Time: 11.31s\n[FedGMT] R40/50 Test Acc: 0.2622 | Time: 11.25s\n[FedGMT] R41/50 Test Acc: 0.2615 | Time: 11.14s\n[FedGMT] R42/50 Test Acc: 0.2603 | Time: 11.24s\n[FedGMT] R43/50 Test Acc: 0.2616 | Time: 11.36s\n[FedGMT] R44/50 Test Acc: 0.2600 | Time: 11.30s\n[FedGMT] R45/50 Test Acc: 0.2612 | Time: 11.14s\n[FedGMT] R46/50 Test Acc: 0.2626 | Time: 11.18s\n[FedGMT] R47/50 Test Acc: 0.2623 | Time: 11.26s\n[FedGMT] R48/50 Test Acc: 0.2659 | Time: 11.24s\n[FedGMT] R49/50 Test Acc: 0.2640 | Time: 11.21s\n[FedGMT] R50/50 Test Acc: 0.2647 | Time: 10.99s\n\n[4/12] FedGMT+Temp...\n\n============================================================\nSTARTING FedGMT + TEMP TRAINING\n============================================================\n[FedGMT+Temp] R1/50 Test Acc: 0.0327 | Avg τ: 0.8054 | Time: 11.34s\n[FedGMT+Temp] R2/50 Test Acc: 0.0920 | Avg τ: 0.4178 | Time: 11.37s\n[FedGMT+Temp] R3/50 Test Acc: 0.1365 | Avg τ: 0.3826 | Time: 11.35s\n[FedGMT+Temp] R4/50 Test Acc: 0.1632 | Avg τ: 0.3360 | Time: 11.52s\n[FedGMT+Temp] R5/50 Test Acc: 0.1523 | Avg τ: 0.3048 | Time: 11.64s\n[FedGMT+Temp] R6/50 Test Acc: 0.1914 | Avg τ: 0.2825 | Time: 11.58s\n[FedGMT+Temp] R7/50 Test Acc: 0.2140 | Avg τ: 0.2606 | Time: 11.64s\n[FedGMT+Temp] R8/50 Test Acc: 0.2247 | Avg τ: 0.2583 | Time: 11.69s\n[FedGMT+Temp] R9/50 Test Acc: 0.2365 | Avg τ: 0.2518 | Time: 11.55s\n[FedGMT+Temp] R10/50 Test Acc: 0.2359 | Avg τ: 0.2343 | Time: 11.47s\n[FedGMT+Temp] R11/50 Test Acc: 0.2463 | Avg τ: 0.2375 | Time: 11.51s\n[FedGMT+Temp] R12/50 Test Acc: 0.2404 | Avg τ: 0.2403 | Time: 11.54s\n[FedGMT+Temp] R13/50 Test Acc: 0.2405 | Avg τ: 0.2238 | Time: 11.64s\n[FedGMT+Temp] R14/50 Test Acc: 0.2561 | Avg τ: 0.2213 | Time: 11.52s\n[FedGMT+Temp] R15/50 Test Acc: 0.2565 | Avg τ: 0.2109 | Time: 11.62s\n[FedGMT+Temp] R16/50 Test Acc: 0.2564 | Avg τ: 0.2068 | Time: 11.59s\n[FedGMT+Temp] R17/50 Test Acc: 0.2538 | Avg τ: 0.1934 | Time: 11.49s\n[FedGMT+Temp] R18/50 Test Acc: 0.2438 | Avg τ: 0.1876 | Time: 11.71s\n[FedGMT+Temp] R19/50 Test Acc: 0.2607 | Avg τ: 0.1865 | Time: 11.74s\n[FedGMT+Temp] R20/50 Test Acc: 0.2522 | Avg τ: 0.1802 | Time: 11.33s\n[FedGMT+Temp] R21/50 Test Acc: 0.2536 | Avg τ: 0.1837 | Time: 11.46s\n[FedGMT+Temp] R22/50 Test Acc: 0.2599 | Avg τ: 0.1696 | Time: 11.67s\n[FedGMT+Temp] R23/50 Test Acc: 0.2534 | Avg τ: 0.1698 | Time: 11.40s\n[FedGMT+Temp] R24/50 Test Acc: 0.2519 | Avg τ: 0.1652 | Time: 11.61s\n[FedGMT+Temp] R25/50 Test Acc: 0.2610 | Avg τ: 0.1600 | Time: 11.46s\n[FedGMT+Temp] R26/50 Test Acc: 0.2623 | Avg τ: 0.1556 | Time: 11.52s\n[FedGMT+Temp] R27/50 Test Acc: 0.2614 | Avg τ: 0.1515 | Time: 11.44s\n[FedGMT+Temp] R28/50 Test Acc: 0.2587 | Avg τ: 0.1479 | Time: 11.57s\n[FedGMT+Temp] R29/50 Test Acc: 0.2571 | Avg τ: 0.1443 | Time: 11.52s\n[FedGMT+Temp] R30/50 Test Acc: 0.2546 | Avg τ: 0.1436 | Time: 11.29s\n[FedGMT+Temp] R31/50 Test Acc: 0.2601 | Avg τ: 0.1400 | Time: 11.64s\n[FedGMT+Temp] R32/50 Test Acc: 0.2623 | Avg τ: 0.1339 | Time: 11.48s\n[FedGMT+Temp] R33/50 Test Acc: 0.2580 | Avg τ: 0.1311 | Time: 11.55s\n[FedGMT+Temp] R34/50 Test Acc: 0.2559 | Avg τ: 0.1315 | Time: 11.43s\n[FedGMT+Temp] R35/50 Test Acc: 0.2585 | Avg τ: 0.1290 | Time: 11.50s\n[FedGMT+Temp] R36/50 Test Acc: 0.2569 | Avg τ: 0.1228 | Time: 11.68s\n[FedGMT+Temp] R37/50 Test Acc: 0.2552 | Avg τ: 0.1276 | Time: 11.44s\n[FedGMT+Temp] R38/50 Test Acc: 0.2562 | Avg τ: 0.1188 | Time: 11.42s\n[FedGMT+Temp] R39/50 Test Acc: 0.2567 | Avg τ: 0.1190 | Time: 11.44s\n[FedGMT+Temp] R40/50 Test Acc: 0.2559 | Avg τ: 0.1181 | Time: 11.62s\n[FedGMT+Temp] R41/50 Test Acc: 0.2556 | Avg τ: 0.1166 | Time: 11.74s\n[FedGMT+Temp] R42/50 Test Acc: 0.2595 | Avg τ: 0.1137 | Time: 11.61s\n[FedGMT+Temp] R43/50 Test Acc: 0.2580 | Avg τ: 0.1151 | Time: 11.35s\n[FedGMT+Temp] R44/50 Test Acc: 0.2605 | Avg τ: 0.1111 | Time: 11.53s\n[FedGMT+Temp] R45/50 Test Acc: 0.2618 | Avg τ: 0.1086 | Time: 11.49s\n[FedGMT+Temp] R46/50 Test Acc: 0.2616 | Avg τ: 0.1083 | Time: 11.44s\n[FedGMT+Temp] R47/50 Test Acc: 0.2607 | Avg τ: 0.1074 | Time: 11.68s\n[FedGMT+Temp] R48/50 Test Acc: 0.2642 | Avg τ: 0.1072 | Time: 11.46s\n[FedGMT+Temp] R49/50 Test Acc: 0.2610 | Avg τ: 0.1049 | Time: 11.49s\n[FedGMT+Temp] R50/50 Test Acc: 0.2614 | Avg τ: 0.1039 | Time: 11.55s\n################################################################################\nRUNNING EXPERIMENTS WITH ALPHA:  5\n################################################################################\nData partitioning complete.\n================================================================================\nClient 0: 97 classes present | Samples: 5014 train, 972 test\n  Top 5 classes: [('Class 90', '0.032'), ('Class 99', '0.028'), ('Class 25', '0.026'), ('Class 12', '0.022'), ('Class 7', '0.020')]\nClient 1: 97 classes present | Samples: 4992 train, 999 test\n  Top 5 classes: [('Class 90', '0.026'), ('Class 61', '0.024'), ('Class 5', '0.022'), ('Class 44', '0.020'), ('Class 47', '0.020')]\nClient 2: 96 classes present | Samples: 4999 train, 1010 test\n  Top 5 classes: [('Class 35', '0.038'), ('Class 1', '0.030'), ('Class 96', '0.028'), ('Class 18', '0.026'), ('Class 94', '0.022')]\nClient 3: 98 classes present | Samples: 5005 train, 1006 test\n  Top 5 classes: [('Class 28', '0.026'), ('Class 98', '0.026'), ('Class 67', '0.024'), ('Class 1', '0.020'), ('Class 11', '0.020')]\nClient 4: 96 classes present | Samples: 4839 train, 975 test\n  Top 5 classes: [('Class 11', '0.025'), ('Class 56', '0.025'), ('Class 45', '0.021'), ('Class 66', '0.021'), ('Class 88', '0.021')]\nClient 5: 98 classes present | Samples: 5081 train, 1019 test\n  Top 5 classes: [('Class 98', '0.039'), ('Class 76', '0.024'), ('Class 21', '0.022'), ('Class 31', '0.020'), ('Class 34', '0.020')]\nClient 6: 89 classes present | Samples: 5041 train, 1003 test\n  Top 5 classes: [('Class 45', '0.032'), ('Class 17', '0.028'), ('Class 43', '0.028'), ('Class 19', '0.026'), ('Class 62', '0.024')]\nClient 7: 92 classes present | Samples: 5031 train, 1005 test\n  Top 5 classes: [('Class 40', '0.034'), ('Class 2', '0.024'), ('Class 16', '0.024'), ('Class 34', '0.024'), ('Class 92', '0.024')]\nClient 8: 96 classes present | Samples: 5046 train, 1010 test\n  Top 5 classes: [('Class 33', '0.028'), ('Class 15', '0.024'), ('Class 94', '0.024'), ('Class 93', '0.022'), ('Class 2', '0.020')]\nClient 9: 97 classes present | Samples: 4952 train, 1001 test\n  Top 5 classes: [('Class 93', '0.028'), ('Class 99', '0.028'), ('Class 14', '0.024'), ('Class 44', '0.024'), ('Class 87', '0.020')]\n================================================================================\n\n[1/12] FedSMOO...\n\n============================================================\nSTARTING FedSMOO TRAINING\n============================================================\n[FedSMOO] R1/50 Test Acc: 0.0592 | Time: 15.69s\n[FedSMOO] R2/50 Test Acc: 0.0801 | Time: 15.74s\n[FedSMOO] R3/50 Test Acc: 0.1198 | Time: 15.63s\n[FedSMOO] R4/50 Test Acc: 0.1500 | Time: 15.71s\n[FedSMOO] R5/50 Test Acc: 0.1737 | Time: 15.59s\n[FedSMOO] R6/50 Test Acc: 0.1852 | Time: 15.56s\n[FedSMOO] R7/50 Test Acc: 0.2076 | Time: 15.71s\n[FedSMOO] R8/50 Test Acc: 0.2183 | Time: 15.66s\n[FedSMOO] R9/50 Test Acc: 0.2253 | Time: 15.71s\n[FedSMOO] R10/50 Test Acc: 0.2570 | Time: 15.66s\n[FedSMOO] R11/50 Test Acc: 0.2656 | Time: 15.59s\n[FedSMOO] R12/50 Test Acc: 0.2696 | Time: 15.58s\n[FedSMOO] R13/50 Test Acc: 0.2730 | Time: 15.72s\n[FedSMOO] R14/50 Test Acc: 0.2728 | Time: 15.62s\n[FedSMOO] R15/50 Test Acc: 0.2738 | Time: 15.72s\n[FedSMOO] R16/50 Test Acc: 0.2702 | Time: 15.84s\n[FedSMOO] R17/50 Test Acc: 0.2696 | Time: 15.62s\n[FedSMOO] R18/50 Test Acc: 0.2701 | Time: 15.65s\n[FedSMOO] R19/50 Test Acc: 0.2774 | Time: 15.69s\n[FedSMOO] R20/50 Test Acc: 0.2650 | Time: 15.60s\n[FedSMOO] R21/50 Test Acc: 0.2725 | Time: 15.53s\n[FedSMOO] R22/50 Test Acc: 0.2773 | Time: 15.71s\n[FedSMOO] R23/50 Test Acc: 0.2752 | Time: 15.90s\n[FedSMOO] R24/50 Test Acc: 0.2735 | Time: 15.57s\n[FedSMOO] R25/50 Test Acc: 0.2760 | Time: 15.53s\n[FedSMOO] R26/50 Test Acc: 0.2721 | Time: 15.52s\n[FedSMOO] R27/50 Test Acc: 0.2745 | Time: 15.72s\n[FedSMOO] R28/50 Test Acc: 0.2794 | Time: 15.73s\n[FedSMOO] R29/50 Test Acc: 0.2778 | Time: 15.76s\n[FedSMOO] R30/50 Test Acc: 0.2833 | Time: 15.73s\n[FedSMOO] R31/50 Test Acc: 0.2823 | Time: 15.61s\n[FedSMOO] R32/50 Test Acc: 0.2790 | Time: 15.54s\n[FedSMOO] R33/50 Test Acc: 0.2826 | Time: 15.53s\n[FedSMOO] R34/50 Test Acc: 0.2822 | Time: 15.59s\n[FedSMOO] R35/50 Test Acc: 0.2873 | Time: 15.68s\n[FedSMOO] R36/50 Test Acc: 0.2829 | Time: 15.48s\n[FedSMOO] R37/50 Test Acc: 0.2849 | Time: 15.56s\n[FedSMOO] R38/50 Test Acc: 0.2867 | Time: 15.83s\n[FedSMOO] R39/50 Test Acc: 0.2869 | Time: 15.69s\n[FedSMOO] R40/50 Test Acc: 0.2885 | Time: 15.59s\n[FedSMOO] R41/50 Test Acc: 0.2872 | Time: 15.70s\n[FedSMOO] R42/50 Test Acc: 0.2882 | Time: 15.62s\n[FedSMOO] R43/50 Test Acc: 0.2904 | Time: 15.75s\n[FedSMOO] R44/50 Test Acc: 0.2907 | Time: 15.56s\n[FedSMOO] R45/50 Test Acc: 0.2905 | Time: 15.78s\n[FedSMOO] R46/50 Test Acc: 0.2912 | Time: 15.52s\n[FedSMOO] R47/50 Test Acc: 0.2930 | Time: 15.72s\n[FedSMOO] R48/50 Test Acc: 0.2924 | Time: 15.53s\n[FedSMOO] R49/50 Test Acc: 0.2953 | Time: 15.55s\n[FedSMOO] R50/50 Test Acc: 0.2936 | Time: 15.68s\n\n[2/12] FedSMOO+Temp...\n\n============================================================\nSTARTING FedSMOO + TEMP TRAINING\n============================================================\n[FedSMOO+Temp] R1/50 Test Acc: 0.0527 | Avg τ: 1.0306 | Time: 15.85s\n[FedSMOO+Temp] R2/50 Test Acc: 0.0777 | Avg τ: 0.8448 | Time: 16.01s\n[FedSMOO+Temp] R3/50 Test Acc: 0.1357 | Avg τ: 0.5489 | Time: 15.91s\n[FedSMOO+Temp] R4/50 Test Acc: 0.1568 | Avg τ: 0.4453 | Time: 15.96s\n[FedSMOO+Temp] R5/50 Test Acc: 0.1793 | Avg τ: 0.4101 | Time: 15.83s\n[FedSMOO+Temp] R6/50 Test Acc: 0.1916 | Avg τ: 0.3629 | Time: 15.83s\n[FedSMOO+Temp] R7/50 Test Acc: 0.2222 | Avg τ: 0.3459 | Time: 15.97s\n[FedSMOO+Temp] R8/50 Test Acc: 0.2400 | Avg τ: 0.3187 | Time: 16.16s\n[FedSMOO+Temp] R9/50 Test Acc: 0.2378 | Avg τ: 0.2970 | Time: 15.79s\n[FedSMOO+Temp] R10/50 Test Acc: 0.2653 | Avg τ: 0.2877 | Time: 15.88s\n[FedSMOO+Temp] R11/50 Test Acc: 0.2621 | Avg τ: 0.2875 | Time: 15.77s\n[FedSMOO+Temp] R12/50 Test Acc: 0.2745 | Avg τ: 0.3020 | Time: 15.87s\n[FedSMOO+Temp] R13/50 Test Acc: 0.2784 | Avg τ: 0.2914 | Time: 15.97s\n[FedSMOO+Temp] R14/50 Test Acc: 0.2805 | Avg τ: 0.2883 | Time: 15.78s\n[FedSMOO+Temp] R15/50 Test Acc: 0.2832 | Avg τ: 0.2972 | Time: 15.96s\n[FedSMOO+Temp] R16/50 Test Acc: 0.2811 | Avg τ: 0.2850 | Time: 15.80s\n[FedSMOO+Temp] R17/50 Test Acc: 0.2872 | Avg τ: 0.2969 | Time: 15.88s\n[FedSMOO+Temp] R18/50 Test Acc: 0.2793 | Avg τ: 0.2911 | Time: 15.83s\n[FedSMOO+Temp] R19/50 Test Acc: 0.2834 | Avg τ: 0.2843 | Time: 15.85s\n[FedSMOO+Temp] R20/50 Test Acc: 0.2875 | Avg τ: 0.2852 | Time: 15.87s\n[FedSMOO+Temp] R21/50 Test Acc: 0.2912 | Avg τ: 0.2842 | Time: 16.10s\n[FedSMOO+Temp] R22/50 Test Acc: 0.2875 | Avg τ: 0.2776 | Time: 15.80s\n[FedSMOO+Temp] R23/50 Test Acc: 0.2898 | Avg τ: 0.2835 | Time: 15.76s\n[FedSMOO+Temp] R24/50 Test Acc: 0.2931 | Avg τ: 0.2859 | Time: 15.78s\n[FedSMOO+Temp] R25/50 Test Acc: 0.2980 | Avg τ: 0.2750 | Time: 16.12s\n[FedSMOO+Temp] R26/50 Test Acc: 0.2951 | Avg τ: 0.2747 | Time: 16.00s\n[FedSMOO+Temp] R27/50 Test Acc: 0.2960 | Avg τ: 0.2722 | Time: 15.90s\n[FedSMOO+Temp] R28/50 Test Acc: 0.2983 | Avg τ: 0.2777 | Time: 15.82s\n[FedSMOO+Temp] R29/50 Test Acc: 0.2962 | Avg τ: 0.2719 | Time: 15.98s\n[FedSMOO+Temp] R30/50 Test Acc: 0.2970 | Avg τ: 0.2724 | Time: 16.04s\n[FedSMOO+Temp] R31/50 Test Acc: 0.2985 | Avg τ: 0.2673 | Time: 15.76s\n[FedSMOO+Temp] R32/50 Test Acc: 0.2968 | Avg τ: 0.2667 | Time: 15.97s\n[FedSMOO+Temp] R33/50 Test Acc: 0.2986 | Avg τ: 0.2667 | Time: 15.92s\n[FedSMOO+Temp] R34/50 Test Acc: 0.2974 | Avg τ: 0.2682 | Time: 16.03s\n[FedSMOO+Temp] R35/50 Test Acc: 0.2955 | Avg τ: 0.2625 | Time: 15.94s\n[FedSMOO+Temp] R36/50 Test Acc: 0.2978 | Avg τ: 0.2583 | Time: 15.83s\n[FedSMOO+Temp] R37/50 Test Acc: 0.2977 | Avg τ: 0.2559 | Time: 16.04s\n[FedSMOO+Temp] R38/50 Test Acc: 0.2998 | Avg τ: 0.2574 | Time: 16.03s\n[FedSMOO+Temp] R39/50 Test Acc: 0.2976 | Avg τ: 0.2596 | Time: 16.05s\n[FedSMOO+Temp] R40/50 Test Acc: 0.3010 | Avg τ: 0.2555 | Time: 15.94s\n[FedSMOO+Temp] R41/50 Test Acc: 0.2986 | Avg τ: 0.2508 | Time: 15.91s\n[FedSMOO+Temp] R42/50 Test Acc: 0.3000 | Avg τ: 0.2492 | Time: 15.94s\n[FedSMOO+Temp] R43/50 Test Acc: 0.3035 | Avg τ: 0.2524 | Time: 15.99s\n[FedSMOO+Temp] R44/50 Test Acc: 0.3022 | Avg τ: 0.2501 | Time: 15.74s\n[FedSMOO+Temp] R45/50 Test Acc: 0.3012 | Avg τ: 0.2475 | Time: 16.01s\n[FedSMOO+Temp] R46/50 Test Acc: 0.3042 | Avg τ: 0.2543 | Time: 16.00s\n[FedSMOO+Temp] R47/50 Test Acc: 0.3027 | Avg τ: 0.2468 | Time: 15.83s\n[FedSMOO+Temp] R48/50 Test Acc: 0.3020 | Avg τ: 0.2459 | Time: 15.87s\n[FedSMOO+Temp] R49/50 Test Acc: 0.2986 | Avg τ: 0.2454 | Time: 15.99s\n[FedSMOO+Temp] R50/50 Test Acc: 0.3014 | Avg τ: 0.2463 | Time: 16.04s\n\n[3/12] FedGMT...\n\n============================================================\nSTARTING FedGMT TRAINING\n============================================================\n[FedGMT] R1/50 Test Acc: 0.0312 | Time: 10.92s\n[FedGMT] R2/50 Test Acc: 0.0888 | Time: 10.98s\n[FedGMT] R3/50 Test Acc: 0.1254 | Time: 11.04s\n[FedGMT] R4/50 Test Acc: 0.1540 | Time: 10.94s\n[FedGMT] R5/50 Test Acc: 0.1741 | Time: 11.10s\n[FedGMT] R6/50 Test Acc: 0.1846 | Time: 11.11s\n[FedGMT] R7/50 Test Acc: 0.2071 | Time: 11.07s\n[FedGMT] R8/50 Test Acc: 0.2271 | Time: 11.15s\n[FedGMT] R9/50 Test Acc: 0.2335 | Time: 11.22s\n[FedGMT] R10/50 Test Acc: 0.2398 | Time: 11.00s\n[FedGMT] R11/50 Test Acc: 0.2396 | Time: 10.99s\n[FedGMT] R12/50 Test Acc: 0.2359 | Time: 11.23s\n[FedGMT] R13/50 Test Acc: 0.2370 | Time: 10.93s\n[FedGMT] R14/50 Test Acc: 0.2366 | Time: 11.14s\n[FedGMT] R15/50 Test Acc: 0.2290 | Time: 11.17s\n[FedGMT] R16/50 Test Acc: 0.2340 | Time: 11.08s\n[FedGMT] R17/50 Test Acc: 0.2334 | Time: 11.01s\n[FedGMT] R18/50 Test Acc: 0.2339 | Time: 11.01s\n[FedGMT] R19/50 Test Acc: 0.2272 | Time: 10.92s\n[FedGMT] R20/50 Test Acc: 0.2309 | Time: 10.97s\n[FedGMT] R21/50 Test Acc: 0.2318 | Time: 11.00s\n[FedGMT] R22/50 Test Acc: 0.2380 | Time: 10.92s\n[FedGMT] R23/50 Test Acc: 0.2400 | Time: 11.00s\n[FedGMT] R24/50 Test Acc: 0.2427 | Time: 11.08s\n[FedGMT] R25/50 Test Acc: 0.2477 | Time: 10.99s\n[FedGMT] R26/50 Test Acc: 0.2528 | Time: 11.03s\n[FedGMT] R27/50 Test Acc: 0.2529 | Time: 11.01s\n[FedGMT] R28/50 Test Acc: 0.2553 | Time: 11.00s\n[FedGMT] R29/50 Test Acc: 0.2534 | Time: 10.96s\n[FedGMT] R30/50 Test Acc: 0.2569 | Time: 11.06s\n[FedGMT] R31/50 Test Acc: 0.2573 | Time: 11.07s\n[FedGMT] R32/50 Test Acc: 0.2594 | Time: 11.06s\n[FedGMT] R33/50 Test Acc: 0.2565 | Time: 11.11s\n[FedGMT] R34/50 Test Acc: 0.2615 | Time: 10.98s\n[FedGMT] R35/50 Test Acc: 0.2652 | Time: 11.16s\n[FedGMT] R36/50 Test Acc: 0.2647 | Time: 11.08s\n[FedGMT] R37/50 Test Acc: 0.2633 | Time: 11.12s\n[FedGMT] R38/50 Test Acc: 0.2654 | Time: 11.13s\n[FedGMT] R39/50 Test Acc: 0.2658 | Time: 11.11s\n[FedGMT] R40/50 Test Acc: 0.2663 | Time: 11.01s\n[FedGMT] R41/50 Test Acc: 0.2666 | Time: 11.06s\n[FedGMT] R42/50 Test Acc: 0.2666 | Time: 11.10s\n[FedGMT] R43/50 Test Acc: 0.2680 | Time: 11.03s\n[FedGMT] R44/50 Test Acc: 0.2687 | Time: 11.09s\n[FedGMT] R45/50 Test Acc: 0.2682 | Time: 11.20s\n[FedGMT] R46/50 Test Acc: 0.2668 | Time: 11.07s\n[FedGMT] R47/50 Test Acc: 0.2707 | Time: 11.08s\n[FedGMT] R48/50 Test Acc: 0.2678 | Time: 10.93s\n[FedGMT] R49/50 Test Acc: 0.2710 | Time: 10.94s\n[FedGMT] R50/50 Test Acc: 0.2695 | Time: 11.06s\n\n[4/12] FedGMT+Temp...\n\n============================================================\nSTARTING FedGMT + TEMP TRAINING\n============================================================\n[FedGMT+Temp] R1/50 Test Acc: 0.0419 | Avg τ: 0.9596 | Time: 11.34s\n[FedGMT+Temp] R2/50 Test Acc: 0.0939 | Avg τ: 0.4920 | Time: 11.50s\n[FedGMT+Temp] R3/50 Test Acc: 0.1400 | Avg τ: 0.4021 | Time: 11.28s\n[FedGMT+Temp] R4/50 Test Acc: 0.1399 | Avg τ: 0.3587 | Time: 11.44s\n[FedGMT+Temp] R5/50 Test Acc: 0.1371 | Avg τ: 0.3262 | Time: 11.45s\n[FedGMT+Temp] R6/50 Test Acc: 0.1928 | Avg τ: 0.2891 | Time: 11.37s\n[FedGMT+Temp] R7/50 Test Acc: 0.2239 | Avg τ: 0.2641 | Time: 11.46s\n[FedGMT+Temp] R8/50 Test Acc: 0.2288 | Avg τ: 0.2543 | Time: 11.52s\n[FedGMT+Temp] R9/50 Test Acc: 0.2184 | Avg τ: 0.2528 | Time: 11.58s\n[FedGMT+Temp] R10/50 Test Acc: 0.2491 | Avg τ: 0.2496 | Time: 11.49s\n[FedGMT+Temp] R11/50 Test Acc: 0.2448 | Avg τ: 0.2387 | Time: 11.19s\n[FedGMT+Temp] R12/50 Test Acc: 0.2493 | Avg τ: 0.2316 | Time: 11.37s\n[FedGMT+Temp] R13/50 Test Acc: 0.2651 | Avg τ: 0.2298 | Time: 11.61s\n[FedGMT+Temp] R14/50 Test Acc: 0.2575 | Avg τ: 0.2298 | Time: 11.27s\n[FedGMT+Temp] R15/50 Test Acc: 0.2474 | Avg τ: 0.2064 | Time: 11.40s\n[FedGMT+Temp] R16/50 Test Acc: 0.2564 | Avg τ: 0.2120 | Time: 11.62s\n[FedGMT+Temp] R17/50 Test Acc: 0.2628 | Avg τ: 0.1986 | Time: 11.21s\n[FedGMT+Temp] R18/50 Test Acc: 0.2476 | Avg τ: 0.1911 | Time: 11.37s\n[FedGMT+Temp] R19/50 Test Acc: 0.2510 | Avg τ: 0.1836 | Time: 11.53s\n[FedGMT+Temp] R20/50 Test Acc: 0.2569 | Avg τ: 0.1824 | Time: 11.34s\n[FedGMT+Temp] R21/50 Test Acc: 0.2573 | Avg τ: 0.1748 | Time: 11.51s\n[FedGMT+Temp] R22/50 Test Acc: 0.2579 | Avg τ: 0.1681 | Time: 11.25s\n[FedGMT+Temp] R23/50 Test Acc: 0.2552 | Avg τ: 0.1603 | Time: 11.57s\n[FedGMT+Temp] R24/50 Test Acc: 0.2596 | Avg τ: 0.1587 | Time: 11.48s\n[FedGMT+Temp] R25/50 Test Acc: 0.2474 | Avg τ: 0.1532 | Time: 11.40s\n[FedGMT+Temp] R26/50 Test Acc: 0.2555 | Avg τ: 0.1470 | Time: 11.42s\n[FedGMT+Temp] R27/50 Test Acc: 0.2608 | Avg τ: 0.1396 | Time: 11.41s\n[FedGMT+Temp] R28/50 Test Acc: 0.2611 | Avg τ: 0.1375 | Time: 11.33s\n[FedGMT+Temp] R29/50 Test Acc: 0.2651 | Avg τ: 0.1375 | Time: 11.60s\n[FedGMT+Temp] R30/50 Test Acc: 0.2635 | Avg τ: 0.1250 | Time: 11.43s\n[FedGMT+Temp] R31/50 Test Acc: 0.2651 | Avg τ: 0.1254 | Time: 11.63s\n[FedGMT+Temp] R32/50 Test Acc: 0.2641 | Avg τ: 0.1248 | Time: 11.32s\n[FedGMT+Temp] R33/50 Test Acc: 0.2637 | Avg τ: 0.1208 | Time: 11.61s\n[FedGMT+Temp] R34/50 Test Acc: 0.2603 | Avg τ: 0.1186 | Time: 11.42s\n[FedGMT+Temp] R35/50 Test Acc: 0.2612 | Avg τ: 0.1172 | Time: 11.50s\n[FedGMT+Temp] R36/50 Test Acc: 0.2621 | Avg τ: 0.1128 | Time: 11.49s\n[FedGMT+Temp] R37/50 Test Acc: 0.2641 | Avg τ: 0.1125 | Time: 11.28s\n[FedGMT+Temp] R38/50 Test Acc: 0.2651 | Avg τ: 0.1111 | Time: 11.51s\n[FedGMT+Temp] R39/50 Test Acc: 0.2659 | Avg τ: 0.1096 | Time: 11.35s\n[FedGMT+Temp] R40/50 Test Acc: 0.2645 | Avg τ: 0.1065 | Time: 11.61s\n[FedGMT+Temp] R41/50 Test Acc: 0.2664 | Avg τ: 0.1069 | Time: 11.41s\n[FedGMT+Temp] R42/50 Test Acc: 0.2686 | Avg τ: 0.1055 | Time: 11.44s\n[FedGMT+Temp] R43/50 Test Acc: 0.2634 | Avg τ: 0.1027 | Time: 11.50s\n[FedGMT+Temp] R44/50 Test Acc: 0.2623 | Avg τ: 0.1027 | Time: 11.53s\n[FedGMT+Temp] R45/50 Test Acc: 0.2624 | Avg τ: 0.0988 | Time: 11.41s\n[FedGMT+Temp] R46/50 Test Acc: 0.2642 | Avg τ: 0.0990 | Time: 11.47s\n[FedGMT+Temp] R47/50 Test Acc: 0.2640 | Avg τ: 0.0981 | Time: 11.29s\n[FedGMT+Temp] R48/50 Test Acc: 0.2686 | Avg τ: 0.0978 | Time: 11.29s\n[FedGMT+Temp] R49/50 Test Acc: 0.2678 | Avg τ: 0.0949 | Time: 11.39s\n[FedGMT+Temp] R50/50 Test Acc: 0.2681 | Avg τ: 0.0961 | Time: 11.41s\n","output_type":"stream"}],"execution_count":12}]}