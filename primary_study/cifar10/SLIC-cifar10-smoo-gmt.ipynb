{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport numpy as np\nfrom torch.utils.data import DataLoader, Subset\nimport copy\nimport random\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport time\n\n# Reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\nNUM_CLASSES = 10\n\n# Hyperparameters\nBATCH_SIZE = 64\nLEARNING_RATE = 0.01\nLOCAL_EPOCHS = 5\nNUM_OF_CLIENTS = 10\nCOMM_ROUND = 50\nALPHA = 0.5\nFRAC = 0.1\nRHO = 0.05  # SAM perturbation radius (from paper)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"execution":{"iopub.status.busy":"2025-12-27T16:24:07.942993Z","iopub.execute_input":"2025-12-27T16:24:07.943906Z","iopub.status.idle":"2025-12-27T16:24:19.778770Z","shell.execute_reply.started":"2025-12-27T16:24:07.943858Z","shell.execute_reply":"2025-12-27T16:24:19.778077Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"class SimpleCNN(nn.Module):\n    def __init__(self, num_classes=NUM_CLASSES):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n        self.bn3 = nn.BatchNorm2d(128)\n        self.conv4 = nn.Conv2d(128, 128, 3, padding=1)\n        self.bn4 = nn.BatchNorm2d(128)\n        self.conv5 = nn.Conv2d(128, 256, 3, padding=1)\n        self.bn5 = nn.BatchNorm2d(256)\n        self.conv6 = nn.Conv2d(256, 256, 3, padding=1)\n        self.bn6 = nn.BatchNorm2d(256)\n        self.conv7 = nn.Conv2d(256, 512, 3, padding=1)\n        self.bn7 = nn.BatchNorm2d(512)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((2, 2))\n        self.fc1 = nn.Linear(512 * 2 * 2, 1024)\n        self.bn_fc1 = nn.BatchNorm1d(1024)\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(1024, 512)\n        self.bn_fc2 = nn.BatchNorm1d(512)\n        self.dropout2 = nn.Dropout(0.3)\n        self.fc3 = nn.Linear(512, num_classes)\n\n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = self.pool(x)\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = F.relu(self.bn4(self.conv4(x)))\n        x = self.pool(x)\n        x = F.relu(self.bn5(self.conv5(x)))\n        x = F.relu(self.bn6(self.conv6(x)))\n        x = self.pool(x)\n        x = F.relu(self.bn7(self.conv7(x)))\n        x = self.adaptive_pool(x)\n        x = x.view(x.size(0), -1)\n        f1 = F.relu(self.bn_fc1(self.fc1(x)))\n        f1 = self.dropout1(f1)\n        f2 = F.relu(self.bn_fc2(self.fc2(f1)))\n        f2 = self.dropout2(f2)\n        logits = self.fc3(f2)\n        return f2, logits  # features, logits\n\n\ndef load_and_partition_data(num_clients=NUM_OF_CLIENTS, alpha=ALPHA, batch_size=BATCH_SIZE, frac=FRAC, rand_seed=42):\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n\n    torch.manual_seed(rand_seed)\n    np.random.seed(rand_seed)\n\n    full_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n    test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n\n    y_train = np.array(full_dataset.targets)\n    y_test = np.array(test_dataset.targets)\n\n    net_dataidx_map = {}\n    net_dataidx_map_test = {}\n\n    min_size = 0\n    while min_size < 10:\n        idx_batch = [[] for _ in range(num_clients)]\n        idx_batch_test = [[] for _ in range(num_clients)]\n        for k in range(10):\n            idx_k = np.where(y_train == k)[0]\n            idx_k_test = np.where(y_test == k)[0]\n            np.random.shuffle(idx_k)\n            np.random.shuffle(idx_k_test)\n            proportions = np.random.dirichlet(np.repeat(alpha, num_clients))\n            proportions_train = np.array([p * (len(idx_j) < len(full_dataset)/num_clients) for p, idx_j in zip(proportions, idx_batch)])\n            proportions_test = np.array([p * (len(idx_j) < len(test_dataset)/num_clients) for p, idx_j in zip(proportions, idx_batch_test)])\n            proportions_train /= proportions_train.sum()\n            proportions_test /= proportions_test.sum()\n            split_train = (np.cumsum(proportions_train) * len(idx_k)).astype(int)[:-1]\n            split_test = (np.cumsum(proportions_test) * len(idx_k_test)).astype(int)[:-1]\n            idx_batch = [idx_j + idx.tolist() for idx_j, idx in zip(idx_batch, np.split(idx_k, split_train))]\n            idx_batch_test = [idx_j + idx.tolist() for idx_j, idx in zip(idx_batch_test, np.split(idx_k_test, split_test))]\n        min_size = min(len(l) for l in idx_batch)\n\n    for j in range(num_clients):\n        np.random.shuffle(idx_batch[j])\n        np.random.shuffle(idx_batch_test[j])\n        net_dataidx_map[j] = idx_batch[j]\n        net_dataidx_map_test[j] = idx_batch_test[j]\n\n    client_train_loaders = []\n    client_val_loaders = []\n    client_class_dist = []\n\n    for i in range(num_clients):\n        np.random.seed(rand_seed + i)\n        train_idx = np.random.choice(net_dataidx_map[i], int(frac * len(net_dataidx_map[i])), replace=False)\n        val_idx = np.random.choice(net_dataidx_map_test[i], int(min(2*frac,1.0)*len(net_dataidx_map_test[i])), replace=False)\n\n        client_labels = [y_train[k] for k in train_idx]\n        dist = {c: client_labels.count(c)/len(client_labels) if client_labels else 0 for c in range(10)}\n        client_class_dist.append(dist)\n\n        train_loader = DataLoader(Subset(full_dataset, train_idx), batch_size=batch_size,\n                                  shuffle=True, generator=torch.Generator().manual_seed(rand_seed+i), drop_last=True)\n        val_loader = DataLoader(Subset(test_dataset, val_idx), batch_size=batch_size,\n                                shuffle=True, generator=torch.Generator().manual_seed(rand_seed+i+num_clients), drop_last=True)\n        client_train_loaders.append(train_loader)\n        client_val_loaders.append(val_loader)\n\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n                             generator=torch.Generator().manual_seed(rand_seed + 2*num_clients + 1))\n\n    print(\"Data partitioning complete.\")\n    for i, d in enumerate(client_class_dist):\n        print(f\"Client {i} class dist: {[f'{c}:{d.get(c,0):.2f}' for c in range(10)]}\")\n\n    # return client_train_loaders, client_val_loaders, test_loader\n    return client_train_loaders, client_val_loaders, test_loader, client_class_dist\n\n# train_loaders, val_loaders, test_loader = load_and_partition_data()\n\ntrain_loaders, val_loaders, test_loader, client_class_dist = load_and_partition_data()\n\n# ========================================\n# 3. TEMPNET\n# ========================================\nclass TempNet(nn.Module):\n    def __init__(self, feature_dim=512, hidden_dim=128, tau_min=0.05, tau_max=2.0):\n        super().__init__()\n        self.fc1 = nn.Linear(feature_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, 1)\n        self.tau_min = tau_min\n        self.tau_max = tau_max\n\n    def forward(self, x):\n        h = F.relu(self.fc1(x))\n        raw = self.fc2(h)\n        tau = torch.sigmoid(raw)\n        tau = tau * (self.tau_max - self.tau_min) + self.tau_min\n        return tau.mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T16:24:21.905513Z","iopub.execute_input":"2025-12-27T16:24:21.906280Z","iopub.status.idle":"2025-12-27T16:24:27.909652Z","shell.execute_reply.started":"2025-12-27T16:24:21.906245Z","shell.execute_reply":"2025-12-27T16:24:27.908780Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 170M/170M [00:02<00:00, 64.3MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Data partitioning complete.\nClient 0 class dist: ['0:0.03', '1:0.01', '2:0.26', '3:0.23', '4:0.09', '5:0.00', '6:0.38', '7:0.00', '8:0.00', '9:0.00']\nClient 1 class dist: ['0:0.05', '1:0.00', '2:0.23', '3:0.06', '4:0.02', '5:0.00', '6:0.00', '7:0.06', '8:0.18', '9:0.40']\nClient 2 class dist: ['0:0.04', '1:0.02', '2:0.00', '3:0.20', '4:0.00', '5:0.70', '6:0.04', '7:0.00', '8:0.00', '9:0.00']\nClient 3 class dist: ['0:0.40', '1:0.60', '2:0.00', '3:0.00', '4:0.00', '5:0.00', '6:0.00', '7:0.00', '8:0.00', '9:0.00']\nClient 4 class dist: ['0:0.16', '1:0.09', '2:0.00', '3:0.06', '4:0.21', '5:0.06', '6:0.10', '7:0.31', '8:0.00', '9:0.00']\nClient 5 class dist: ['0:0.11', '1:0.07', '2:0.35', '3:0.12', '4:0.03', '5:0.17', '6:0.03', '7:0.00', '8:0.10', '9:0.02']\nClient 6 class dist: ['0:0.01', '1:0.06', '2:0.11', '3:0.01', '4:0.00', '5:0.06', '6:0.01', '7:0.53', '8:0.00', '9:0.22']\nClient 7 class dist: ['0:0.12', '1:0.02', '2:0.03', '3:0.18', '4:0.06', '5:0.05', '6:0.11', '7:0.13', '8:0.01', '9:0.29']\nClient 8 class dist: ['0:0.01', '1:0.02', '2:0.04', '3:0.01', '4:0.12', '5:0.01', '6:0.02', '7:0.13', '8:0.65', '9:0.00']\nClient 9 class dist: ['0:0.01', '1:0.07', '2:0.05', '3:0.01', '4:0.38', '5:0.00', '6:0.01', '7:0.10', '8:0.00', '9:0.36']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"class BaseClient:\n    \"\"\"Base class for federated learning clients\"\"\"\n    \n    def __init__(self, client_id, model, train_loader, val_loader, device, \n                 learning_rate=0.01, local_epochs=5, momentum=0.9):\n        self.client_id = client_id\n        self.model = model\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.device = device\n        self.learning_rate = learning_rate\n        self.local_epochs = local_epochs\n        self.momentum = momentum\n        self.loss_fn = nn.CrossEntropyLoss()\n        self.train_samples = 0\n    \n    def train(self):\n        raise NotImplementedError\n    \n    def set_parameters(self, model_state_dict):\n        \"\"\"Load model parameters from server\"\"\"\n        self.model.load_state_dict(model_state_dict)\n    \n    def get_parameters(self):\n        \"\"\"Get model parameters\"\"\"\n        return self.model.state_dict()\n    \n    def get_train_samples(self):\n        \"\"\"Get number of training samples\"\"\"\n        try:\n            return len(self.train_loader.dataset)\n        except:\n            return len(self.train_loader) * BATCH_SIZE\n\n\nclass BaseServer:\n    \"\"\"Base class for federated learning server\"\"\"\n    \n    def __init__(self, global_model, test_loader, num_clients, learning_rate=0.01, \n                 lr_decay=0.998, device='cuda'):\n        self.global_model = copy.deepcopy(global_model)\n        self.test_loader = test_loader\n        self.num_clients = num_clients\n        self.learning_rate = learning_rate\n        self.lr_decay = lr_decay\n        self.device = device\n        self.clients = []\n        self.loss_fn = nn.CrossEntropyLoss()\n    \n    def send_models(self, clients):\n        \"\"\"Send global model to clients\"\"\"\n        model_state = self.global_model.state_dict()\n        for client in clients:\n            client.set_parameters(model_state)\n    \n    def receive_models(self, clients):\n        \"\"\"Receive trained models from clients and aggregate\"\"\"\n        self.client_models = [client.get_parameters() for client in clients]\n        self.client_samples = [client.get_train_samples() for client in clients]\n    \n    def aggregate_parameters(self):\n        \"\"\"Aggregate client models using weighted averaging\n        \n        IMPORTANT: This method handles mixed dtypes in model state dict.\n        Batch normalization layers have both float parameters AND integer buffers \n        (like num_batches_tracked). We must convert to float for weighted averaging,\n        then convert back to original dtype to avoid RuntimeError.\n        \"\"\"\n        total_samples = sum(self.client_samples)\n        avg_state = {}\n        \n        # Get first model as reference\n        for key in self.client_models[0].keys():\n            # Get original dtype (may be float32, float64, or int64)\n            first_tensor = self.client_models[0][key]\n            original_dtype = first_tensor.dtype\n            \n            # Initialize accumulator with zeros, using float32 for safe averaging\n            avg_state[key] = torch.zeros_like(first_tensor, dtype=torch.float32)\n            \n            # Weighted average: convert each tensor to float before summing\n            for model_state, num_samples in zip(self.client_models, self.client_samples):\n                weight = num_samples / total_samples\n                # .float() converts int64 buffers to float32, allows weighted addition\n                avg_state[key] += weight * model_state[key].float()\n            \n            # Convert result back to original dtype\n            # Float params stay float, int buffers become int again\n            avg_state[key] = avg_state[key].to(original_dtype)\n        \n        self.global_model.load_state_dict(avg_state)\n    \n    def evaluate(self):\n        \"\"\"Evaluate global model on test data\"\"\"\n        self.global_model.eval()\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for X, y in self.test_loader:\n                X, y = X.to(self.device), y.to(self.device)\n                output = self.model_forward(X)\n                if isinstance(output, tuple):\n                    _, logits = output\n                else:\n                    logits = output\n                _, predicted = torch.max(logits, 1)\n                total += y.size(0)\n                correct += (predicted == y).sum().item()\n        \n        accuracy = correct / total\n        return accuracy\n    \n    def model_forward(self, X):\n        \"\"\"Forward pass through model\"\"\"\n        return self.global_model(X)\n\n\n# ========================================\n# FEDAVG IMPLEMENTATION\n# ========================================\n\nclass ClientFedAvg(BaseClient):\n    \"\"\"Standard FedAvg client using SGD\"\"\"\n    \n    def train(self):\n        self.model.train()\n        optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate)\n        \n        for epoch in range(self.local_epochs):\n            for X, y in self.train_loader:\n                X, y = X.to(self.device), y.to(self.device)\n                \n                optimizer.zero_grad()\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    _, logits = output\n                else:\n                    logits = output\n                loss = self.loss_fn(logits, y)\n                loss.backward()\n                optimizer.step()\n\n\nclass FedAvgServer(BaseServer):\n    \"\"\"Standard FedAvg server\"\"\"\n    \n    def __init__(self, global_model, test_loader, num_clients, client_loaders, \n                 learning_rate=0.01, lr_decay=0.998, device='cuda', local_epochs=5):\n        super().__init__(global_model, test_loader, num_clients, learning_rate, \n                        lr_decay, device)\n        self.local_epochs = local_epochs\n        \n        for i, (train_loader, val_loader) in enumerate(client_loaders):\n            client = ClientFedAvg(i, copy.deepcopy(global_model), train_loader, \n                                 val_loader, device, learning_rate=learning_rate, \n                                 local_epochs=local_epochs)\n            client.train_samples = client.get_train_samples()\n            self.clients.append(client)\n    \n    def train_round(self):\n        \"\"\"Execute one communication round\"\"\"\n        self.send_models(self.clients)\n        \n        for client in self.clients:\n            client.train()\n        \n        self.receive_models(self.clients)\n        self.aggregate_parameters()\n        \n        acc = self.evaluate()\n        self.learning_rate *= self.lr_decay\n        \n        return acc\n\n\n# ========================================\n# FEDAVG + TEMPNET IMPLEMENTATION\n# ========================================\n\nclass ClientFedAvgTemp(BaseClient):\n    \"\"\"FedAvg + Temperature: Standard SGD with learned temperature scaling\"\"\"\n    \n    def __init__(self, client_id, model, train_loader, val_loader, device, \n                 learning_rate=0.01, local_epochs=5, momentum=0.9, train_samples=None):\n        super().__init__(client_id, model, train_loader, val_loader, device, \n                        learning_rate, local_epochs, momentum)\n        self.train_samples = train_samples or 0\n        self.tempnet = TempNet(feature_dim=512, hidden_dim=128).to(device)\n        self.temp_optimizer = optim.SGD(self.tempnet.parameters(), \n                                       lr=learning_rate)\n    \n    def train(self):\n        self.model.train()\n        self.tempnet.train()\n        optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate)\n        \n        for epoch in range(self.local_epochs):\n            for X, y in self.train_loader:\n                X, y = X.to(self.device), y.to(self.device)\n                \n                optimizer.zero_grad()\n                self.temp_optimizer.zero_grad()\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    features, logits = output\n                else:\n                    features = None\n                    logits = output\n                \n                tau = self.tempnet(features.detach()) if features is not None else torch.tensor(1.0, device=self.device)\n                scaled_logits = logits / tau\n                loss = self.loss_fn(scaled_logits, y)\n                loss.backward()\n                \n                optimizer.step()\n                self.temp_optimizer.step()\n    \n    def get_temperature(self):\n        \"\"\"Get current temperature value\"\"\"\n        self.model.eval()\n        self.tempnet.eval()\n        try:\n            with torch.no_grad():\n                X, _ = next(iter(self.train_loader))\n                X = X.to(self.device)\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    features, _ = output\n                    tau = self.tempnet(features)\n                    return tau.item()\n                else:\n                    return 1.0\n        except:\n            return 1.0\n\n\nclass FedAvgTempServer(BaseServer):\n    \"\"\"FedAvg + Temperature Server\"\"\"\n    \n    def __init__(self, global_model, test_loader, num_clients, client_loaders, \n                 learning_rate=0.01, lr_decay=0.998, device='cuda', local_epochs=5):\n        super().__init__(global_model, test_loader, num_clients, learning_rate, \n                        lr_decay, device)\n        self.local_epochs = local_epochs\n        self.tau_history = []\n        \n        for i, (train_loader, val_loader) in enumerate(client_loaders):\n            client = ClientFedAvgTemp(i, copy.deepcopy(global_model), train_loader, \n                                     val_loader, device, learning_rate=learning_rate, \n                                     local_epochs=local_epochs)\n            client.train_samples = client.get_train_samples()\n            self.clients.append(client)\n    \n    def train_round(self):\n        self.send_models(self.clients)\n        \n        local_taus = []\n        for client in self.clients:\n            client.train()\n            tau = client.get_temperature()\n            local_taus.append(tau)\n        \n        self.receive_models(self.clients)\n        self.aggregate_parameters()\n        \n        acc = self.evaluate()\n        avg_tau = sum(local_taus) / len(local_taus) if local_taus else 1.0\n        self.tau_history.append(avg_tau)\n        \n        self.learning_rate *= self.lr_decay\n        \n        return acc, avg_tau\n\n# ========================================\n# SAM OPTIMIZER (Sharpness Aware Minimization)\n# ========================================\n\nclass SAMOptimizer(optim.Optimizer):\n    \"\"\"SAM optimizer - Sharpness Aware Minimization\"\"\"\n    \n    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n        assert rho >= 0.0, f\"Invalid rho: {rho}\"\n        defaults = dict(rho=rho, adaptive=adaptive)\n        super().__init__(params, defaults)\n        \n        self.base_optimizer = base_optimizer\n        self.param_groups = self.base_optimizer.param_groups\n        for group in self.param_groups:\n            group[\"rho\"] = rho\n            group[\"adaptive\"] = adaptive\n    \n    @torch.no_grad()\n    def first_step(self):\n        \"\"\"Perturbation step: climb to local maximum\"\"\"\n        grad_norm = self._grad_norm()\n        \n        for group in self.param_groups:\n            scale = group[\"rho\"] / (grad_norm + 1e-7)\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n                p.add_(e_w)  # Move to perturbed point θ + ε\n                self.state[p][\"e_w\"] = e_w\n    \n    @torch.no_grad()\n    def second_step(self):\n        \"\"\"Restore to original point (do this BEFORE applying gradients)\"\"\"\n        for group in self.param_groups:\n            for p in group[\"params\"]:\n                if \"e_w\" in self.state[p]:\n                    p.sub_(self.state[p][\"e_w\"])  # Return to original θ\n                    self.state[p][\"e_w\"] = 0\n    \n    def _grad_norm(self):\n        \"\"\"Compute gradient norm across all parameters\"\"\"\n        norm = torch.norm(torch.stack([\n            ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(p=2)\n            for group in self.param_groups for p in group[\"params\"]\n            if p.grad is not None\n        ]), p=2)\n        return norm\n\n\n# ========================================\n# FEDSAM IMPLEMENTATION\n# ========================================\n\nclass ClientFedSAM(BaseClient):\n    \"\"\"FedSAM client using SAM optimizer\"\"\"\n    \n    def __init__(self, client_id, model, train_loader, val_loader, device, \n                 learning_rate=0.01, local_epochs=5, momentum=0.9, rho=0.05, \n                 train_samples=None):\n        super().__init__(client_id, model, train_loader, val_loader, device, \n                        learning_rate, local_epochs, momentum)\n        self.rho = rho\n        self.train_samples = train_samples or 0\n    \n    def train(self):\n        self.model.train()\n        base_optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate)\n        sam_optimizer = SAMOptimizer(self.model.parameters(), base_optimizer, rho=self.rho)\n        \n        for epoch in range(self.local_epochs):\n            for X, y in self.train_loader:\n                X, y = X.to(self.device), y.to(self.device)\n                \n                # First forward-backward: compute gradient at θ\n                base_optimizer.zero_grad()\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    _, logits = output\n                else:\n                    logits = output\n                loss = self.loss_fn(logits, y)\n                loss.backward()\n                \n                # SAM first step: ascent to θ + ε\n                sam_optimizer.first_step()\n                \n                # Second forward-backward: compute gradient at perturbed point θ + ε\n                base_optimizer.zero_grad()\n                output_pert = self.model(X)\n                if isinstance(output_pert, tuple):\n                    _, logits_pert = output_pert\n                else:\n                    logits_pert = output_pert\n                loss_pert = self.loss_fn(logits_pert, y)\n                loss_pert.backward()\n                \n                # SAM second step: restore to original θ (BEFORE gradient descent)\n                sam_optimizer.second_step()\n                \n                # Apply gradient from perturbed point to original θ\n                base_optimizer.step()\n\n\nclass FedSAMServer(BaseServer):\n    \"\"\"FedSAM server\"\"\"\n    \n    def __init__(self, global_model, test_loader, num_clients, client_loaders, \n                 learning_rate=0.01, lr_decay=0.998, device='cuda', local_epochs=5, \n                 rho=0.05):\n        super().__init__(global_model, test_loader, num_clients, learning_rate, \n                        lr_decay, device)\n        self.local_epochs = local_epochs\n        self.rho = rho\n        \n        for i, (train_loader, val_loader) in enumerate(client_loaders):\n            client = ClientFedSAM(i, copy.deepcopy(global_model), train_loader, \n                                 val_loader, device, learning_rate=learning_rate, \n                                 local_epochs=local_epochs, rho=rho)\n            client.train_samples = client.get_train_samples()\n            self.clients.append(client)\n    \n    def train_round(self):\n        self.send_models(self.clients)\n        \n        for client in self.clients:\n            client.train()\n        \n        self.receive_models(self.clients)\n        self.aggregate_parameters()\n        \n        acc = self.evaluate()\n        self.learning_rate *= self.lr_decay\n        \n        return acc\n\n\n# ========================================\n# FEDSAM + TEMPNET IMPLEMENTATION\n# ========================================\n\nclass ClientFedSAMTemp(BaseClient):\n    \"\"\"FedSAM + Temperature: SAM with learned temperature scaling\"\"\"\n    \n    def __init__(self, client_id, model, train_loader, val_loader, device, \n                 learning_rate=0.01, local_epochs=5, momentum=0.9, rho=0.05, \n                 train_samples=None):\n        super().__init__(client_id, model, train_loader, val_loader, device, \n                        learning_rate, local_epochs, momentum)\n        self.rho = rho\n        self.train_samples = train_samples or 0\n        self.tempnet = TempNet(feature_dim=512, hidden_dim=128).to(device)\n        self.temp_optimizer = optim.SGD(self.tempnet.parameters(), \n                                       lr=learning_rate)\n    \n    def train(self):\n        self.model.train()\n        self.tempnet.train()\n        base_optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate)\n        sam_optimizer = SAMOptimizer(self.model.parameters(), base_optimizer, rho=self.rho)\n        \n        for epoch in range(self.local_epochs):\n            for X, y in self.train_loader:\n                X, y = X.to(self.device), y.to(self.device)\n                \n                # First forward-backward: compute gradient at θ with temperature\n                base_optimizer.zero_grad()\n                self.temp_optimizer.zero_grad()\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    features, logits = output\n                else:\n                    features = None\n                    logits = output\n                \n                tau = self.tempnet(features.detach()) if features is not None else torch.tensor(1.0, device=self.device)\n                scaled_logits = logits / tau\n                loss = self.loss_fn(scaled_logits, y)\n                loss.backward()\n                \n                # SAM first step: ascent to θ + ε (model only)\n                sam_optimizer.first_step()\n                \n                # Second forward-backward: at perturbed point θ + ε\n                base_optimizer.zero_grad()\n                self.temp_optimizer.zero_grad()\n                output_pert = self.model(X)\n                if isinstance(output_pert, tuple):\n                    features_pert, logits_pert = output_pert\n                else:\n                    features_pert = None\n                    logits_pert = output_pert\n                \n                tau_pert = self.tempnet(features_pert.detach()) if features_pert is not None else torch.tensor(1.0, device=self.device)\n                scaled_logits_pert = logits_pert / tau_pert\n                loss_pert = self.loss_fn(scaled_logits_pert, y)\n                loss_pert.backward()\n                \n                # SAM second step: restore model to original θ (BEFORE gradient descent)\n                sam_optimizer.second_step()\n                \n                # Apply gradients: model gets gradient from perturbed point, tempnet standard update\n                base_optimizer.step()\n                self.temp_optimizer.step()\n    \n    def get_temperature(self):\n        \"\"\"Get current temperature value\"\"\"\n        self.model.eval()\n        self.tempnet.eval()\n        try:\n            with torch.no_grad():\n                X, _ = next(iter(self.train_loader))\n                X = X.to(self.device)\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    features, _ = output\n                    tau = self.tempnet(features)\n                    return tau.item()\n                else:\n                    return 1.0\n        except:\n            return 1.0\n\n\nclass FedSAMTempServer(BaseServer):\n    \"\"\"FedSAM + Temperature Server\"\"\"\n    \n    def __init__(self, global_model, test_loader, num_clients, client_loaders, \n                 learning_rate=0.01, lr_decay=0.998, device='cuda', local_epochs=5, \n                 rho=0.05):\n        super().__init__(global_model, test_loader, num_clients, learning_rate, \n                        lr_decay, device)\n        self.local_epochs = local_epochs\n        self.rho = rho\n        self.tau_history = []\n        \n        for i, (train_loader, val_loader) in enumerate(client_loaders):\n            client = ClientFedSAMTemp(i, copy.deepcopy(global_model), train_loader, \n                                     val_loader, device, learning_rate=learning_rate, \n                                     local_epochs=local_epochs, rho=rho)\n            client.train_samples = client.get_train_samples()\n            self.clients.append(client)\n    \n    def train_round(self):\n        self.send_models(self.clients)\n        \n        local_taus = []\n        for client in self.clients:\n            client.train()\n            tau = client.get_temperature()\n            local_taus.append(tau)\n        \n        self.receive_models(self.clients)\n        self.aggregate_parameters()\n        \n        acc = self.evaluate()\n        avg_tau = sum(local_taus) / len(local_taus) if local_taus else 1.0\n        self.tau_history.append(avg_tau)\n        \n        self.learning_rate *= self.lr_decay\n        \n        return acc, avg_tau\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T16:24:30.770632Z","iopub.execute_input":"2025-12-27T16:24:30.771492Z","iopub.status.idle":"2025-12-27T16:24:30.818655Z","shell.execute_reply.started":"2025-12-27T16:24:30.771453Z","shell.execute_reply":"2025-12-27T16:24:30.818053Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ========================================\n# FEDLESAM IMPLEMENTATION\n# ========================================\nclass LESAMOptimizer(optim.Optimizer):\n    \n    def __init__(self, params, base_optimizer, rho=0.05, **kwargs):\n        assert rho >= 0.0, f\"Invalid rho: {rho}\"\n        defaults = dict(rho=rho)\n        super().__init__(params, defaults)\n        \n        self.base_optimizer = base_optimizer\n        self.param_groups = self.base_optimizer.param_groups\n        for group in self.param_groups:\n            group[\"rho\"] = rho\n    \n    @torch.no_grad()\n    def first_step(self, g_update=None):\n        \"\"\"Perturbation step using gradient update\"\"\"\n        if g_update is None:\n            g_update = [p.grad.clone() if p.grad is not None else None \n                       for p in self.param_groups[0][\"params\"]]\n        \n        grad_norm = torch.norm(torch.stack([\n            g.norm(p=2) for g in g_update if g is not None\n        ]), p=2)\n        \n        for group in self.param_groups:\n            scale = group[\"rho\"] / (grad_norm + 1e-7)\n            for idx, p in enumerate(group[\"params\"]):\n                if g_update[idx] is None:\n                    continue\n                e_w = g_update[idx] * scale.to(p)\n                p.add_(e_w)\n                self.state[p][\"e_w\"] = e_w\n    \n    @torch.no_grad()\n    def second_step(self):\n        \"\"\"Restore step\"\"\"\n        for group in self.param_groups:\n            for p in group[\"params\"]:\n                if p.grad is None or not self.state[p]:\n                    continue\n                p.sub_(self.state[p][\"e_w\"])\n                self.state[p][\"e_w\"] = 0\n\n\nclass ClientFedLESAM(BaseClient):\n    \"\"\"FedLESAM Client - SAM with momentum\"\"\"\n    \n    def __init__(self, client_id, model, train_loader, val_loader, device, \n                 learning_rate=0.01, local_epochs=5, momentum=0.9, rho=0.05, \n                 train_samples=None):\n        super().__init__(client_id, model, train_loader, val_loader, device, \n                        learning_rate, local_epochs, momentum)\n        self.rho = rho\n        self.train_samples = train_samples or 0\n        self.grad_momentum = None\n    \n    def train(self):\n        self.model.train()\n        base_optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate)\n        lesam_optimizer = LESAMOptimizer(self.model.parameters(), base_optimizer, rho=self.rho)\n        \n        for epoch in range(self.local_epochs):\n            for X, y in self.train_loader:\n                X, y = X.to(self.device), y.to(self.device)\n                \n                # First forward-backward: compute gradient\n                base_optimizer.zero_grad()\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    _, logits = output\n                else:\n                    logits = output\n                loss = self.loss_fn(logits, y)\n                loss.backward()\n                \n                # Initialize gradient momentum\n                if self.grad_momentum is None:\n                    self.grad_momentum = [p.grad.clone() if p.grad is not None else None \n                                         for p in self.model.parameters()]\n                \n                # Apply momentum to gradient\n                for i, p in enumerate(self.model.parameters()):\n                    if p.grad is not None and self.grad_momentum[i] is not None:\n                        p.grad.data = p.grad.data + self.momentum * self.grad_momentum[i]\n                        self.grad_momentum[i] = p.grad.clone()\n                \n                # LESAM first step using momentum-updated gradient\n                lesam_optimizer.first_step(self.grad_momentum)\n                \n                # Second forward-backward: at perturbed point\n                base_optimizer.zero_grad()\n                output_pert = self.model(X)\n                if isinstance(output_pert, tuple):\n                    _, logits_pert = output_pert\n                else:\n                    logits_pert = output_pert\n                loss_pert = self.loss_fn(logits_pert, y)\n                loss_pert.backward()\n                \n                # LESAM second step: descent\n                lesam_optimizer.second_step()\n                \n                # SGD step\n                base_optimizer.step()\n\n\nclass FedLESAMServer(BaseServer):\n    \"\"\"FedLESAM Server\"\"\"\n    \n    def __init__(self, global_model, test_loader, num_clients, client_loaders, \n                 learning_rate=0.01, lr_decay=0.998, device='cuda', local_epochs=5, \n                 rho=0.05):\n        super().__init__(global_model, test_loader, num_clients, learning_rate, \n                        lr_decay, device)\n        self.local_epochs = local_epochs\n        self.rho = rho\n        \n        for i, (train_loader, val_loader) in enumerate(client_loaders):\n            client = ClientFedLESAM(i, copy.deepcopy(global_model), train_loader, \n                                   val_loader, device, learning_rate=learning_rate, \n                                   local_epochs=local_epochs, rho=rho)\n            client.train_samples = client.get_train_samples()\n            self.clients.append(client)\n    \n    def train_round(self):\n        self.send_models(self.clients)\n        \n        for client in self.clients:\n            client.train()\n        \n        self.receive_models(self.clients)\n        self.aggregate_parameters()\n        \n        acc = self.evaluate()\n        self.learning_rate *= self.lr_decay\n        \n        return acc\n\n\nclass ClientFedLESAMTemp(BaseClient):\n    \"\"\"FedLESAM + Temperature Client\"\"\"\n    \n    def __init__(self, client_id, model, train_loader, val_loader, device, \n                 learning_rate=0.01, local_epochs=5, momentum=0.9, rho=0.05, \n                 train_samples=None):\n        super().__init__(client_id, model, train_loader, val_loader, device, \n                        learning_rate, local_epochs, momentum)\n        self.rho = rho\n        self.train_samples = train_samples or 0\n        self.tempnet = TempNet(feature_dim=512, hidden_dim=128).to(device)\n        self.temp_optimizer = optim.SGD(self.tempnet.parameters(), \n                                       lr=learning_rate)\n        self.grad_momentum = None\n    \n    def train(self):\n        self.model.train()\n        self.tempnet.train()\n        base_optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate)\n        lesam_optimizer = LESAMOptimizer(self.model.parameters(), base_optimizer, rho=self.rho)\n        \n        for epoch in range(self.local_epochs):\n            for X, y in self.train_loader:\n                X, y = X.to(self.device), y.to(self.device)\n                \n                # First forward-backward\n                base_optimizer.zero_grad()\n                self.temp_optimizer.zero_grad()\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    features, logits = output\n                else:\n                    features = None\n                    logits = output\n                \n                tau = self.tempnet(features.detach()) if features is not None else torch.tensor(1.0, device=self.device)\n                scaled_logits = logits / tau\n                loss = self.loss_fn(scaled_logits, y)\n                loss.backward()\n                \n                # Initialize gradient momentum\n                if self.grad_momentum is None:\n                    self.grad_momentum = [p.grad.clone() if p.grad is not None else None \n                                         for p in self.model.parameters()]\n                \n                # Apply momentum\n                for i, p in enumerate(self.model.parameters()):\n                    if p.grad is not None and self.grad_momentum[i] is not None:\n                        p.grad.data = p.grad.data + self.momentum * self.grad_momentum[i]\n                        self.grad_momentum[i] = p.grad.clone()\n                \n                # LESAM first step\n                lesam_optimizer.first_step(self.grad_momentum)\n                \n                # Second forward-backward at perturbed point\n                base_optimizer.zero_grad()\n                self.temp_optimizer.zero_grad()\n                output_pert = self.model(X)\n                if isinstance(output_pert, tuple):\n                    features_pert, logits_pert = output_pert\n                else:\n                    features_pert = None\n                    logits_pert = output_pert\n                \n                tau_pert = self.tempnet(features_pert.detach()) if features_pert is not None else torch.tensor(1.0, device=self.device)\n                scaled_logits_pert = logits_pert / tau_pert\n                loss_pert = self.loss_fn(scaled_logits_pert, y)\n                loss_pert.backward()\n                \n                # LESAM second step\n                lesam_optimizer.second_step()\n                \n                # SGD step\n                base_optimizer.step()\n                self.temp_optimizer.step()\n    \n    def get_temperature(self):\n        \"\"\"Get current temperature value\"\"\"\n        self.model.eval()\n        self.tempnet.eval()\n        try:\n            with torch.no_grad():\n                X, _ = next(iter(self.train_loader))\n                X = X.to(self.device)\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    features, _ = output\n                    tau = self.tempnet(features)\n                    return tau.item()\n                else:\n                    return 1.0\n        except:\n            return 1.0\n\n\nclass FedLESAMTempServer(BaseServer):\n    \"\"\"FedLESAM + Temperature Server\"\"\"\n    \n    def __init__(self, global_model, test_loader, num_clients, client_loaders, \n                 learning_rate=0.01, lr_decay=0.998, device='cuda', local_epochs=5, \n                 rho=0.05):\n        super().__init__(global_model, test_loader, num_clients, learning_rate, \n                        lr_decay, device)\n        self.local_epochs = local_epochs\n        self.rho = rho\n        self.tau_history = []\n        \n        for i, (train_loader, val_loader) in enumerate(client_loaders):\n            client = ClientFedLESAMTemp(i, copy.deepcopy(global_model), train_loader, \n                                       val_loader, device, learning_rate=learning_rate, \n                                       local_epochs=local_epochs, rho=rho)\n            client.train_samples = client.get_train_samples()\n            self.clients.append(client)\n    \n    def train_round(self):\n        self.send_models(self.clients)\n        \n        local_taus = []\n        for client in self.clients:\n            client.train()\n            tau = client.get_temperature()\n            local_taus.append(tau)\n        \n        self.receive_models(self.clients)\n        self.aggregate_parameters()\n        \n        acc = self.evaluate()\n        avg_tau = sum(local_taus) / len(local_taus) if local_taus else 1.0\n        self.tau_history.append(avg_tau)\n        \n        self.learning_rate *= self.lr_decay\n        \n        return acc, avg_tau\n\n\n# ========================================\n# FEDSMOO IMPLEMENTATION\n# ========================================\n\nclass SMOOOptimizer:\n    \"\"\"Optimizer for FedSMOO - combines SAM-like perturbations with smoothness regularization\"\"\"\n    \n    def __init__(self, params, base_optimizer, rho=0.05):\n        self.params = list(params)\n        self.base_optimizer = base_optimizer\n        self.rho = rho\n        self.state = {}\n        \n    def zero_grad(self):\n        self.base_optimizer.zero_grad()\n    \n    @torch.no_grad()\n    def first_step(self):\n        \"\"\"Perturb parameters in gradient direction (SAM first step)\"\"\"\n        grad_norm = self._grad_norm()\n        scale = self.rho / (grad_norm + 1e-7)\n        \n        perturbations = []\n        for p in self.params:\n            if p.grad is None:\n                continue\n            # Compute perturbation\n            e_w = p.grad * scale\n            # Store perturbation\n            self.state[p] = {\"e_w\": e_w.clone()}\n            # Apply perturbation\n            p.add_(e_w)\n            perturbations.append(e_w.reshape(-1))\n        \n        return torch.cat(perturbations) if perturbations else torch.zeros(1, device=self.params[0].device)\n    \n    @torch.no_grad()\n    def second_step(self):\n        \"\"\"Remove perturbation (SAM second step)\"\"\"\n        for p in self.params:\n            if p in self.state and \"e_w\" in self.state[p]:\n                # Remove perturbation\n                p.sub_(self.state[p][\"e_w\"])\n    \n    def _grad_norm(self):\n        \"\"\"Compute gradient norm\"\"\"\n        shared_device = self.params[0].device\n        norm = torch.norm(\n            torch.stack([\n                p.grad.norm(p=2).to(shared_device) for p in self.params \n                if p.grad is not None\n            ]),\n            p=2\n        )\n        return norm\n\n\ndef param_to_vector(model):\n    \"\"\"Convert model parameters to a single vector\"\"\"\n    vec = []\n    for param in model.parameters():\n        vec.append(param.data.reshape(-1))\n    return torch.cat(vec)\n\n\ndef vector_to_param(vector, model):\n    \"\"\"Assign vector values back to model parameters\"\"\"\n    index = 0\n    for param in model.parameters():\n        param_size = param.numel()\n        param.data.copy_(vector[index:index + param_size].view(param.shape))\n        index += param_size\n\n\nclass ClientFedSMOO(BaseClient):\n    \"\"\"FedSMOO Client - Smoothness Optimized Federated Learning\"\"\"\n    \n    def __init__(self, client_id, model, train_loader, val_loader, device, \n                 learning_rate=0.01, local_epochs=5, momentum=0.9, rho=0.05, \n                 beta=0.1, train_samples=None):\n        super().__init__(client_id, model, train_loader, val_loader, device, \n                        learning_rate, local_epochs, momentum)\n        self.rho = rho\n        self.beta = beta\n        self.train_samples = train_samples or 0\n        \n        # FedSMOO specific variables\n        self.dual_variable = None\n        self.global_s = None\n        self.mu_i = None\n        self.local_update = None\n        self.local_s_i = None\n    \n    def initialize_variables(self):\n        \"\"\"Initialize dual variables and momentum terms\"\"\"\n        with torch.no_grad():\n            param_vector = param_to_vector(self.model)\n            if self.mu_i is None:\n                self.mu_i = torch.zeros_like(param_vector, device=self.device)\n            if self.dual_variable is None:\n                self.dual_variable = torch.zeros_like(param_vector, device=self.device)\n            if self.global_s is None:\n                self.global_s = torch.zeros_like(param_vector, device=self.device)\n    \n    def train(self):\n        self.model.train()\n        self.initialize_variables()\n        \n        # Create optimizers\n        base_optimizer = optim.SGD(\n            self.model.parameters(), \n            lr=self.learning_rate, \n        )\n        smoo_optimizer = SMOOOptimizer(\n            self.model.parameters(), \n            base_optimizer, \n            rho=self.rho\n        )\n        \n        # Store initial parameters for dual variable update\n        with torch.no_grad():\n            initial_params = param_to_vector(self.model).clone()\n        \n        s_i_k_last = None\n        \n        for epoch in range(self.local_epochs):\n            for batch_idx, (X, y) in enumerate(self.train_loader):\n                X, y = X.to(self.device), y.to(self.device)\n                \n                # === Part 1: Standard gradient computation ===\n                base_optimizer.zero_grad()\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    _, logits = output\n                else:\n                    logits = output\n                loss = self.loss_fn(logits, y)\n                loss.backward()\n                \n                # Store original gradients before SAM\n                original_grads = []\n                for p in self.model.parameters():\n                    if p.grad is not None:\n                        original_grads.append(p.grad.clone())\n                \n                # === Part 2: Apply smoothness adjustment to gradients ===\n                with torch.no_grad():\n                    idx = 0\n                    for p in self.model.parameters():\n                        if p.grad is not None:\n                            numel = p.grad.numel()\n                            # Subtract smoothness terms from gradient\n                            grad_flat = p.grad.reshape(-1)\n                            adjustment = self.mu_i[idx:idx+numel] + self.global_s[idx:idx+numel]\n                            grad_flat -= adjustment\n                            idx += numel\n                \n                # === Part 3: SAM first step (perturb in gradient direction) ===\n                s_i_k = smoo_optimizer.first_step()\n                \n                # === Part 4: Compute gradient at perturbed point ===\n                base_optimizer.zero_grad()\n                output_pert = self.model(X)\n                if isinstance(output_pert, tuple):\n                    _, logits_pert = output_pert\n                else:\n                    logits_pert = output_pert\n                loss_pert = self.loss_fn(logits_pert, y)\n                loss_pert.backward()\n                \n                # === Part 5: SAM second step (return to original point) ===\n                smoo_optimizer.second_step()\n                \n                # === Part 6: Update momentum term ===\n                with torch.no_grad():\n                    self.mu_i += (s_i_k - self.global_s)\n                \n                # === Part 7: Apply optimizer step with proximal term ===\n                # Add proximal regularization gradient\n                with torch.no_grad():\n                    current_params = param_to_vector(self.model)\n                    # Proximal gradient: beta * (current_params - initial_params - dual_variable)\n                    prox_grad = self.beta * (current_params - initial_params + self.dual_variable)\n                    \n                    idx = 0\n                    for p in self.model.parameters():\n                        if p.grad is not None:\n                            numel = p.grad.numel()\n                            # Add proximal gradient term\n                            p.grad += prox_grad[idx:idx+numel].view(p.grad.shape)\n                            idx += numel\n                \n                # Apply optimizer step\n                base_optimizer.step()\n                \n                s_i_k_last = s_i_k\n        \n        # Store local update for server aggregation\n        with torch.no_grad():\n            final_params = param_to_vector(self.model)\n            self.local_update = final_params - initial_params\n            if s_i_k_last is not None:\n                self.local_s_i = self.mu_i - s_i_k_last\n            else:\n                self.local_s_i = self.mu_i.clone()\n\n\nclass FedSMOOServer(BaseServer):\n    \"\"\"FedSMOO Server - Coordinates smoothness-optimized federated learning\"\"\"\n    \n    def __init__(self, global_model, test_loader, num_clients, client_loaders, \n                 learning_rate=0.01, lr_decay=0.998, device='cuda', local_epochs=5, \n                 rho=0.05, beta=0.1):\n        super().__init__(global_model, test_loader, num_clients, learning_rate, \n                        lr_decay, device)\n        self.local_epochs = local_epochs\n        self.rho = rho\n        self.beta = beta\n        \n        # Initialize FedSMOO specific variables\n        with torch.no_grad():\n            init_params = param_to_vector(global_model)\n            self.global_s = torch.zeros_like(init_params, device=device)\n            self.dual_variable_list = torch.zeros(\n                (num_clients, init_params.shape[0]),\n                device=device\n            )\n        \n        # Create clients\n        for i, (train_loader, val_loader) in enumerate(client_loaders):\n            client = ClientFedSMOO(\n                i, copy.deepcopy(global_model), train_loader, val_loader, device,\n                learning_rate=learning_rate, local_epochs=local_epochs, \n                rho=rho, beta=beta\n            )\n            client.train_samples = client.get_train_samples()\n            self.clients.append(client)\n    \n    def train_round(self):\n        # Send models and variables to clients\n        self.send_models_with_variables(self.clients)\n        \n        # Reset global smoothness term\n        self.global_s.zero_()\n        \n        # Train clients\n        for client in self.clients:\n            client.train()\n            \n            # Update dual variables\n            with torch.no_grad():\n                self.dual_variable_list[client.client_id] += client.local_update.to(self.device)\n            \n            # Accumulate smoothness terms\n            with torch.no_grad():\n                self.global_s += client.local_s_i.to(self.device) / len(self.clients)\n        \n        # Normalize global smoothness term\n        with torch.no_grad():\n            global_s_norm = torch.norm(self.global_s)\n            if global_s_norm > 1e-7:\n                self.global_s = (self.rho * self.global_s) / global_s_norm\n        \n        # Aggregate models\n        self.receive_models(self.clients)\n        self.aggregate_parameters()\n        \n        # Add dual variable correction to global model\n        with torch.no_grad():\n            global_params = param_to_vector(self.global_model)\n            dual_mean = torch.mean(self.dual_variable_list, dim=0)\n            global_params = global_params + dual_mean\n            vector_to_param(global_params, self.global_model)\n        \n        # Evaluate and update learning rate\n        acc = self.evaluate()\n        self.learning_rate *= self.lr_decay\n        \n        return acc\n    \n    def send_models_with_variables(self, clients):\n        \"\"\"Send global model and FedSMOO variables to clients\"\"\"\n        model_state = self.global_model.state_dict()\n        for client in clients:\n            client.set_parameters(model_state)\n            with torch.no_grad():\n                client.global_s = self.global_s.clone()\n                client.dual_variable = self.dual_variable_list[client.client_id].clone()\n\n\nclass ClientFedSMOOTemp(ClientFedSMOO):\n    \"\"\"FedSMOO + Temperature Client\"\"\"\n    \n    def __init__(self, client_id, model, train_loader, val_loader, device, \n                 learning_rate=0.01, local_epochs=5, momentum=0.9, rho=0.05, \n                 beta=0.1, train_samples=None, feature_dim=512):\n        super().__init__(client_id, model, train_loader, val_loader, device, \n                        learning_rate, local_epochs, momentum, rho, beta, train_samples)\n        # Temperature network\n        self.tempnet = TempNet(feature_dim=feature_dim, hidden_dim=128).to(device)\n        self.temp_optimizer = optim.SGD(\n            self.tempnet.parameters(), \n            lr=learning_rate\n        )\n    \n    def train(self):\n        self.model.train()\n        self.tempnet.train()\n        self.initialize_variables()\n        \n        # Create optimizers\n        base_optimizer = optim.SGD(\n            self.model.parameters(), \n            lr=self.learning_rate, \n        )\n        smoo_optimizer = SMOOOptimizer(\n            self.model.parameters(), \n            base_optimizer, \n            rho=self.rho\n        )\n        \n        # Store initial parameters\n        with torch.no_grad():\n            initial_params = param_to_vector(self.model).clone()\n        \n        s_i_k_last = None\n        \n        for epoch in range(self.local_epochs):\n            for X, y in self.train_loader:\n                X, y = X.to(self.device), y.to(self.device)\n                \n                # === Standard forward with temperature scaling ===\n                base_optimizer.zero_grad()\n                self.temp_optimizer.zero_grad()\n                \n                output = self.model(X)\n                if isinstance(output, tuple):\n                    features, logits = output\n                    tau = self.tempnet(features.detach())\n                    scaled_logits = logits / tau\n                else:\n                    logits = output\n                    scaled_logits = logits\n                \n                loss = self.loss_fn(scaled_logits, y)\n                loss.backward()\n                \n                # === Apply smoothness adjustment ===\n                with torch.no_grad():\n                    idx = 0\n                    for p in self.model.parameters():\n                        if p.grad is not None:\n                            numel = p.grad.numel()\n                            grad_flat = p.grad.reshape(-1)\n                            adjustment = (self.mu_i[idx:idx+numel] + self.global_s[idx:idx+numel])/tau\n                            grad_flat -= adjustment\n                            idx += numel\n                \n                # === SAM first step ===\n                s_i_k = smoo_optimizer.first_step()\n                \n                # === Forward at perturbed point ===\n                base_optimizer.zero_grad()\n                self.temp_optimizer.zero_grad()\n                \n                output_pert = self.model(X)\n                if isinstance(output_pert, tuple):\n                    features_pert, logits_pert = output_pert\n                    tau_pert = self.tempnet(features_pert.detach())\n                    scaled_logits_pert = logits_pert / tau_pert\n                else:\n                    logits_pert = output_pert\n                    scaled_logits_pert = logits_pert\n                \n                loss_pert = self.loss_fn(scaled_logits_pert, y)\n                loss_pert.backward()\n                \n                # === SAM second step ===\n                smoo_optimizer.second_step()\n                \n                # === Update momentum ===\n                with torch.no_grad():\n                    self.mu_i += (s_i_k - self.global_s)\n                \n                # === Add proximal term and step ===\n                with torch.no_grad():\n                    current_params = param_to_vector(self.model)\n                    prox_grad = self.beta * (current_params - initial_params + self.dual_variable)\n\n                    prox_grad /= tau_pert\n                    \n                    idx = 0\n                    for p in self.model.parameters():\n                        if p.grad is not None:\n                            numel = p.grad.numel()\n                            p.grad += prox_grad[idx:idx+numel].view(p.grad.shape)\n                            idx += numel\n                \n                base_optimizer.step()\n                self.temp_optimizer.step()\n                \n                s_i_k_last = s_i_k\n        \n        # Store updates\n        with torch.no_grad():\n            final_params = param_to_vector(self.model)\n            self.local_update = final_params - initial_params\n            if s_i_k_last is not None:\n                self.local_s_i = self.mu_i - s_i_k_last\n            else:\n                self.local_s_i = self.mu_i.clone()\n    \n    def get_temperature(self):\n        \"\"\"Get current temperature value\"\"\"\n        self.model.eval()\n        self.tempnet.eval()\n        try:\n            with torch.no_grad():\n                X, _ = next(iter(self.train_loader))\n                X = X.to(self.device)\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    features, _ = output\n                    tau = self.tempnet(features)\n                    return tau.item()\n                else:\n                    return 1.0\n        except:\n            return 1.0\n\n\nclass FedSMOOTempServer(FedSMOOServer):\n    \"\"\"FedSMOO + Temperature Server\"\"\"\n    \n    def __init__(self, global_model, test_loader, num_clients, client_loaders, \n                 learning_rate=0.01, lr_decay=0.998, device='cuda', local_epochs=5, \n                 rho=0.05, beta=0.1, feature_dim=512):\n        # Initialize parent without creating clients\n        BaseServer.__init__(self, global_model, test_loader, num_clients, learning_rate, \n                           lr_decay, device)\n        self.local_epochs = local_epochs\n        self.rho = rho\n        self.beta = beta\n        \n        # Initialize FedSMOO specific variables\n        with torch.no_grad():\n            init_params = param_to_vector(global_model)\n            self.global_s = torch.zeros_like(init_params, device=device)\n            self.dual_variable_list = torch.zeros(\n                (num_clients, init_params.shape[0]),\n                device=device\n            )\n        \n        # Create temperature-aware clients\n        for i, (train_loader, val_loader) in enumerate(client_loaders):\n            client = ClientFedSMOOTemp(\n                i, copy.deepcopy(global_model), train_loader, val_loader, device,\n                learning_rate=learning_rate, local_epochs=local_epochs, \n                rho=rho, beta=beta, feature_dim=feature_dim\n            )\n            client.train_samples = client.get_train_samples()\n            self.clients.append(client)\n        \n        self.tau_history = []\n    \n    def train_round(self):\n        # Send models and variables to clients\n        self.send_models_with_variables(self.clients)\n        \n        # Reset global smoothness term\n        self.global_s.zero_()\n        \n        # Train clients and collect temperatures\n        local_taus = []\n        for client in self.clients:\n            client.train()\n            tau = client.get_temperature()\n            local_taus.append(tau)\n            \n            # Update dual variables\n            with torch.no_grad():\n                self.dual_variable_list[client.client_id] += client.local_update.to(self.device)\n            \n            # Accumulate smoothness terms\n            with torch.no_grad():\n                self.global_s += client.local_s_i.to(self.device) / len(self.clients)\n        \n        # Normalize global smoothness\n        with torch.no_grad():\n            global_s_norm = torch.norm(self.global_s)\n            if global_s_norm > 1e-7:\n                self.global_s = (self.rho * self.global_s) / global_s_norm\n        \n        # Aggregate models\n        self.receive_models(self.clients)\n        self.aggregate_parameters()\n        \n        # Add dual variable correction\n        with torch.no_grad():\n            global_params = param_to_vector(self.global_model)\n            dual_mean = torch.mean(self.dual_variable_list, dim=0)\n            global_params = global_params + dual_mean\n            vector_to_param(global_params, self.global_model)\n        \n        # Evaluate and update learning rate\n        acc = self.evaluate()\n        avg_tau = sum(local_taus) / len(local_taus) if local_taus else 1.0\n        self.tau_history.append(avg_tau)\n        \n        self.learning_rate *= self.lr_decay\n        \n        return acc, avg_tau\n\n# ========================================\n# FEDGMT IMPLEMENTATION \n# ========================================\n\n\ndef param_to_vector(model):\n    \"\"\"Convert model parameters to a single vector\"\"\"\n    vec = []\n    for param in model.parameters():\n        vec.append(param.data.reshape(-1))\n    return torch.cat(vec)\n\n\ndef vector_to_param(vector, model):\n    \"\"\"Assign vector values back to model parameters\"\"\"\n    index = 0\n    for param in model.parameters():\n        param_size = param.numel()\n        param.data.copy_(vector[index:index + param_size].view(param.shape))\n        index += param_size\n\n\nclass ClientFedGMT(BaseClient):\n    \"\"\"FedGMT Client - Global Model Teaching with dual variables\"\"\"\n    \n    def __init__(self, client_id, model, train_loader, val_loader, device, \n                 learning_rate=0.01, local_epochs=5, momentum=0.9, tau=3.0, \n                 gamma=1.0, beta=0.1, train_samples=None):\n        super().__init__(client_id, model, train_loader, val_loader, device, \n                        learning_rate, local_epochs, momentum)\n        self.tau = tau  # Temperature for KL divergence\n        self.gamma = gamma  # Weight for KL divergence loss\n        self.beta = beta  # Proximal term coefficient\n        self.train_samples = train_samples or 0\n        \n        # FedGMT specific variables\n        self.EMA_model = None  # Received from server (global EMA)\n        self.dual_variable = None  # Received from server\n        self.local_update = None  # To send back to server\n    \n    def train(self):\n        \"\"\"Train with KL divergence to global EMA model + dual variable correction\"\"\"\n        self.model.train()\n        if self.EMA_model is not None:\n            self.EMA_model.eval()\n        \n        optimizer = optim.SGD(\n            self.model.parameters(), \n            lr=self.learning_rate,\n        )\n        kl_loss_fn = nn.KLDivLoss(reduction=\"batchmean\")\n        \n        # Store initial parameters for dual variable update\n        with torch.no_grad():\n            initial_params = param_to_vector(self.model).clone()\n        \n        for epoch in range(self.local_epochs):\n            for X, y in self.train_loader:\n                X, y = X.to(self.device), y.to(self.device)\n                \n                optimizer.zero_grad()\n                \n                # === Forward pass on current model ===\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    _, logits = output\n                else:\n                    logits = output\n                \n                # Cross-entropy loss\n                ce_loss = self.loss_fn(logits, y)\n                total_loss = ce_loss\n                \n                # === KL divergence to global EMA model ===\n                if self.EMA_model is not None:\n                    with torch.no_grad():\n                        output_ema = self.EMA_model(X)\n                        if isinstance(output_ema, tuple):\n                            _, logits_ema = output_ema\n                        else:\n                            logits_ema = output_ema\n                    \n                    # KL(model || EMA_model)\n                    pred_log_prob = F.log_softmax(logits / self.tau, dim=1)\n                    target_prob = F.softmax(logits_ema / self.tau, dim=1)\n                    kl_div_loss = kl_loss_fn(pred_log_prob, target_prob)\n                    \n                    # Add KL term with temperature scaling\n                    total_loss = total_loss + self.gamma * (self.tau ** 2) * kl_div_loss\n                \n                # Backward pass\n                total_loss.backward()\n                \n                # === Add proximal term for dual variable ===\n                if self.dual_variable is not None:\n                    with torch.no_grad():\n                        current_params = param_to_vector(self.model)\n                        # Proximal gradient: beta * (current - initial + dual)\n                        prox_grad = self.beta * (current_params - initial_params + self.dual_variable)\n                        \n                        idx = 0\n                        for p in self.model.parameters():\n                            if p.grad is not None:\n                                numel = p.grad.numel()\n                                p.grad += prox_grad[idx:idx+numel].view(p.grad.shape)\n                                idx += numel\n                \n                optimizer.step()\n        \n        # Store local update for server\n        with torch.no_grad():\n            final_params = param_to_vector(self.model)\n            self.local_update = final_params - initial_params\n\n\nclass FedGMTServer(BaseServer):\n    \"\"\"FedGMT Server - Maintains global EMA model and dual variables\"\"\"\n    \n    def __init__(self, global_model, test_loader, num_clients, client_loaders, \n                 learning_rate=0.01, lr_decay=0.998, device='cuda', local_epochs=5, \n                 tau=3.0, gamma=1.0, beta=0.1, alpha=0.95):\n        super().__init__(global_model, test_loader, num_clients, learning_rate, \n                        lr_decay, device)\n        self.local_epochs = local_epochs\n        self.tau = tau\n        self.gamma = gamma\n        self.beta = beta\n        self.alpha = alpha  # EMA momentum for global EMA model\n        \n        # Global EMA model (key difference from original FedAvg)\n        self.EMA_model = copy.deepcopy(global_model)\n        self.EMA_model.to(device)\n        \n        # Dual variables for each client\n        with torch.no_grad():\n            init_params = param_to_vector(global_model)\n            self.dual_variable_list = torch.zeros(\n                (num_clients, init_params.shape[0]),\n                device=device\n            )\n        \n        # Create clients\n        for i, (train_loader, val_loader) in enumerate(client_loaders):\n            client = ClientFedGMT(\n                i, copy.deepcopy(global_model), train_loader, val_loader, device,\n                learning_rate=learning_rate, local_epochs=local_epochs, \n                tau=tau, gamma=gamma, beta=beta\n            )\n            client.train_samples = client.get_train_samples()\n            self.clients.append(client)\n    \n    def train_round(self):\n        \"\"\"Train one round with global EMA teaching\"\"\"\n        # Send global model, EMA model, and dual variables to clients\n        self.send_models_with_ema(self.clients)\n        \n        # Train clients\n        for client in self.clients:\n            client.train()\n            \n            # Update dual variables with local updates\n            with torch.no_grad():\n                self.dual_variable_list[client.client_id] += client.local_update.to(self.device)\n        \n        # Aggregate client models\n        self.receive_models(self.clients)\n        self.aggregate_parameters()\n        \n        # Add dual variable correction to global model\n        with torch.no_grad():\n            global_params = param_to_vector(self.global_model)\n            dual_mean = torch.mean(self.dual_variable_list, dim=0)\n            global_params = global_params + dual_mean\n            vector_to_param(global_params, self.global_model)\n        \n        # Update global EMA model\n        with torch.no_grad():\n            ema_params = param_to_vector(self.EMA_model)\n            global_params = param_to_vector(self.global_model)\n            \n            # EMA update: EMA = alpha * EMA + (1 - alpha) * global\n            ema_params = self.alpha * ema_params + (1 - self.alpha) * global_params\n            vector_to_param(ema_params, self.EMA_model)\n        \n        # Evaluate and update learning rate\n        acc = self.evaluate()\n        self.learning_rate *= self.lr_decay\n        \n        return acc\n    \n    def send_models_with_ema(self, clients):\n        \"\"\"Send global model, EMA model, and dual variables to clients\"\"\"\n        global_state = self.global_model.state_dict()\n        \n        for client in clients:\n            # Send global model\n            client.set_parameters(global_state)\n            \n            # Send EMA model (deep copy to avoid sharing)\n            client.EMA_model = copy.deepcopy(self.EMA_model)\n            client.EMA_model.eval()\n            \n            # Send dual variable\n            with torch.no_grad():\n                client.dual_variable = self.dual_variable_list[client.client_id].clone()\n\n\nclass ClientFedGMTTemp(ClientFedGMT):\n    \n    def __init__(self, client_id, model, train_loader, val_loader, device, \n                 learning_rate=0.01, local_epochs=5, momentum=0.9, tau=3.0, \n                 gamma=1.0, beta=0.1, train_samples=None, feature_dim=512):\n        super().__init__(client_id, model, train_loader, val_loader, device, \n                        learning_rate, local_epochs, momentum, tau, gamma, beta, train_samples)\n        \n        # Learnable temperature network\n        self.tempnet = TempNet(feature_dim=feature_dim, hidden_dim=128).to(device)\n        self.temp_optimizer = optim.SGD(self.tempnet.parameters(), lr=learning_rate)\n    \n    def train(self):\n        \"\"\"Train with adaptive temperature scaling\"\"\"\n        self.model.train()\n        self.tempnet.train()\n        if self.EMA_model is not None:\n            self.EMA_model.eval()\n        \n        optimizer = optim.SGD(\n            self.model.parameters(), \n            lr=self.learning_rate,\n        )\n        kl_loss_fn = nn.KLDivLoss(reduction=\"batchmean\")\n        \n        with torch.no_grad():\n            initial_params = param_to_vector(self.model).clone()\n        \n        for epoch in range(self.local_epochs):\n            for X, y in self.train_loader:\n                X, y = X.to(self.device), y.to(self.device)\n                \n                optimizer.zero_grad()\n                self.temp_optimizer.zero_grad()\n                \n                # Forward pass\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    features, logits = output\n                else:\n                    features = None\n                    logits = output\n                \n                # Compute adaptive temperature\n                tau_adaptive = self.tempnet(features.detach())\n                \n                # Cross-entropy loss\n                ce_loss = self.loss_fn(logits/tau_adaptive, y)\n                total_loss = ce_loss\n                \n                # KL divergence with adaptive temperature\n                if self.EMA_model is not None:\n                    with torch.no_grad():\n                        output_ema = self.EMA_model(X)\n                        if isinstance(output_ema, tuple):\n                            _, logits_ema = output_ema\n                        else:\n                            logits_ema = output_ema\n                    \n                    pred_log_prob = F.log_softmax(logits / self.tau, dim=1)\n                    target_prob = F.softmax(logits_ema / self.tau, dim=1)\n                    kl_div_loss = kl_loss_fn(pred_log_prob, target_prob)\n                    \n                    total_loss = total_loss + self.gamma * (self.tau ** 2) * kl_div_loss\n                \n                total_loss.backward()\n                \n                # Proximal term\n                if self.dual_variable is not None:\n                    with torch.no_grad():\n                        current_params = param_to_vector(self.model)\n                        prox_grad = self.beta * (current_params - initial_params + self.dual_variable)\n\n                        prox_grad /= tau_adaptive\n                        \n                        idx = 0\n                        for p in self.model.parameters():\n                            if p.grad is not None:\n                                numel = p.grad.numel()\n                                p.grad += prox_grad[idx:idx+numel].view(p.grad.shape)\n                                idx += numel\n                \n                optimizer.step()\n                self.temp_optimizer.step()\n        \n        with torch.no_grad():\n            final_params = param_to_vector(self.model)\n            self.local_update = final_params - initial_params\n    \n    def get_temperature(self):\n        \"\"\"Get current average temperature\"\"\"\n        self.model.eval()\n        self.tempnet.eval()\n        try:\n            with torch.no_grad():\n                X, _ = next(iter(self.train_loader))\n                X = X.to(self.device)\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    features, _ = output\n                    tau = self.tempnet(features)\n                    return tau.item()\n                else:\n                    return self.tau\n        except:\n            return self.tau\n\n\nclass FedGMTTempServer(FedGMTServer):\n    \"\"\"FedGMT + Temperature Server\"\"\"\n    \n    def __init__(self, global_model, test_loader, num_clients, client_loaders, \n                 learning_rate=0.01, lr_decay=0.998, device='cuda', local_epochs=5, \n                 tau=3.0, gamma=1.0, beta=0.1, alpha=0.95, feature_dim=512):\n        # Initialize parent without creating clients\n        BaseServer.__init__(self, global_model, test_loader, num_clients, \n                           learning_rate, lr_decay, device)\n        self.local_epochs = local_epochs\n        self.tau = tau\n        self.gamma = gamma\n        self.beta = beta\n        self.alpha = alpha\n        \n        # Global EMA model\n        self.EMA_model = copy.deepcopy(global_model)\n        self.EMA_model.to(device)\n        \n        # Dual variables\n        with torch.no_grad():\n            init_params = param_to_vector(global_model)\n            self.dual_variable_list = torch.zeros(\n                (num_clients, init_params.shape[0]),\n                device=device\n            )\n        \n        # Create temperature-aware clients\n        for i, (train_loader, val_loader) in enumerate(client_loaders):\n            client = ClientFedGMTTemp(\n                i, copy.deepcopy(global_model), train_loader, val_loader, device,\n                learning_rate=learning_rate, local_epochs=local_epochs, \n                tau=tau, gamma=gamma, beta=beta, feature_dim=feature_dim\n            )\n            client.train_samples = client.get_train_samples()\n            self.clients.append(client)\n        \n        self.tau_history = []\n    \n    def train_round(self):\n        \"\"\"Train round with temperature tracking\"\"\"\n        self.send_models_with_ema(self.clients)\n        \n        local_taus = []\n        for client in self.clients:\n            client.train()\n            tau = client.get_temperature()\n            local_taus.append(tau)\n            \n            with torch.no_grad():\n                self.dual_variable_list[client.client_id] += client.local_update.to(self.device)\n        \n        self.receive_models(self.clients)\n        self.aggregate_parameters()\n        \n        # Apply dual variable correction\n        with torch.no_grad():\n            global_params = param_to_vector(self.global_model)\n            dual_mean = torch.mean(self.dual_variable_list, dim=0)\n            global_params = global_params + dual_mean\n            vector_to_param(global_params, self.global_model)\n        \n        # Update global EMA\n        with torch.no_grad():\n            ema_params = param_to_vector(self.EMA_model)\n            global_params = param_to_vector(self.global_model)\n            ema_params = self.alpha * ema_params + (1 - self.alpha) * global_params\n            vector_to_param(ema_params, self.EMA_model)\n        \n        acc = self.evaluate()\n        avg_tau = sum(local_taus) / len(local_taus) if local_taus else self.tau\n        self.tau_history.append(avg_tau)\n        \n        self.learning_rate *= self.lr_decay\n        \n        return acc, avg_tau\n    \n\n# ========================================\n# FEDPROX IMPLEMENTATION\n# ========================================\n\nclass ClientFedProx(BaseClient):\n    \"\"\"FedProx client - FedAvg with proximal term\"\"\"\n    \n    def __init__(self, client_id, model, train_loader, val_loader, device, \n                 learning_rate=0.01, local_epochs=5, momentum=0.9, mu=0.01, \n                 train_samples=None):\n        super().__init__(client_id, model, train_loader, val_loader, device, \n                        learning_rate, local_epochs, momentum)\n        self.mu = mu  # Proximal term coefficient\n        self.train_samples = train_samples or 0\n        self.global_model = None\n    \n    def set_global_model(self, global_state_dict):\n        \"\"\"Store global model state for proximal term\"\"\"\n        self.global_model = copy.deepcopy(self.model)\n        self.global_model.load_state_dict(global_state_dict)\n        self.global_model.eval()\n    \n    def train(self):\n        self.model.train()\n        optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate)\n        \n        for epoch in range(self.local_epochs):\n            for X, y in self.train_loader:\n                X, y = X.to(self.device), y.to(self.device)\n                \n                optimizer.zero_grad()\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    _, logits = output\n                else:\n                    logits = output\n                \n                # Standard cross-entropy loss\n                loss = self.loss_fn(logits, y)\n                \n                # Add proximal term: (mu/2) * ||w - w_global||^2\n                if self.global_model is not None:\n                    proximal_term = 0.0\n                    for param, global_param in zip(self.model.parameters(), \n                                                   self.global_model.parameters()):\n                        proximal_term += torch.norm(param - global_param.detach()) ** 2\n                    loss += (self.mu / 2) * proximal_term\n                \n                loss.backward()\n                optimizer.step()\n\n\nclass FedProxServer(BaseServer):\n    \"\"\"FedProx server\"\"\"\n    \n    def __init__(self, global_model, test_loader, num_clients, client_loaders, \n                 learning_rate=0.01, lr_decay=0.998, device='cuda', local_epochs=5, \n                 mu=0.01):\n        super().__init__(global_model, test_loader, num_clients, learning_rate, \n                        lr_decay, device)\n        self.local_epochs = local_epochs\n        self.mu = mu\n        \n        for i, (train_loader, val_loader) in enumerate(client_loaders):\n            client = ClientFedProx(i, copy.deepcopy(global_model), train_loader, \n                                  val_loader, device, learning_rate=learning_rate, \n                                  local_epochs=local_epochs, mu=mu)\n            client.train_samples = client.get_train_samples()\n            self.clients.append(client)\n    \n    def train_round(self):\n        self.send_models(self.clients)\n        \n        # Set global model for proximal term\n        global_state = self.global_model.state_dict()\n        for client in self.clients:\n            client.set_global_model(global_state)\n            client.train()\n        \n        self.receive_models(self.clients)\n        self.aggregate_parameters()\n        \n        acc = self.evaluate()\n        self.learning_rate *= self.lr_decay\n        \n        return acc\n\n\n# ========================================\n# FEDPROX + TEMPNET IMPLEMENTATION\n# ========================================\n\nclass ClientFedProxTemp(BaseClient):\n    \"\"\"FedProx + Temperature: FedProx with learned temperature scaling\"\"\"\n    \n    def __init__(self, client_id, model, train_loader, val_loader, device, \n                 learning_rate=0.01, local_epochs=5, momentum=0.9, mu=0.01, \n                 train_samples=None):\n        super().__init__(client_id, model, train_loader, val_loader, device, \n                        learning_rate, local_epochs, momentum)\n        self.mu = mu\n        self.train_samples = train_samples or 0\n        self.global_model = None\n        self.tempnet = TempNet(feature_dim=512, hidden_dim=128).to(device)\n        self.temp_optimizer = optim.SGD(self.tempnet.parameters(), \n                                       lr=learning_rate)\n    \n    def set_global_model(self, global_state_dict):\n        \"\"\"Store global model state for proximal term\"\"\"\n        self.global_model = copy.deepcopy(self.model)\n        self.global_model.load_state_dict(global_state_dict)\n        self.global_model.eval()\n    \n    def train(self):\n        self.model.train()\n        self.tempnet.train()\n        optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate)\n        \n        for epoch in range(self.local_epochs):\n            for X, y in self.train_loader:\n                X, y = X.to(self.device), y.to(self.device)\n                \n                optimizer.zero_grad()\n                self.temp_optimizer.zero_grad()\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    features, logits = output\n                else:\n                    features = None\n                    logits = output\n                \n                # Get temperature\n                tau = self.tempnet(features.detach()) if features is not None else torch.tensor(1.0, device=self.device)\n                \n                # Standard cross-entropy loss with temperature\n                scaled_logits = logits / tau\n                loss = self.loss_fn(scaled_logits, y)\n                \n                # Add proximal term\n                if self.global_model is not None:\n                    proximal_term = 0.0\n                    for param, global_param in zip(self.model.parameters(), \n                                                   self.global_model.parameters()):\n                        proximal_term += torch.norm(param - global_param.detach()) ** 2\n                    loss += (self.mu / 2) * proximal_term\n                \n                loss.backward()\n                optimizer.step()\n                self.temp_optimizer.step()\n    \n    def get_temperature(self):\n        \"\"\"Get current temperature value\"\"\"\n        self.model.eval()\n        self.tempnet.eval()\n        try:\n            with torch.no_grad():\n                X, _ = next(iter(self.train_loader))\n                X = X.to(self.device)\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    features, _ = output\n                    tau = self.tempnet(features)\n                    return tau.item()\n                else:\n                    return 1.0\n        except:\n            return 1.0\n\n\nclass FedProxTempServer(BaseServer):\n    \"\"\"FedProx + Temperature Server\"\"\"\n    \n    def __init__(self, global_model, test_loader, num_clients, client_loaders, \n                 learning_rate=0.01, lr_decay=0.998, device='cuda', local_epochs=5, \n                 mu=0.01):\n        super().__init__(global_model, test_loader, num_clients, learning_rate, \n                        lr_decay, device)\n        self.local_epochs = local_epochs\n        self.mu = mu\n        self.tau_history = []\n        \n        for i, (train_loader, val_loader) in enumerate(client_loaders):\n            client = ClientFedProxTemp(i, copy.deepcopy(global_model), train_loader, \n                                      val_loader, device, learning_rate=learning_rate, \n                                      local_epochs=local_epochs, mu=mu)\n            client.train_samples = client.get_train_samples()\n            self.clients.append(client)\n    \n    def train_round(self):\n        self.send_models(self.clients)\n        \n        # Set global model for proximal term\n        global_state = self.global_model.state_dict()\n        for client in self.clients:\n            client.set_global_model(global_state)\n        \n        local_taus = []\n        for client in self.clients:\n            client.train()\n            tau = client.get_temperature()\n            local_taus.append(tau)\n        \n        self.receive_models(self.clients)\n        self.aggregate_parameters()\n        \n        acc = self.evaluate()\n        avg_tau = sum(local_taus) / len(local_taus) if local_taus else 1.0\n        self.tau_history.append(avg_tau)\n        \n        self.learning_rate *= self.lr_decay\n        \n        return acc, avg_tau","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T16:24:41.378912Z","iopub.execute_input":"2025-12-27T16:24:41.379707Z","iopub.status.idle":"2025-12-27T16:24:41.501543Z","shell.execute_reply.started":"2025-12-27T16:24:41.379675Z","shell.execute_reply":"2025-12-27T16:24:41.500729Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# ========================================\n# TRAINING FUNCTIONS FOR ALL ALGORITHMS\n# ========================================\ndef run_fedavg():\n    \"\"\"Run FedAvg training\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STARTING FedAvg TRAINING\")\n    print(\"=\"*60)\n    \n    client_loaders = [(train_loaders[i], val_loaders[i]) for i in range(NUM_OF_CLIENTS)]\n    \n    server = FedAvgServer(\n        global_model=SimpleCNN().to(device),\n        test_loader=test_loader,\n        num_clients=NUM_OF_CLIENTS,\n        client_loaders=client_loaders,\n        learning_rate=LEARNING_RATE,\n        lr_decay=1,\n        device=device,\n        local_epochs=LOCAL_EPOCHS\n    )\n    \n    test_accs = []\n    for rnd in range(COMM_ROUND):\n        round_start = time.time()\n        acc = server.train_round()\n        test_accs.append(acc)\n        \n        print(f\"[FedAvg] R{rnd+1}/{COMM_ROUND} Test Acc: {acc:.4f} | Time: {time.time()-round_start:.2f}s\")\n    \n    return test_accs, server.global_model\n\n\ndef run_fedavg_temp():\n    \"\"\"Run FedAvg + Temperature training\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STARTING FedAvg + TEMP TRAINING\")\n    print(\"=\"*60)\n    \n    client_loaders = [(train_loaders[i], val_loaders[i]) for i in range(NUM_OF_CLIENTS)]\n    \n    server = FedAvgTempServer(\n        global_model=SimpleCNN().to(device),\n        test_loader=test_loader,\n        num_clients=NUM_OF_CLIENTS,\n        client_loaders=client_loaders,\n        learning_rate=LEARNING_RATE,\n        lr_decay=1,\n        device=device,\n        local_epochs=LOCAL_EPOCHS\n    )\n    \n    test_accs = []\n    tau_history = []\n    for rnd in range(COMM_ROUND):\n        round_start = time.time()\n        acc, avg_tau = server.train_round()\n        test_accs.append(acc)\n        tau_history.append(avg_tau)\n        \n        print(f\"[FedAvg+Temp] R{rnd+1}/{COMM_ROUND} Test Acc: {acc:.4f} | Avg τ: {avg_tau:.4f} | Time: {time.time()-round_start:.2f}s\")\n    \n    return test_accs, tau_history, server.global_model\n\n\ndef run_fedsam():\n    \"\"\"Run FedSAM training\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STARTING FedSAM TRAINING\")\n    print(\"=\"*60)\n    \n    client_loaders = [(train_loaders[i], val_loaders[i]) for i in range(NUM_OF_CLIENTS)]\n    \n    server = FedSAMServer(\n        global_model=SimpleCNN().to(device),\n        test_loader=test_loader,\n        num_clients=NUM_OF_CLIENTS,\n        client_loaders=client_loaders,\n        learning_rate=LEARNING_RATE,\n        lr_decay=1,\n        device=device,\n        local_epochs=LOCAL_EPOCHS,\n        rho=RHO\n    )\n    \n    test_accs = []\n    for rnd in range(COMM_ROUND):\n        round_start = time.time()\n        acc = server.train_round()\n        test_accs.append(acc)\n        \n        print(f\"[FedSAM] R{rnd+1}/{COMM_ROUND} Test Acc: {acc:.4f} | Time: {time.time()-round_start:.2f}s\")\n    \n    return test_accs, server.global_model\n\n\ndef run_fedsam_temp():\n    \"\"\"Run FedSAM + Temperature training\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STARTING FedSAM + TEMP TRAINING\")\n    print(\"=\"*60)\n    \n    client_loaders = [(train_loaders[i], val_loaders[i]) for i in range(NUM_OF_CLIENTS)]\n    \n    server = FedSAMTempServer(\n        global_model=SimpleCNN().to(device),\n        test_loader=test_loader,\n        num_clients=NUM_OF_CLIENTS,\n        client_loaders=client_loaders,\n        learning_rate=LEARNING_RATE,\n        lr_decay=1,\n        device=device,\n        local_epochs=LOCAL_EPOCHS,\n        rho=RHO\n    )\n    \n    test_accs = []\n    tau_history = []\n    for rnd in range(COMM_ROUND):\n        round_start = time.time()\n        acc, avg_tau = server.train_round()\n        test_accs.append(acc)\n        tau_history.append(avg_tau)\n        \n        print(f\"[FedSAM+Temp] R{rnd+1}/{COMM_ROUND} Test Acc: {acc:.4f} | Avg τ: {avg_tau:.4f} | Time: {time.time()-round_start:.2f}s\")\n    \n    return test_accs, tau_history, server.global_model\n\n\ndef run_fedlesam():\n    \"\"\"Run FedLESAM training\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STARTING FedLESAM TRAINING\")\n    print(\"=\"*60)\n    \n    client_loaders = [(train_loaders[i], val_loaders[i]) for i in range(NUM_OF_CLIENTS)]\n    \n    server = FedLESAMServer(\n        global_model=SimpleCNN().to(device),\n        test_loader=test_loader,\n        num_clients=NUM_OF_CLIENTS,\n        client_loaders=client_loaders,\n        learning_rate=LEARNING_RATE,\n        lr_decay=1,\n        device=device,\n        local_epochs=LOCAL_EPOCHS,\n        rho=RHO\n    )\n    \n    test_accs = []\n    for rnd in range(COMM_ROUND):\n        round_start = time.time()\n        acc = server.train_round()\n        test_accs.append(acc)\n        \n        print(f\"[FedLESAM] R{rnd+1}/{COMM_ROUND} Test Acc: {acc:.4f} | Time: {time.time()-round_start:.2f}s\")\n    \n    return test_accs, server.global_model\n\n\ndef run_fedlesam_temp():\n    \"\"\"Run FedLESAM + Temperature training\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STARTING FedLESAM + TEMP TRAINING\")\n    print(\"=\"*60)\n    \n    client_loaders = [(train_loaders[i], val_loaders[i]) for i in range(NUM_OF_CLIENTS)]\n    \n    server = FedLESAMTempServer(\n        global_model=SimpleCNN().to(device),\n        test_loader=test_loader,\n        num_clients=NUM_OF_CLIENTS,\n        client_loaders=client_loaders,\n        learning_rate=LEARNING_RATE,\n        lr_decay=1,\n        device=device,\n        local_epochs=LOCAL_EPOCHS,\n        rho=RHO\n    )\n    \n    test_accs = []\n    tau_history = []\n    for rnd in range(COMM_ROUND):\n        round_start = time.time()\n        acc, avg_tau = server.train_round()\n        test_accs.append(acc)\n        tau_history.append(avg_tau)\n        \n        print(f\"[FedLESAM+Temp] R{rnd+1}/{COMM_ROUND} Test Acc: {acc:.4f} | Avg τ: {avg_tau:.4f} | Time: {time.time()-round_start:.2f}s\")\n    \n    return test_accs, tau_history, server.global_model\n\n\ndef run_fedsmoo():\n    \"\"\"Run FedSMOO training\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STARTING FedSMOO TRAINING\")\n    print(\"=\"*60)\n    \n    client_loaders = [(train_loaders[i], val_loaders[i]) for i in range(NUM_OF_CLIENTS)]\n    \n    server = FedSMOOServer(\n        global_model=SimpleCNN().to(device),\n        test_loader=test_loader,\n        num_clients=NUM_OF_CLIENTS,\n        client_loaders=client_loaders,\n        learning_rate=LEARNING_RATE,\n        lr_decay=1,\n        device=device,\n        local_epochs=LOCAL_EPOCHS,\n        rho=RHO\n    )\n    \n    test_accs = []\n    for rnd in range(COMM_ROUND):\n        round_start = time.time()\n        acc = server.train_round()\n        test_accs.append(acc)\n        \n        print(f\"[FedSMOO] R{rnd+1}/{COMM_ROUND} Test Acc: {acc:.4f} | Time: {time.time()-round_start:.2f}s\")\n    \n    return test_accs, server.global_model\n\n\ndef run_fedsmoo_temp():\n    \"\"\"Run FedSMOO + Temperature training\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STARTING FedSMOO + TEMP TRAINING\")\n    print(\"=\"*60)\n    \n    client_loaders = [(train_loaders[i], val_loaders[i]) for i in range(NUM_OF_CLIENTS)]\n    \n    server = FedSMOOTempServer(\n        global_model=SimpleCNN().to(device),\n        test_loader=test_loader,\n        num_clients=NUM_OF_CLIENTS,\n        client_loaders=client_loaders,\n        learning_rate=LEARNING_RATE,\n        lr_decay=1,\n        device=device,\n        local_epochs=LOCAL_EPOCHS,\n        rho=RHO\n    )\n    \n    test_accs = []\n    tau_history = []\n    for rnd in range(COMM_ROUND):\n        round_start = time.time()\n        acc, avg_tau = server.train_round()\n        test_accs.append(acc)\n        tau_history.append(avg_tau)\n        \n        print(f\"[FedSMOO+Temp] R{rnd+1}/{COMM_ROUND} Test Acc: {acc:.4f} | Avg τ: {avg_tau:.4f} | Time: {time.time()-round_start:.2f}s\")\n    \n    return test_accs, tau_history, server.global_model\n\n\ndef run_fedgmt():\n    \"\"\"Run FedGMT training\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STARTING FedGMT TRAINING\")\n    print(\"=\"*60)\n    \n    client_loaders = [(train_loaders[i], val_loaders[i]) for i in range(NUM_OF_CLIENTS)]\n    \n    server = FedGMTServer(\n        global_model=SimpleCNN().to(device),\n        test_loader=test_loader,\n        num_clients=NUM_OF_CLIENTS,\n        client_loaders=client_loaders,\n        learning_rate=LEARNING_RATE,\n        lr_decay=1,\n        device=device,\n        local_epochs=LOCAL_EPOCHS,\n        tau=3.0,\n        gamma=1.0\n    )\n    \n    test_accs = []\n    for rnd in range(COMM_ROUND):\n        round_start = time.time()\n        acc = server.train_round()\n        test_accs.append(acc)\n        \n        print(f\"[FedGMT] R{rnd+1}/{COMM_ROUND} Test Acc: {acc:.4f} | Time: {time.time()-round_start:.2f}s\")\n    \n    return test_accs, server.global_model\n\n\ndef run_fedgmt_temp():\n    \"\"\"Run FedGMT + Temperature training\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STARTING FedGMT + TEMP TRAINING\")\n    print(\"=\"*60)\n    \n    client_loaders = [(train_loaders[i], val_loaders[i]) for i in range(NUM_OF_CLIENTS)]\n    \n    server = FedGMTTempServer(\n        global_model=SimpleCNN().to(device),\n        test_loader=test_loader,\n        num_clients=NUM_OF_CLIENTS,\n        client_loaders=client_loaders,\n        learning_rate=LEARNING_RATE,\n        lr_decay=1,\n        device=device,\n        local_epochs=LOCAL_EPOCHS,\n        tau=3.0,\n        gamma=1.0\n    )\n    \n    test_accs = []\n    tau_history = []\n    for rnd in range(COMM_ROUND):\n        round_start = time.time()\n        acc, avg_tau = server.train_round()\n        test_accs.append(acc)\n        tau_history.append(avg_tau)\n        \n        print(f\"[FedGMT+Temp] R{rnd+1}/{COMM_ROUND} Test Acc: {acc:.4f} | Avg τ: {avg_tau:.4f} | Time: {time.time()-round_start:.2f}s\")\n    \n    return test_accs, tau_history, server.global_model\n\ndef run_fedprox():\n    \"\"\"Run FedProx training\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STARTING FedProx TRAINING\")\n    print(\"=\"*60)\n    \n    client_loaders = [(train_loaders[i], val_loaders[i]) for i in range(NUM_OF_CLIENTS)]\n    \n    server = FedProxServer(\n        global_model=SimpleCNN().to(device),\n        test_loader=test_loader,\n        num_clients=NUM_OF_CLIENTS,\n        client_loaders=client_loaders,\n        learning_rate=LEARNING_RATE,\n        lr_decay=1,\n        device=device,\n        local_epochs=LOCAL_EPOCHS,\n        mu=0.01\n    )\n    \n    test_accs = []\n    for rnd in range(COMM_ROUND):\n        round_start = time.time()\n        acc = server.train_round()\n        test_accs.append(acc)\n        \n        print(f\"[FedProx] R{rnd+1}/{COMM_ROUND} Test Acc: {acc:.4f} | Time: {time.time()-round_start:.2f}s\")\n    \n    return test_accs, server.global_model\n\n\ndef run_fedprox_temp():\n    \"\"\"Run FedProx + Temperature training\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STARTING FedProx + TEMP TRAINING\")\n    print(\"=\"*60)\n    \n    client_loaders = [(train_loaders[i], val_loaders[i]) for i in range(NUM_OF_CLIENTS)]\n    \n    server = FedProxTempServer(\n        global_model=SimpleCNN().to(device),\n        test_loader=test_loader,\n        num_clients=NUM_OF_CLIENTS,\n        client_loaders=client_loaders,\n        learning_rate=LEARNING_RATE,\n        lr_decay=1,\n        device=device,\n        local_epochs=LOCAL_EPOCHS,\n        mu=0.01\n    )\n    \n    test_accs = []\n    tau_history = []\n    for rnd in range(COMM_ROUND):\n        round_start = time.time()\n        acc, avg_tau = server.train_round()\n        test_accs.append(acc)\n        tau_history.append(avg_tau)\n        \n        print(f\"[FedProx+Temp] R{rnd+1}/{COMM_ROUND} Test Acc: {acc:.4f} | Avg τ: {avg_tau:.4f} | Time: {time.time()-round_start:.2f}s\")\n    \n    return test_accs, tau_history, server.global_model\n\n# ========================================\n# RUN ALL EXPERIMENTS\n# ========================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"FEDERATED LEARNING EXPERIMENTS - COMPLETE FL-SAM SUITE\")\nprint(\"=\"*80)\nprint(f\"Configuration:\")\nprint(f\"  - Num Clients: {NUM_OF_CLIENTS}\")\nprint(f\"  - Comm Rounds: {COMM_ROUND}\")\nprint(f\"  - Local Epochs: {LOCAL_EPOCHS}\")\nprint(f\"  - Batch Size: {BATCH_SIZE}\")\nprint(f\"  - Learning Rate: {LEARNING_RATE}\")\nprint(f\"  - RHO (SAM): {RHO}\")\nprint(f\"  - Alpha (Dirichlet): {ALPHA}\")\nprint(\"=\"*80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T16:24:50.363190Z","iopub.execute_input":"2025-12-27T16:24:50.363516Z","iopub.status.idle":"2025-12-27T16:24:50.394719Z","shell.execute_reply.started":"2025-12-27T16:24:50.363486Z","shell.execute_reply":"2025-12-27T16:24:50.394058Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nFEDERATED LEARNING EXPERIMENTS - COMPLETE FL-SAM SUITE\n================================================================================\nConfiguration:\n  - Num Clients: 10\n  - Comm Rounds: 50\n  - Local Epochs: 5\n  - Batch Size: 64\n  - Learning Rate: 0.01\n  - RHO (SAM): 0.05\n  - Alpha (Dirichlet): 0.5\n================================================================================\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"print(\"\\n[1/12] FedSMOO...\")\ntest_accs_fedsmoo, model_fedsmoo = run_fedsmoo()\n\nprint(\"\\n[2/12] FedSMOO+Temp...\")\ntest_accs_fedsmoo_temp, tau_fedsmoo_temp, model_fedsmoo_temp = run_fedsmoo_temp()\n\nprint(\"\\n[3/12] FedGMT...\")\ntest_accs_fedgmt, model_fedgmt = run_fedgmt()\n\nprint(\"\\n[4/12] FedGMT+Temp...\")\ntest_accs_fedgmt_temp, tau_fedgmt_temp, model_fedgmt_temp = run_fedgmt_temp()\n\n# print(\"\\n[5/12] FedAvg...\")\n# test_accs_fedavg, model_fedavg = run_fedavg()\n\n# print(\"\\n[6/12] FedAvg+Temp...\")\n# test_accs_fedavg_temp, tau_fedavg_temp, model_fedavg_temp = run_fedavg_temp()\n\n# print(\"\\n[7/12] FedSAM...\")\n# test_accs_fedsam, model_fedsam = run_fedsam()\n\n# print(\"\\n[8/12] FedSAM+Temp...\")\n# test_accs_fedsam_temp, tau_fedsam_temp, model_fedsam_temp = run_fedsam_temp()\n\n# print(\"\\n[9/12] FedLESAM...\")\n# test_accs_fedlesam, model_fedlesam = run_fedlesam()\n\n# print(\"\\n[10/12] FedLESAM+Temp...\")\n# test_accs_fedlesam_temp, tau_fedlesam_temp, model_fedlesam_temp = run_fedlesam_temp()\n\n# print(\"\\n[11/12] FedProx...\")\n# test_accs_fedprox, model_fedprox = run_fedprox()\n\n# print(\"\\n[12/12] FedProx+Temp...\")\n# test_accs_fedprox_temp, tau_fedprox_temp, model_fedprox_temp = run_fedprox_temp()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# ========================================\n# PLOTTING ALL ALGORITHMS\n# ========================================\n\nplt.figure(figsize=(14, 8))\n\n# Define colors and line styles for better distinction\ncolors = {\n    'FedAvg': '#1f77b4',\n    'FedAvg+Temp': '#aec7e8',\n    'FedSAM': '#ff7f0e',\n    'FedSAM+Temp': '#ffbb78',\n    'FedLESAM': '#2ca02c',\n    'FedLESAM+Temp': '#98df8a',\n    'FedSMOO': '#d62728',\n    'FedSMOO+Temp': '#ff9896',\n    'FedGMT': '#9467bd',\n    'FedGMT+Temp': '#c5b0d5',\n    'FedProx': '#8c564b',\n    'FedProx+Temp': '#c49c94'\n}\n\nline_styles = {\n    'FedAvg': '-',\n    'FedAvg+Temp': '--',\n    'FedSAM': '-',\n    'FedSAM+Temp': '--',\n    'FedLESAM': '-',\n    'FedLESAM+Temp': '--',\n    'FedSMOO': '-',\n    'FedSMOO+Temp': '--',\n    'FedGMT': '-',\n    'FedGMT+Temp': '--',\n    'FedProx': '-',\n    'FedProx+Temp': '--'\n}\n\n# Communication rounds (x-axis)\nrounds = np.arange(1, len(test_accs_fedavg) + 1)\n\n# Plot all algorithms\nplt.plot(rounds, test_accs_fedavg, \n         color=colors['FedAvg'], linestyle=line_styles['FedAvg'], \n         linewidth=2, label='FedAvg', marker='o', markersize=4, markevery=5)\n\nplt.plot(rounds, test_accs_fedavg_temp, \n         color=colors['FedAvg+Temp'], linestyle=line_styles['FedAvg+Temp'], \n         linewidth=2, label='FedAvg+Temp', marker='s', markersize=4, markevery=5)\n\nplt.plot(rounds, test_accs_fedsam, \n         color=colors['FedSAM'], linestyle=line_styles['FedSAM'], \n         linewidth=2, label='FedSAM', marker='o', markersize=4, markevery=5)\n\nplt.plot(rounds, test_accs_fedsam_temp, \n         color=colors['FedSAM+Temp'], linestyle=line_styles['FedSAM+Temp'], \n         linewidth=2, label='FedSAM+Temp', marker='s', markersize=4, markevery=5)\n\nplt.plot(rounds, test_accs_fedlesam, \n         color=colors['FedLESAM'], linestyle=line_styles['FedLESAM'], \n         linewidth=2, label='FedLESAM', marker='o', markersize=4, markevery=5)\n\nplt.plot(rounds, test_accs_fedlesam_temp, \n         color=colors['FedLESAM+Temp'], linestyle=line_styles['FedLESAM+Temp'], \n         linewidth=2, label='FedLESAM+Temp', marker='s', markersize=4, markevery=5)\n\nplt.plot(rounds, test_accs_fedsmoo, \n         color=colors['FedSMOO'], linestyle=line_styles['FedSMOO'], \n         linewidth=2, label='FedSMOO', marker='o', markersize=4, markevery=5)\n\nplt.plot(rounds, test_accs_fedsmoo_temp, \n         color=colors['FedSMOO+Temp'], linestyle=line_styles['FedSMOO+Temp'], \n         linewidth=2, label='FedSMOO+Temp', marker='s', markersize=4, markevery=5)\n\nplt.plot(rounds, test_accs_fedgmt, \n         color=colors['FedGMT'], linestyle=line_styles['FedGMT'], \n         linewidth=2, label='FedGMT', marker='o', markersize=4, markevery=5)\n\nplt.plot(rounds, test_accs_fedgmt_temp, \n         color=colors['FedGMT+Temp'], linestyle=line_styles['FedGMT+Temp'], \n         linewidth=2, label='FedGMT+Temp', marker='s', markersize=4, markevery=5)\n\nplt.plot(rounds, test_accs_fedprox, \n         color=colors['FedProx'], linestyle=line_styles['FedProx'], \n         linewidth=2, label='FedProx', marker='o', markersize=4, markevery=5)\n\nplt.plot(rounds, test_accs_fedprox_temp, \n         color=colors['FedProx+Temp'], linestyle=line_styles['FedProx+Temp'], \n         linewidth=2, label='FedProx+Temp', marker='s', markersize=4, markevery=5)\n\n# Formatting\nplt.xlabel('Communication Rounds', fontsize=14, fontweight='bold')\nplt.ylabel('Test Accuracy', fontsize=14, fontweight='bold')\nplt.title('Federated Learning Algorithm Comparison\\nTest Accuracy vs Communication Rounds', \n          fontsize=16, fontweight='bold', pad=20)\nplt.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)\nplt.legend(loc='lower right', fontsize=10, ncol=2, framealpha=0.95)\n\n# Set reasonable axis limits\nplt.xlim(1, len(test_accs_fedavg))\n\nplt.tight_layout()\nplt.show()\n\n# ========================================\n# PRINT FINAL ACCURACIES\n# ========================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"FINAL TEST ACCURACIES (Last Round)\")\nprint(\"=\"*80)\nprint(f\"FedAvg:          {test_accs_fedavg[-1]:.4f}\")\nprint(f\"FedAvg+Temp:     {test_accs_fedavg_temp[-1]:.4f}\")\nprint(f\"FedSAM:          {test_accs_fedsam[-1]:.4f}\")\nprint(f\"FedSAM+Temp:     {test_accs_fedsam_temp[-1]:.4f}\")\nprint(f\"FedLESAM:        {test_accs_fedlesam[-1]:.4f}\")\nprint(f\"FedLESAM+Temp:   {test_accs_fedlesam_temp[-1]:.4f}\")\nprint(f\"FedSMOO:         {test_accs_fedsmoo[-1]:.4f}\")\nprint(f\"FedSMOO+Temp:    {test_accs_fedsmoo_temp[-1]:.4f}\")\nprint(f\"FedGMT:          {test_accs_fedgmt[-1]:.4f}\")\nprint(f\"FedGMT+Temp:     {test_accs_fedgmt_temp[-1]:.4f}\")\nprint(f\"FedProx:         {test_accs_fedprox[-1]:.4f}\")\nprint(f\"FedProx+Temp:    {test_accs_fedprox_temp[-1]:.4f}\")\nprint(\"=\"*80)\n\n# Find best performing algorithm\nall_results = {\n    'FedAvg': test_accs_fedavg[-1],\n    'FedAvg+Temp': test_accs_fedavg_temp[-1],\n    'FedSAM': test_accs_fedsam[-1],\n    'FedSAM+Temp': test_accs_fedsam_temp[-1],\n    'FedLESAM': test_accs_fedlesam[-1],\n    'FedLESAM+Temp': test_accs_fedlesam_temp[-1],\n    'FedSMOO': test_accs_fedsmoo[-1],\n    'FedSMOO+Temp': test_accs_fedsmoo_temp[-1],\n    'FedGMT': test_accs_fedgmt[-1],vader\n    'FedGMT+Temp': test_accs_fedgmt_temp[-1],\n    'FedProx': test_accs_fedprox[-1],\n    'FedProx+Temp': test_accs_fedprox_temp[-1]\n}\n\nbest_algo = max(all_results, key=all_results.get)\nprint(f\"\\n🏆 BEST PERFORMING ALGORITHM: {best_algo} with {all_results[best_algo]:.4f} accuracy\")\nprint(\"=\"*80)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for alph in [0.05, 0.5, 5]:\n    print(\"#\"*80)\n    print(\"RUNNING EXPERIMENTS WITH ALPHA: \", alph)\n    print(\"#\"*80)\n    \n    train_loaders, val_loaders, test_loader, client_class_dist = load_and_partition_data(alpha=alph)\n\n    print(\"\\n[1/12] FedSMOO...\")\n    test_accs_fedsmoo, model_fedsmoo = run_fedsmoo()\n    \n    print(\"\\n[2/12] FedSMOO+Temp...\")\n    test_accs_fedsmoo_temp, tau_fedsmoo_temp, model_fedsmoo_temp = run_fedsmoo_temp()\n    \n    print(\"\\n[3/12] FedGMT...\")\n    test_accs_fedgmt, model_fedgmt = run_fedgmt()\n    \n    print(\"\\n[4/12] FedGMT+Temp...\")\n    test_accs_fedgmt_temp, tau_fedgmt_temp, model_fedgmt_temp = run_fedgmt_temp()\n    \n    continue\n    \n    import matplotlib.pyplot as plt\n    import numpy as np\n    \n    # ========================================\n    # PLOTTING ALL ALGORITHMS\n    # ========================================\n    \n    plt.figure(figsize=(14, 8))\n    \n    # Define colors and line styles for better distinction\n    colors = {\n        'FedAvg': '#1f77b4',\n        'FedAvg+Temp': '#aec7e8',\n        'FedSAM': '#ff7f0e',\n        'FedSAM+Temp': '#ffbb78',\n        'FedLESAM': '#2ca02c',\n        'FedLESAM+Temp': '#98df8a',\n        'FedSMOO': '#d62728',\n        'FedSMOO+Temp': '#ff9896',\n        'FedGMT': '#9467bd',\n        'FedGMT+Temp': '#c5b0d5',\n        'FedProx': '#8c564b',\n        'FedProx+Temp': '#c49c94'\n    }\n    \n    line_styles = {\n        'FedAvg': '-',\n        'FedAvg+Temp': '--',\n        'FedSAM': '-',\n        'FedSAM+Temp': '--',\n        'FedLESAM': '-',\n        'FedLESAM+Temp': '--',\n        'FedSMOO': '-',\n        'FedSMOO+Temp': '--',\n        'FedGMT': '-',\n        'FedGMT+Temp': '--',\n        'FedProx': '-',\n        'FedProx+Temp': '--'\n    }\n    \n    # Communication rounds (x-axis)\n    rounds = np.arange(1, len(test_accs_fedavg) + 1)\n    \n    # Plot all algorithms\n    plt.plot(rounds, test_accs_fedavg, \n             color=colors['FedAvg'], linestyle=line_styles['FedAvg'], \n             linewidth=2, label='FedAvg', marker='o', markersize=4, markevery=5)\n    \n    plt.plot(rounds, test_accs_fedavg_temp, \n             color=colors['FedAvg+Temp'], linestyle=line_styles['FedAvg+Temp'], \n             linewidth=2, label='FedAvg+Temp', marker='s', markersize=4, markevery=5)\n    \n    plt.plot(rounds, test_accs_fedsam, \n             color=colors['FedSAM'], linestyle=line_styles['FedSAM'], \n             linewidth=2, label='FedSAM', marker='o', markersize=4, markevery=5)\n    \n    plt.plot(rounds, test_accs_fedsam_temp, \n             color=colors['FedSAM+Temp'], linestyle=line_styles['FedSAM+Temp'], \n             linewidth=2, label='FedSAM+Temp', marker='s', markersize=4, markevery=5)\n    \n    plt.plot(rounds, test_accs_fedlesam, \n             color=colors['FedLESAM'], linestyle=line_styles['FedLESAM'], \n             linewidth=2, label='FedLESAM', marker='o', markersize=4, markevery=5)\n    \n    plt.plot(rounds, test_accs_fedlesam_temp, \n             color=colors['FedLESAM+Temp'], linestyle=line_styles['FedLESAM+Temp'], \n             linewidth=2, label='FedLESAM+Temp', marker='s', markersize=4, markevery=5)\n    \n    # plt.plot(rounds, test_accs_fedsmoo, \n    #          color=colors['FedSMOO'], linestyle=line_styles['FedSMOO'], \n    #          linewidth=2, label='FedSMOO', marker='o', markersize=4, markevery=5)\n    \n    # plt.plot(rounds, test_accs_fedsmoo_temp, \n    #          color=colors['FedSMOO+Temp'], linestyle=line_styles['FedSMOO+Temp'], \n    #          linewidth=2, label='FedSMOO+Temp', marker='s', markersize=4, markevery=5)\n    \n    # plt.plot(rounds, test_accs_fedgmt, \n    #          color=colors['FedGMT'], linestyle=line_styles['FedGMT'], \n    #          linewidth=2, label='FedGMT', marker='o', markersize=4, markevery=5)\n    \n    # plt.plot(rounds, test_accs_fedgmt_temp, \n    #          color=colors['FedGMT+Temp'], linestyle=line_styles['FedGMT+Temp'], \n    #          linewidth=2, label='FedGMT+Temp', marker='s', markersize=4, markevery=5)\n    \n    plt.plot(rounds, test_accs_fedprox, \n             color=colors['FedProx'], linestyle=line_styles['FedProx'], \n             linewidth=2, label='FedProx', marker='o', markersize=4, markevery=5)\n    \n    plt.plot(rounds, test_accs_fedprox_temp, \n             color=colors['FedProx+Temp'], linestyle=line_styles['FedProx+Temp'], \n             linewidth=2, label='FedProx+Temp', marker='s', markersize=4, markevery=5)\n    \n    # Formatting\n    plt.xlabel('Communication Rounds', fontsize=14, fontweight='bold')\n    plt.ylabel('Test Accuracy', fontsize=14, fontweight='bold')\n    plt.title('Federated Learning Algorithm Comparison\\nTest Accuracy vs Communication Rounds', \n              fontsize=16, fontweight='bold', pad=20)\n    plt.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)\n    plt.legend(loc='lower right', fontsize=10, ncol=2, framealpha=0.95)\n    \n    # Set reasonable axis limits\n    plt.xlim(1, len(test_accs_fedavg))\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # ========================================\n    # PRINT FINAL ACCURACIES\n    # ========================================\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"FINAL TEST ACCURACIES (Last Round)\")\n    print(\"=\"*80)\n    print(f\"FedAvg:          {test_accs_fedavg[-1]:.4f}\")\n    print(f\"FedAvg+Temp:     {test_accs_fedavg_temp[-1]:.4f}\")\n    print(f\"FedSAM:          {test_accs_fedsam[-1]:.4f}\")\n    print(f\"FedSAM+Temp:     {test_accs_fedsam_temp[-1]:.4f}\")\n    print(f\"FedLESAM:        {test_accs_fedlesam[-1]:.4f}\")\n    print(f\"FedLESAM+Temp:   {test_accs_fedlesam_temp[-1]:.4f}\")\n    # print(f\"FedSMOO:         {test_accs_fedsmoo[-1]:.4f}\")\n    # print(f\"FedSMOO+Temp:    {test_accs_fedsmoo_temp[-1]:.4f}\")\n    # print(f\"FedGMT:          {test_accs_fedgmt[-1]:.4f}\")\n    # print(f\"FedGMT+Temp:     {test_accs_fedgmt_temp[-1]:.4f}\")\n    print(f\"FedProx:         {test_accs_fedprox[-1]:.4f}\")\n    print(f\"FedProx+Temp:    {test_accs_fedprox_temp[-1]:.4f}\")\n    print(\"=\"*80)\n    \n    # Find best performing algorithm\n    all_results = {\n        'FedAvg': test_accs_fedavg[-1],\n        'FedAvg+Temp': test_accs_fedavg_temp[-1],\n        'FedSAM': test_accs_fedsam[-1],\n        'FedSAM+Temp': test_accs_fedsam_temp[-1],\n        'FedLESAM': test_accs_fedlesam[-1],\n        'FedLESAM+Temp': test_accs_fedlesam_temp[-1],\n        # 'FedSMOO': test_accs_fedsmoo[-1],\n        # 'FedSMOO+Temp': test_accs_fedsmoo_temp[-1],\n        # 'FedGMT': test_accs_fedgmt[-1],\n        # 'FedGMT+Temp': test_accs_fedgmt_temp[-1],\n        'FedProx': test_accs_fedprox[-1],\n        'FedProx+Temp': test_accs_fedprox_temp[-1]\n    }\n    \n    best_algo = max(all_results, key=all_results.get)\n    print(f\"\\n🏆 BEST PERFORMING ALGORITHM: {best_algo} with {all_results[best_algo]:.4f} accuracy\")\n    print(\"=\"*80)\n    ","metadata":{"execution":{"iopub.status.busy":"2025-12-27T16:25:17.404773Z","iopub.execute_input":"2025-12-27T16:25:17.405587Z","iopub.status.idle":"2025-12-27T18:50:41.400514Z","shell.execute_reply.started":"2025-12-27T16:25:17.405551Z","shell.execute_reply":"2025-12-27T18:50:41.399717Z"},"trusted":true},"outputs":[{"name":"stdout","text":"################################################################################\nRUNNING EXPERIMENTS WITH ALPHA:  0.05\n################################################################################\nData partitioning complete.\nClient 0 class dist: ['0:0.00', '1:0.00', '2:1.00', '3:0.00', '4:0.00', '5:0.00', '6:0.00', '7:0.00', '8:0.00', '9:0.00']\nClient 1 class dist: ['0:0.00', '1:0.00', '2:0.00', '3:0.00', '4:0.00', '5:0.00', '6:0.99', '7:0.00', '8:0.01', '9:0.00']\nClient 2 class dist: ['0:0.00', '1:0.00', '2:0.00', '3:0.00', '4:0.61', '5:0.39', '6:0.00', '7:0.00', '8:0.00', '9:0.00']\nClient 3 class dist: ['0:0.48', '1:0.52', '2:0.00', '3:0.00', '4:0.00', '5:0.00', '6:0.00', '7:0.00', '8:0.00', '9:0.00']\nClient 4 class dist: ['0:0.02', '1:0.00', '2:0.00', '3:0.00', '4:0.02', '5:0.33', '6:0.00', '7:0.63', '8:0.00', '9:0.00']\nClient 5 class dist: ['0:0.00', '1:0.00', '2:0.00', '3:0.00', '4:0.00', '5:0.00', '6:0.00', '7:0.00', '8:1.00', '9:0.00']\nClient 6 class dist: ['0:0.00', '1:0.00', '2:0.69', '3:0.31', '4:0.00', '5:0.00', '6:0.00', '7:0.00', '8:0.00', '9:0.00']\nClient 7 class dist: ['0:0.02', '1:0.00', '2:0.00', '3:0.00', '4:0.96', '5:0.00', '6:0.00', '7:0.02', '8:0.00', '9:0.00']\nClient 8 class dist: ['0:0.00', '1:0.00', '2:0.00', '3:0.96', '4:0.00', '5:0.00', '6:0.00', '7:0.03', '8:0.00', '9:0.00']\nClient 9 class dist: ['0:0.00', '1:0.00', '2:0.00', '3:0.00', '4:0.00', '5:0.00', '6:0.00', '7:0.00', '8:0.00', '9:0.99']\n\n[1/12] FedSMOO...\n\n============================================================\nSTARTING FedSMOO TRAINING\n============================================================\n[FedSMOO] R1/50 Test Acc: 0.1780 | Time: 17.99s\n[FedSMOO] R2/50 Test Acc: 0.2679 | Time: 15.58s\n[FedSMOO] R3/50 Test Acc: 0.2806 | Time: 15.72s\n[FedSMOO] R4/50 Test Acc: 0.2739 | Time: 15.90s\n[FedSMOO] R5/50 Test Acc: 0.2854 | Time: 16.17s\n[FedSMOO] R6/50 Test Acc: 0.2990 | Time: 16.37s\n[FedSMOO] R7/50 Test Acc: 0.3111 | Time: 16.67s\n[FedSMOO] R8/50 Test Acc: 0.3264 | Time: 17.30s\n[FedSMOO] R9/50 Test Acc: 0.3306 | Time: 17.20s\n[FedSMOO] R10/50 Test Acc: 0.3301 | Time: 16.77s\n[FedSMOO] R11/50 Test Acc: 0.3340 | Time: 16.59s\n[FedSMOO] R12/50 Test Acc: 0.3306 | Time: 16.61s\n[FedSMOO] R13/50 Test Acc: 0.3316 | Time: 16.70s\n[FedSMOO] R14/50 Test Acc: 0.3089 | Time: 16.83s\n[FedSMOO] R15/50 Test Acc: 0.3351 | Time: 16.66s\n[FedSMOO] R16/50 Test Acc: 0.3289 | Time: 16.63s\n[FedSMOO] R17/50 Test Acc: 0.3387 | Time: 16.67s\n[FedSMOO] R18/50 Test Acc: 0.3265 | Time: 16.75s\n[FedSMOO] R19/50 Test Acc: 0.3568 | Time: 16.76s\n[FedSMOO] R20/50 Test Acc: 0.3614 | Time: 16.74s\n[FedSMOO] R21/50 Test Acc: 0.3659 | Time: 16.91s\n[FedSMOO] R22/50 Test Acc: 0.3633 | Time: 16.77s\n[FedSMOO] R23/50 Test Acc: 0.3741 | Time: 16.69s\n[FedSMOO] R24/50 Test Acc: 0.3702 | Time: 16.69s\n[FedSMOO] R25/50 Test Acc: 0.3726 | Time: 16.67s\n[FedSMOO] R26/50 Test Acc: 0.3399 | Time: 16.69s\n[FedSMOO] R27/50 Test Acc: 0.3839 | Time: 16.61s\n[FedSMOO] R28/50 Test Acc: 0.3766 | Time: 16.63s\n[FedSMOO] R29/50 Test Acc: 0.3886 | Time: 16.71s\n[FedSMOO] R30/50 Test Acc: 0.3901 | Time: 16.73s\n[FedSMOO] R31/50 Test Acc: 0.3862 | Time: 16.72s\n[FedSMOO] R32/50 Test Acc: 0.3823 | Time: 16.74s\n[FedSMOO] R33/50 Test Acc: 0.3935 | Time: 16.71s\n[FedSMOO] R34/50 Test Acc: 0.3799 | Time: 16.73s\n[FedSMOO] R35/50 Test Acc: 0.3870 | Time: 16.77s\n[FedSMOO] R36/50 Test Acc: 0.3906 | Time: 16.74s\n[FedSMOO] R37/50 Test Acc: 0.3916 | Time: 16.72s\n[FedSMOO] R38/50 Test Acc: 0.3881 | Time: 16.67s\n[FedSMOO] R39/50 Test Acc: 0.3852 | Time: 16.70s\n[FedSMOO] R40/50 Test Acc: 0.3966 | Time: 16.72s\n[FedSMOO] R41/50 Test Acc: 0.3958 | Time: 16.74s\n[FedSMOO] R42/50 Test Acc: 0.3917 | Time: 16.74s\n[FedSMOO] R43/50 Test Acc: 0.3934 | Time: 16.89s\n[FedSMOO] R44/50 Test Acc: 0.3864 | Time: 16.75s\n[FedSMOO] R45/50 Test Acc: 0.3985 | Time: 16.71s\n[FedSMOO] R46/50 Test Acc: 0.4008 | Time: 16.73s\n[FedSMOO] R47/50 Test Acc: 0.3959 | Time: 16.77s\n[FedSMOO] R48/50 Test Acc: 0.3818 | Time: 16.71s\n[FedSMOO] R49/50 Test Acc: 0.3845 | Time: 16.66s\n[FedSMOO] R50/50 Test Acc: 0.3859 | Time: 16.76s\n\n[2/12] FedSMOO+Temp...\n\n============================================================\nSTARTING FedSMOO + TEMP TRAINING\n============================================================\n[FedSMOO+Temp] R1/50 Test Acc: 0.1593 | Avg τ: 0.8463 | Time: 17.12s\n[FedSMOO+Temp] R2/50 Test Acc: 0.2293 | Avg τ: 0.7015 | Time: 16.95s\n[FedSMOO+Temp] R3/50 Test Acc: 0.2712 | Avg τ: 0.6276 | Time: 16.97s\n[FedSMOO+Temp] R4/50 Test Acc: 0.2918 | Avg τ: 0.5872 | Time: 17.05s\n[FedSMOO+Temp] R5/50 Test Acc: 0.2967 | Avg τ: 0.5549 | Time: 17.11s\n[FedSMOO+Temp] R6/50 Test Acc: 0.3032 | Avg τ: 0.5293 | Time: 16.99s\n[FedSMOO+Temp] R7/50 Test Acc: 0.3277 | Avg τ: 0.5071 | Time: 17.01s\n[FedSMOO+Temp] R8/50 Test Acc: 0.3073 | Avg τ: 0.4990 | Time: 16.99s\n[FedSMOO+Temp] R9/50 Test Acc: 0.3214 | Avg τ: 0.4891 | Time: 17.02s\n[FedSMOO+Temp] R10/50 Test Acc: 0.3165 | Avg τ: 0.4677 | Time: 17.17s\n[FedSMOO+Temp] R11/50 Test Acc: 0.3257 | Avg τ: 0.4716 | Time: 17.08s\n[FedSMOO+Temp] R12/50 Test Acc: 0.2800 | Avg τ: 0.4404 | Time: 16.96s\n[FedSMOO+Temp] R13/50 Test Acc: 0.3376 | Avg τ: 0.4399 | Time: 16.95s\n[FedSMOO+Temp] R14/50 Test Acc: 0.2643 | Avg τ: 0.4403 | Time: 17.14s\n[FedSMOO+Temp] R15/50 Test Acc: 0.3114 | Avg τ: 0.4525 | Time: 17.04s\n[FedSMOO+Temp] R16/50 Test Acc: 0.2709 | Avg τ: 0.4265 | Time: 17.03s\n[FedSMOO+Temp] R17/50 Test Acc: 0.3224 | Avg τ: 0.4679 | Time: 16.94s\n[FedSMOO+Temp] R18/50 Test Acc: 0.2478 | Avg τ: 0.4241 | Time: 17.04s\n[FedSMOO+Temp] R19/50 Test Acc: 0.3303 | Avg τ: 0.4909 | Time: 17.01s\n[FedSMOO+Temp] R20/50 Test Acc: 0.2708 | Avg τ: 0.4372 | Time: 16.88s\n[FedSMOO+Temp] R21/50 Test Acc: 0.3488 | Avg τ: 0.4643 | Time: 16.93s\n[FedSMOO+Temp] R22/50 Test Acc: 0.2556 | Avg τ: 0.4041 | Time: 16.87s\n[FedSMOO+Temp] R23/50 Test Acc: 0.3240 | Avg τ: 0.4574 | Time: 16.87s\n[FedSMOO+Temp] R24/50 Test Acc: 0.2878 | Avg τ: 0.4099 | Time: 16.95s\n[FedSMOO+Temp] R25/50 Test Acc: 0.3244 | Avg τ: 0.4612 | Time: 16.97s\n[FedSMOO+Temp] R26/50 Test Acc: 0.2775 | Avg τ: 0.4095 | Time: 16.86s\n[FedSMOO+Temp] R27/50 Test Acc: 0.3176 | Avg τ: 0.4561 | Time: 16.92s\n[FedSMOO+Temp] R28/50 Test Acc: 0.2702 | Avg τ: 0.4031 | Time: 16.89s\n[FedSMOO+Temp] R29/50 Test Acc: 0.3387 | Avg τ: 0.4394 | Time: 16.92s\n[FedSMOO+Temp] R30/50 Test Acc: 0.3193 | Avg τ: 0.3656 | Time: 16.96s\n[FedSMOO+Temp] R31/50 Test Acc: 0.3115 | Avg τ: 0.4134 | Time: 16.92s\n[FedSMOO+Temp] R32/50 Test Acc: 0.3431 | Avg τ: 0.3785 | Time: 16.93s\n[FedSMOO+Temp] R33/50 Test Acc: 0.3213 | Avg τ: 0.3875 | Time: 16.96s\n[FedSMOO+Temp] R34/50 Test Acc: 0.3187 | Avg τ: 0.3861 | Time: 16.95s\n[FedSMOO+Temp] R35/50 Test Acc: 0.3092 | Avg τ: 0.3967 | Time: 16.89s\n[FedSMOO+Temp] R36/50 Test Acc: 0.3313 | Avg τ: 0.3985 | Time: 16.91s\n[FedSMOO+Temp] R37/50 Test Acc: 0.3221 | Avg τ: 0.4184 | Time: 16.94s\n[FedSMOO+Temp] R38/50 Test Acc: 0.3273 | Avg τ: 0.3725 | Time: 16.89s\n[FedSMOO+Temp] R39/50 Test Acc: 0.3416 | Avg τ: 0.3895 | Time: 17.09s\n[FedSMOO+Temp] R40/50 Test Acc: 0.3175 | Avg τ: 0.3861 | Time: 17.21s\n[FedSMOO+Temp] R41/50 Test Acc: 0.3426 | Avg τ: 0.3694 | Time: 17.12s\n[FedSMOO+Temp] R42/50 Test Acc: 0.3388 | Avg τ: 0.3791 | Time: 17.33s\n[FedSMOO+Temp] R43/50 Test Acc: 0.3306 | Avg τ: 0.4094 | Time: 17.28s\n[FedSMOO+Temp] R44/50 Test Acc: 0.3411 | Avg τ: 0.3647 | Time: 17.04s\n[FedSMOO+Temp] R45/50 Test Acc: 0.3563 | Avg τ: 0.3961 | Time: 17.02s\n[FedSMOO+Temp] R46/50 Test Acc: 0.3558 | Avg τ: 0.3649 | Time: 17.02s\n[FedSMOO+Temp] R47/50 Test Acc: 0.3453 | Avg τ: 0.3661 | Time: 17.09s\n[FedSMOO+Temp] R48/50 Test Acc: 0.3431 | Avg τ: 0.4037 | Time: 17.04s\n[FedSMOO+Temp] R49/50 Test Acc: 0.3516 | Avg τ: 0.3529 | Time: 16.95s\n[FedSMOO+Temp] R50/50 Test Acc: 0.3543 | Avg τ: 0.3596 | Time: 16.95s\n\n[3/12] FedGMT...\n\n============================================================\nSTARTING FedGMT TRAINING\n============================================================\n[FedGMT] R1/50 Test Acc: 0.1230 | Time: 11.71s\n[FedGMT] R2/50 Test Acc: 0.2289 | Time: 11.72s\n[FedGMT] R3/50 Test Acc: 0.2354 | Time: 11.68s\n[FedGMT] R4/50 Test Acc: 0.2266 | Time: 11.70s\n[FedGMT] R5/50 Test Acc: 0.2368 | Time: 11.68s\n[FedGMT] R6/50 Test Acc: 0.2421 | Time: 11.67s\n[FedGMT] R7/50 Test Acc: 0.2652 | Time: 11.65s\n[FedGMT] R8/50 Test Acc: 0.2827 | Time: 11.67s\n[FedGMT] R9/50 Test Acc: 0.2757 | Time: 11.72s\n[FedGMT] R10/50 Test Acc: 0.2628 | Time: 11.71s\n[FedGMT] R11/50 Test Acc: 0.2832 | Time: 11.72s\n[FedGMT] R12/50 Test Acc: 0.2825 | Time: 11.74s\n[FedGMT] R13/50 Test Acc: 0.2837 | Time: 11.72s\n[FedGMT] R14/50 Test Acc: 0.2853 | Time: 11.71s\n[FedGMT] R15/50 Test Acc: 0.2872 | Time: 11.78s\n[FedGMT] R16/50 Test Acc: 0.2853 | Time: 11.73s\n[FedGMT] R17/50 Test Acc: 0.2876 | Time: 11.71s\n[FedGMT] R18/50 Test Acc: 0.3005 | Time: 11.69s\n[FedGMT] R19/50 Test Acc: 0.2859 | Time: 11.63s\n[FedGMT] R20/50 Test Acc: 0.2981 | Time: 11.66s\n[FedGMT] R21/50 Test Acc: 0.2945 | Time: 11.67s\n[FedGMT] R22/50 Test Acc: 0.3081 | Time: 11.64s\n[FedGMT] R23/50 Test Acc: 0.3060 | Time: 11.62s\n[FedGMT] R24/50 Test Acc: 0.3026 | Time: 11.74s\n[FedGMT] R25/50 Test Acc: 0.2931 | Time: 11.81s\n[FedGMT] R26/50 Test Acc: 0.3168 | Time: 11.75s\n[FedGMT] R27/50 Test Acc: 0.3044 | Time: 11.74s\n[FedGMT] R28/50 Test Acc: 0.3141 | Time: 11.71s\n[FedGMT] R29/50 Test Acc: 0.3106 | Time: 11.61s\n[FedGMT] R30/50 Test Acc: 0.3097 | Time: 11.70s\n[FedGMT] R31/50 Test Acc: 0.3177 | Time: 11.70s\n[FedGMT] R32/50 Test Acc: 0.3114 | Time: 11.70s\n[FedGMT] R33/50 Test Acc: 0.3196 | Time: 11.75s\n[FedGMT] R34/50 Test Acc: 0.3234 | Time: 11.74s\n[FedGMT] R35/50 Test Acc: 0.3168 | Time: 11.73s\n[FedGMT] R36/50 Test Acc: 0.3380 | Time: 11.74s\n[FedGMT] R37/50 Test Acc: 0.3157 | Time: 11.75s\n[FedGMT] R38/50 Test Acc: 0.3206 | Time: 11.71s\n[FedGMT] R39/50 Test Acc: 0.3262 | Time: 11.69s\n[FedGMT] R40/50 Test Acc: 0.3253 | Time: 11.70s\n[FedGMT] R41/50 Test Acc: 0.3360 | Time: 11.64s\n[FedGMT] R42/50 Test Acc: 0.3309 | Time: 11.64s\n[FedGMT] R43/50 Test Acc: 0.3275 | Time: 11.65s\n[FedGMT] R44/50 Test Acc: 0.3310 | Time: 11.64s\n[FedGMT] R45/50 Test Acc: 0.3286 | Time: 11.61s\n[FedGMT] R46/50 Test Acc: 0.3368 | Time: 11.60s\n[FedGMT] R47/50 Test Acc: 0.3382 | Time: 11.55s\n[FedGMT] R48/50 Test Acc: 0.3461 | Time: 11.56s\n[FedGMT] R49/50 Test Acc: 0.3434 | Time: 11.64s\n[FedGMT] R50/50 Test Acc: 0.3334 | Time: 11.58s\n\n[4/12] FedGMT+Temp...\n\n============================================================\nSTARTING FedGMT + TEMP TRAINING\n============================================================\n[FedGMT+Temp] R1/50 Test Acc: 0.0992 | Avg τ: 0.8252 | Time: 11.79s\n[FedGMT+Temp] R2/50 Test Acc: 0.2430 | Avg τ: 0.6575 | Time: 11.88s\n[FedGMT+Temp] R3/50 Test Acc: 0.2585 | Avg τ: 0.6206 | Time: 11.85s\n[FedGMT+Temp] R4/50 Test Acc: 0.2329 | Avg τ: 0.5894 | Time: 11.84s\n[FedGMT+Temp] R5/50 Test Acc: 0.2221 | Avg τ: 0.5781 | Time: 11.81s\n[FedGMT+Temp] R6/50 Test Acc: 0.2630 | Avg τ: 0.5610 | Time: 11.81s\n[FedGMT+Temp] R7/50 Test Acc: 0.2827 | Avg τ: 0.5645 | Time: 11.88s\n[FedGMT+Temp] R8/50 Test Acc: 0.2957 | Avg τ: 0.5529 | Time: 11.86s\n[FedGMT+Temp] R9/50 Test Acc: 0.2976 | Avg τ: 0.5394 | Time: 11.81s\n[FedGMT+Temp] R10/50 Test Acc: 0.2721 | Avg τ: 0.5255 | Time: 11.80s\n[FedGMT+Temp] R11/50 Test Acc: 0.2929 | Avg τ: 0.5231 | Time: 11.84s\n[FedGMT+Temp] R12/50 Test Acc: 0.2002 | Avg τ: 0.5450 | Time: 11.84s\n[FedGMT+Temp] R13/50 Test Acc: 0.2961 | Avg τ: 0.4874 | Time: 11.86s\n[FedGMT+Temp] R14/50 Test Acc: 0.2919 | Avg τ: 0.4683 | Time: 11.79s\n[FedGMT+Temp] R15/50 Test Acc: 0.3027 | Avg τ: 0.4602 | Time: 11.83s\n[FedGMT+Temp] R16/50 Test Acc: 0.2359 | Avg τ: 0.4497 | Time: 11.88s\n[FedGMT+Temp] R17/50 Test Acc: 0.2801 | Avg τ: 0.4346 | Time: 11.88s\n[FedGMT+Temp] R18/50 Test Acc: 0.2705 | Avg τ: 0.4712 | Time: 11.94s\n[FedGMT+Temp] R19/50 Test Acc: 0.2342 | Avg τ: 0.4361 | Time: 11.85s\n[FedGMT+Temp] R20/50 Test Acc: 0.1231 | Avg τ: 0.4485 | Time: 11.84s\n[FedGMT+Temp] R21/50 Test Acc: 0.2827 | Avg τ: 0.4237 | Time: 11.81s\n[FedGMT+Temp] R22/50 Test Acc: 0.2301 | Avg τ: 0.4168 | Time: 11.84s\n[FedGMT+Temp] R23/50 Test Acc: 0.2519 | Avg τ: 0.4123 | Time: 11.84s\n[FedGMT+Temp] R24/50 Test Acc: 0.1889 | Avg τ: 0.4137 | Time: 11.84s\n[FedGMT+Temp] R25/50 Test Acc: 0.2789 | Avg τ: 0.3971 | Time: 11.80s\n[FedGMT+Temp] R26/50 Test Acc: 0.2927 | Avg τ: 0.3788 | Time: 11.84s\n[FedGMT+Temp] R27/50 Test Acc: 0.2704 | Avg τ: 0.3987 | Time: 11.80s\n[FedGMT+Temp] R28/50 Test Acc: 0.2667 | Avg τ: 0.4064 | Time: 11.81s\n[FedGMT+Temp] R29/50 Test Acc: 0.2340 | Avg τ: 0.3847 | Time: 11.92s\n[FedGMT+Temp] R30/50 Test Acc: 0.1688 | Avg τ: 0.3832 | Time: 11.94s\n[FedGMT+Temp] R31/50 Test Acc: 0.1448 | Avg τ: 0.3790 | Time: 11.83s\n[FedGMT+Temp] R32/50 Test Acc: 0.2008 | Avg τ: 0.3996 | Time: 11.80s\n[FedGMT+Temp] R33/50 Test Acc: 0.1997 | Avg τ: 0.3820 | Time: 11.79s\n[FedGMT+Temp] R34/50 Test Acc: 0.2159 | Avg τ: 0.3776 | Time: 11.82s\n[FedGMT+Temp] R35/50 Test Acc: 0.2001 | Avg τ: 0.3952 | Time: 11.88s\n[FedGMT+Temp] R36/50 Test Acc: 0.1827 | Avg τ: 0.3810 | Time: 11.80s\n[FedGMT+Temp] R37/50 Test Acc: 0.1026 | Avg τ: 0.3910 | Time: 11.85s\n[FedGMT+Temp] R38/50 Test Acc: 0.2660 | Avg τ: 0.3858 | Time: 11.88s\n[FedGMT+Temp] R39/50 Test Acc: 0.2855 | Avg τ: 0.4081 | Time: 11.90s\n[FedGMT+Temp] R40/50 Test Acc: 0.2315 | Avg τ: 0.3822 | Time: 11.99s\n[FedGMT+Temp] R41/50 Test Acc: 0.2134 | Avg τ: 0.3823 | Time: 11.96s\n[FedGMT+Temp] R42/50 Test Acc: 0.1235 | Avg τ: 0.3815 | Time: 11.90s\n[FedGMT+Temp] R43/50 Test Acc: 0.2356 | Avg τ: 0.3822 | Time: 11.87s\n[FedGMT+Temp] R44/50 Test Acc: 0.1634 | Avg τ: 0.3791 | Time: 11.93s\n[FedGMT+Temp] R45/50 Test Acc: 0.2622 | Avg τ: 0.3823 | Time: 11.93s\n[FedGMT+Temp] R46/50 Test Acc: 0.1065 | Avg τ: 0.4079 | Time: 11.88s\n[FedGMT+Temp] R47/50 Test Acc: 0.2660 | Avg τ: 0.3935 | Time: 11.86s\n[FedGMT+Temp] R48/50 Test Acc: 0.2066 | Avg τ: 0.3878 | Time: 11.86s\n[FedGMT+Temp] R49/50 Test Acc: 0.1914 | Avg τ: 0.4032 | Time: 11.90s\n[FedGMT+Temp] R50/50 Test Acc: 0.1161 | Avg τ: 0.3814 | Time: 11.92s\n################################################################################\nRUNNING EXPERIMENTS WITH ALPHA:  0.5\n################################################################################\nData partitioning complete.\nClient 0 class dist: ['0:0.03', '1:0.01', '2:0.26', '3:0.23', '4:0.09', '5:0.00', '6:0.38', '7:0.00', '8:0.00', '9:0.00']\nClient 1 class dist: ['0:0.05', '1:0.00', '2:0.23', '3:0.06', '4:0.02', '5:0.00', '6:0.00', '7:0.06', '8:0.18', '9:0.40']\nClient 2 class dist: ['0:0.04', '1:0.02', '2:0.00', '3:0.20', '4:0.00', '5:0.70', '6:0.04', '7:0.00', '8:0.00', '9:0.00']\nClient 3 class dist: ['0:0.40', '1:0.60', '2:0.00', '3:0.00', '4:0.00', '5:0.00', '6:0.00', '7:0.00', '8:0.00', '9:0.00']\nClient 4 class dist: ['0:0.16', '1:0.09', '2:0.00', '3:0.06', '4:0.21', '5:0.06', '6:0.10', '7:0.31', '8:0.00', '9:0.00']\nClient 5 class dist: ['0:0.11', '1:0.07', '2:0.35', '3:0.12', '4:0.03', '5:0.17', '6:0.03', '7:0.00', '8:0.10', '9:0.02']\nClient 6 class dist: ['0:0.01', '1:0.06', '2:0.11', '3:0.01', '4:0.00', '5:0.06', '6:0.01', '7:0.53', '8:0.00', '9:0.22']\nClient 7 class dist: ['0:0.12', '1:0.02', '2:0.03', '3:0.18', '4:0.06', '5:0.05', '6:0.11', '7:0.13', '8:0.01', '9:0.29']\nClient 8 class dist: ['0:0.01', '1:0.02', '2:0.04', '3:0.01', '4:0.12', '5:0.01', '6:0.02', '7:0.13', '8:0.65', '9:0.00']\nClient 9 class dist: ['0:0.01', '1:0.07', '2:0.05', '3:0.01', '4:0.38', '5:0.00', '6:0.01', '7:0.10', '8:0.00', '9:0.36']\n\n[1/12] FedSMOO...\n\n============================================================\nSTARTING FedSMOO TRAINING\n============================================================\n[FedSMOO] R1/50 Test Acc: 0.2842 | Time: 17.08s\n[FedSMOO] R2/50 Test Acc: 0.3655 | Time: 17.13s\n[FedSMOO] R3/50 Test Acc: 0.4415 | Time: 17.06s\n[FedSMOO] R4/50 Test Acc: 0.4802 | Time: 17.06s\n[FedSMOO] R5/50 Test Acc: 0.5229 | Time: 16.96s\n[FedSMOO] R6/50 Test Acc: 0.5482 | Time: 17.02s\n[FedSMOO] R7/50 Test Acc: 0.5514 | Time: 17.16s\n[FedSMOO] R8/50 Test Acc: 0.5607 | Time: 17.07s\n[FedSMOO] R9/50 Test Acc: 0.5667 | Time: 17.17s\n[FedSMOO] R10/50 Test Acc: 0.5652 | Time: 17.11s\n[FedSMOO] R11/50 Test Acc: 0.5751 | Time: 17.05s\n[FedSMOO] R12/50 Test Acc: 0.5704 | Time: 17.04s\n[FedSMOO] R13/50 Test Acc: 0.5782 | Time: 17.01s\n[FedSMOO] R14/50 Test Acc: 0.5835 | Time: 17.19s\n[FedSMOO] R15/50 Test Acc: 0.5769 | Time: 17.06s\n[FedSMOO] R16/50 Test Acc: 0.5851 | Time: 17.09s\n[FedSMOO] R17/50 Test Acc: 0.5777 | Time: 17.11s\n[FedSMOO] R18/50 Test Acc: 0.5941 | Time: 17.06s\n[FedSMOO] R19/50 Test Acc: 0.5883 | Time: 17.15s\n[FedSMOO] R20/50 Test Acc: 0.5974 | Time: 17.01s\n[FedSMOO] R21/50 Test Acc: 0.5952 | Time: 17.11s\n[FedSMOO] R22/50 Test Acc: 0.5911 | Time: 17.06s\n[FedSMOO] R23/50 Test Acc: 0.6067 | Time: 17.00s\n[FedSMOO] R24/50 Test Acc: 0.6023 | Time: 17.04s\n[FedSMOO] R25/50 Test Acc: 0.6086 | Time: 16.95s\n[FedSMOO] R26/50 Test Acc: 0.6172 | Time: 17.05s\n[FedSMOO] R27/50 Test Acc: 0.6165 | Time: 17.45s\n[FedSMOO] R28/50 Test Acc: 0.6128 | Time: 17.30s\n[FedSMOO] R29/50 Test Acc: 0.6192 | Time: 17.31s\n[FedSMOO] R30/50 Test Acc: 0.6224 | Time: 17.44s\n[FedSMOO] R31/50 Test Acc: 0.6198 | Time: 17.46s\n[FedSMOO] R32/50 Test Acc: 0.6153 | Time: 17.44s\n[FedSMOO] R33/50 Test Acc: 0.6183 | Time: 17.32s\n[FedSMOO] R34/50 Test Acc: 0.6244 | Time: 17.41s\n[FedSMOO] R35/50 Test Acc: 0.6148 | Time: 17.46s\n[FedSMOO] R36/50 Test Acc: 0.6276 | Time: 17.26s\n[FedSMOO] R37/50 Test Acc: 0.6222 | Time: 17.26s\n[FedSMOO] R38/50 Test Acc: 0.6223 | Time: 17.32s\n[FedSMOO] R39/50 Test Acc: 0.6200 | Time: 17.34s\n[FedSMOO] R40/50 Test Acc: 0.6236 | Time: 17.27s\n[FedSMOO] R41/50 Test Acc: 0.6258 | Time: 17.33s\n[FedSMOO] R42/50 Test Acc: 0.6267 | Time: 17.38s\n[FedSMOO] R43/50 Test Acc: 0.6223 | Time: 17.34s\n[FedSMOO] R44/50 Test Acc: 0.6247 | Time: 17.28s\n[FedSMOO] R45/50 Test Acc: 0.6289 | Time: 17.22s\n[FedSMOO] R46/50 Test Acc: 0.6237 | Time: 17.31s\n[FedSMOO] R47/50 Test Acc: 0.6276 | Time: 17.24s\n[FedSMOO] R48/50 Test Acc: 0.6341 | Time: 17.28s\n[FedSMOO] R49/50 Test Acc: 0.6333 | Time: 17.31s\n[FedSMOO] R50/50 Test Acc: 0.6288 | Time: 17.46s\n\n[2/12] FedSMOO+Temp...\n\n============================================================\nSTARTING FedSMOO + TEMP TRAINING\n============================================================\n[FedSMOO+Temp] R1/50 Test Acc: 0.2801 | Avg τ: 0.8909 | Time: 17.62s\n[FedSMOO+Temp] R2/50 Test Acc: 0.3922 | Avg τ: 0.7359 | Time: 17.61s\n[FedSMOO+Temp] R3/50 Test Acc: 0.4416 | Avg τ: 0.6511 | Time: 17.72s\n[FedSMOO+Temp] R4/50 Test Acc: 0.4568 | Avg τ: 0.5907 | Time: 17.71s\n[FedSMOO+Temp] R5/50 Test Acc: 0.5228 | Avg τ: 0.5344 | Time: 17.67s\n[FedSMOO+Temp] R6/50 Test Acc: 0.5367 | Avg τ: 0.4971 | Time: 17.60s\n[FedSMOO+Temp] R7/50 Test Acc: 0.5462 | Avg τ: 0.4916 | Time: 17.65s\n[FedSMOO+Temp] R8/50 Test Acc: 0.5394 | Avg τ: 0.4734 | Time: 17.70s\n[FedSMOO+Temp] R9/50 Test Acc: 0.5439 | Avg τ: 0.4507 | Time: 17.59s\n[FedSMOO+Temp] R10/50 Test Acc: 0.5548 | Avg τ: 0.4327 | Time: 17.61s\n[FedSMOO+Temp] R11/50 Test Acc: 0.5699 | Avg τ: 0.4215 | Time: 17.61s\n[FedSMOO+Temp] R12/50 Test Acc: 0.5746 | Avg τ: 0.4067 | Time: 17.71s\n[FedSMOO+Temp] R13/50 Test Acc: 0.5705 | Avg τ: 0.3853 | Time: 17.72s\n[FedSMOO+Temp] R14/50 Test Acc: 0.5708 | Avg τ: 0.3733 | Time: 17.73s\n[FedSMOO+Temp] R15/50 Test Acc: 0.5722 | Avg τ: 0.3636 | Time: 17.71s\n[FedSMOO+Temp] R16/50 Test Acc: 0.5882 | Avg τ: 0.3441 | Time: 17.70s\n[FedSMOO+Temp] R17/50 Test Acc: 0.5724 | Avg τ: 0.3283 | Time: 17.72s\n[FedSMOO+Temp] R18/50 Test Acc: 0.5812 | Avg τ: 0.3100 | Time: 17.73s\n[FedSMOO+Temp] R19/50 Test Acc: 0.6025 | Avg τ: 0.3005 | Time: 17.70s\n[FedSMOO+Temp] R20/50 Test Acc: 0.5750 | Avg τ: 0.2929 | Time: 17.63s\n[FedSMOO+Temp] R21/50 Test Acc: 0.5989 | Avg τ: 0.2889 | Time: 17.71s\n[FedSMOO+Temp] R22/50 Test Acc: 0.5733 | Avg τ: 0.2774 | Time: 17.65s\n[FedSMOO+Temp] R23/50 Test Acc: 0.6065 | Avg τ: 0.2771 | Time: 17.65s\n[FedSMOO+Temp] R24/50 Test Acc: 0.5848 | Avg τ: 0.2692 | Time: 17.63s\n[FedSMOO+Temp] R25/50 Test Acc: 0.5898 | Avg τ: 0.2626 | Time: 17.64s\n[FedSMOO+Temp] R26/50 Test Acc: 0.5900 | Avg τ: 0.2738 | Time: 17.64s\n[FedSMOO+Temp] R27/50 Test Acc: 0.6003 | Avg τ: 0.2780 | Time: 17.65s\n[FedSMOO+Temp] R28/50 Test Acc: 0.6079 | Avg τ: 0.2733 | Time: 17.63s\n[FedSMOO+Temp] R29/50 Test Acc: 0.6008 | Avg τ: 0.2740 | Time: 17.62s\n[FedSMOO+Temp] R30/50 Test Acc: 0.5960 | Avg τ: 0.2739 | Time: 17.71s\n[FedSMOO+Temp] R31/50 Test Acc: 0.6058 | Avg τ: 0.2828 | Time: 17.64s\n[FedSMOO+Temp] R32/50 Test Acc: 0.5981 | Avg τ: 0.2803 | Time: 17.62s\n[FedSMOO+Temp] R33/50 Test Acc: 0.6138 | Avg τ: 0.2782 | Time: 17.59s\n[FedSMOO+Temp] R34/50 Test Acc: 0.6082 | Avg τ: 0.2821 | Time: 17.68s\n[FedSMOO+Temp] R35/50 Test Acc: 0.6132 | Avg τ: 0.2909 | Time: 17.67s\n[FedSMOO+Temp] R36/50 Test Acc: 0.5981 | Avg τ: 0.2996 | Time: 17.64s\n[FedSMOO+Temp] R37/50 Test Acc: 0.6107 | Avg τ: 0.3115 | Time: 17.62s\n[FedSMOO+Temp] R38/50 Test Acc: 0.6080 | Avg τ: 0.3160 | Time: 17.75s\n[FedSMOO+Temp] R39/50 Test Acc: 0.6082 | Avg τ: 0.3173 | Time: 17.71s\n[FedSMOO+Temp] R40/50 Test Acc: 0.6182 | Avg τ: 0.3102 | Time: 17.62s\n[FedSMOO+Temp] R41/50 Test Acc: 0.6120 | Avg τ: 0.3256 | Time: 17.64s\n[FedSMOO+Temp] R42/50 Test Acc: 0.6174 | Avg τ: 0.3286 | Time: 17.66s\n[FedSMOO+Temp] R43/50 Test Acc: 0.6244 | Avg τ: 0.3405 | Time: 17.64s\n[FedSMOO+Temp] R44/50 Test Acc: 0.6209 | Avg τ: 0.3375 | Time: 17.60s\n[FedSMOO+Temp] R45/50 Test Acc: 0.6143 | Avg τ: 0.3434 | Time: 17.60s\n[FedSMOO+Temp] R46/50 Test Acc: 0.6200 | Avg τ: 0.3558 | Time: 17.61s\n[FedSMOO+Temp] R47/50 Test Acc: 0.6228 | Avg τ: 0.3521 | Time: 17.68s\n[FedSMOO+Temp] R48/50 Test Acc: 0.6242 | Avg τ: 0.3505 | Time: 17.58s\n[FedSMOO+Temp] R49/50 Test Acc: 0.6270 | Avg τ: 0.3616 | Time: 17.61s\n[FedSMOO+Temp] R50/50 Test Acc: 0.6245 | Avg τ: 0.3609 | Time: 17.59s\n\n[3/12] FedGMT...\n\n============================================================\nSTARTING FedGMT TRAINING\n============================================================\n[FedGMT] R1/50 Test Acc: 0.1227 | Time: 12.22s\n[FedGMT] R2/50 Test Acc: 0.3979 | Time: 12.18s\n[FedGMT] R3/50 Test Acc: 0.4113 | Time: 12.21s\n[FedGMT] R4/50 Test Acc: 0.4761 | Time: 12.23s\n[FedGMT] R5/50 Test Acc: 0.4818 | Time: 12.14s\n[FedGMT] R6/50 Test Acc: 0.5128 | Time: 12.21s\n[FedGMT] R7/50 Test Acc: 0.5052 | Time: 12.25s\n[FedGMT] R8/50 Test Acc: 0.4950 | Time: 12.23s\n[FedGMT] R9/50 Test Acc: 0.5031 | Time: 12.22s\n[FedGMT] R10/50 Test Acc: 0.5033 | Time: 12.20s\n[FedGMT] R11/50 Test Acc: 0.5177 | Time: 12.21s\n[FedGMT] R12/50 Test Acc: 0.5263 | Time: 12.22s\n[FedGMT] R13/50 Test Acc: 0.5006 | Time: 12.18s\n[FedGMT] R14/50 Test Acc: 0.5183 | Time: 12.23s\n[FedGMT] R15/50 Test Acc: 0.5172 | Time: 12.17s\n[FedGMT] R16/50 Test Acc: 0.5306 | Time: 12.25s\n[FedGMT] R17/50 Test Acc: 0.5188 | Time: 12.21s\n[FedGMT] R18/50 Test Acc: 0.5412 | Time: 12.14s\n[FedGMT] R19/50 Test Acc: 0.5394 | Time: 12.16s\n[FedGMT] R20/50 Test Acc: 0.5388 | Time: 12.21s\n[FedGMT] R21/50 Test Acc: 0.5381 | Time: 12.19s\n[FedGMT] R22/50 Test Acc: 0.5397 | Time: 12.20s\n[FedGMT] R23/50 Test Acc: 0.5483 | Time: 12.16s\n[FedGMT] R24/50 Test Acc: 0.5461 | Time: 12.18s\n[FedGMT] R25/50 Test Acc: 0.5527 | Time: 12.21s\n[FedGMT] R26/50 Test Acc: 0.5458 | Time: 12.27s\n[FedGMT] R27/50 Test Acc: 0.5460 | Time: 12.20s\n[FedGMT] R28/50 Test Acc: 0.5574 | Time: 12.20s\n[FedGMT] R29/50 Test Acc: 0.5558 | Time: 12.23s\n[FedGMT] R30/50 Test Acc: 0.5550 | Time: 12.31s\n[FedGMT] R31/50 Test Acc: 0.5581 | Time: 12.19s\n[FedGMT] R32/50 Test Acc: 0.5615 | Time: 12.25s\n[FedGMT] R33/50 Test Acc: 0.5527 | Time: 12.16s\n[FedGMT] R34/50 Test Acc: 0.5471 | Time: 12.25s\n[FedGMT] R35/50 Test Acc: 0.5483 | Time: 12.25s\n[FedGMT] R36/50 Test Acc: 0.5654 | Time: 12.24s\n[FedGMT] R37/50 Test Acc: 0.5521 | Time: 12.14s\n[FedGMT] R38/50 Test Acc: 0.5580 | Time: 12.14s\n[FedGMT] R39/50 Test Acc: 0.5531 | Time: 12.18s\n[FedGMT] R40/50 Test Acc: 0.5604 | Time: 12.18s\n[FedGMT] R41/50 Test Acc: 0.5662 | Time: 12.19s\n[FedGMT] R42/50 Test Acc: 0.5547 | Time: 12.18s\n[FedGMT] R43/50 Test Acc: 0.5641 | Time: 12.26s\n[FedGMT] R44/50 Test Acc: 0.5517 | Time: 12.27s\n[FedGMT] R45/50 Test Acc: 0.5659 | Time: 12.21s\n[FedGMT] R46/50 Test Acc: 0.5589 | Time: 12.21s\n[FedGMT] R47/50 Test Acc: 0.5525 | Time: 12.15s\n[FedGMT] R48/50 Test Acc: 0.5617 | Time: 12.21s\n[FedGMT] R49/50 Test Acc: 0.5593 | Time: 12.15s\n[FedGMT] R50/50 Test Acc: 0.5610 | Time: 12.23s\n\n[4/12] FedGMT+Temp...\n\n============================================================\nSTARTING FedGMT + TEMP TRAINING\n============================================================\n[FedGMT+Temp] R1/50 Test Acc: 0.1714 | Avg τ: 0.6603 | Time: 12.42s\n[FedGMT+Temp] R2/50 Test Acc: 0.3803 | Avg τ: 0.4612 | Time: 12.50s\n[FedGMT+Temp] R3/50 Test Acc: 0.3878 | Avg τ: 0.3726 | Time: 12.46s\n[FedGMT+Temp] R4/50 Test Acc: 0.4559 | Avg τ: 0.3438 | Time: 12.52s\n[FedGMT+Temp] R5/50 Test Acc: 0.4761 | Avg τ: 0.3026 | Time: 12.56s\n[FedGMT+Temp] R6/50 Test Acc: 0.4906 | Avg τ: 0.2763 | Time: 12.56s\n[FedGMT+Temp] R7/50 Test Acc: 0.5260 | Avg τ: 0.2775 | Time: 12.53s\n[FedGMT+Temp] R8/50 Test Acc: 0.4879 | Avg τ: 0.2760 | Time: 12.51s\n[FedGMT+Temp] R9/50 Test Acc: 0.5290 | Avg τ: 0.2801 | Time: 12.49s\n[FedGMT+Temp] R10/50 Test Acc: 0.5220 | Avg τ: 0.2751 | Time: 12.59s\n[FedGMT+Temp] R11/50 Test Acc: 0.5475 | Avg τ: 0.2709 | Time: 12.55s\n[FedGMT+Temp] R12/50 Test Acc: 0.5082 | Avg τ: 0.2322 | Time: 12.59s\n[FedGMT+Temp] R13/50 Test Acc: 0.5366 | Avg τ: 0.2331 | Time: 12.53s\n[FedGMT+Temp] R14/50 Test Acc: 0.5467 | Avg τ: 0.2206 | Time: 12.69s\n[FedGMT+Temp] R15/50 Test Acc: 0.5386 | Avg τ: 0.2651 | Time: 12.59s\n[FedGMT+Temp] R16/50 Test Acc: 0.5258 | Avg τ: 0.2719 | Time: 12.55s\n[FedGMT+Temp] R17/50 Test Acc: 0.5251 | Avg τ: 0.2003 | Time: 12.34s\n[FedGMT+Temp] R18/50 Test Acc: 0.4979 | Avg τ: 0.2041 | Time: 12.40s\n[FedGMT+Temp] R19/50 Test Acc: 0.4877 | Avg τ: 0.2179 | Time: 12.40s\n[FedGMT+Temp] R20/50 Test Acc: 0.5105 | Avg τ: 0.2108 | Time: 12.40s\n[FedGMT+Temp] R21/50 Test Acc: 0.5257 | Avg τ: 0.2325 | Time: 12.38s\n[FedGMT+Temp] R22/50 Test Acc: 0.5225 | Avg τ: 0.2229 | Time: 12.41s\n[FedGMT+Temp] R23/50 Test Acc: 0.4323 | Avg τ: 0.2531 | Time: 12.24s\n[FedGMT+Temp] R24/50 Test Acc: 0.4850 | Avg τ: 0.2070 | Time: 12.28s\n[FedGMT+Temp] R25/50 Test Acc: 0.5367 | Avg τ: 0.2281 | Time: 12.24s\n[FedGMT+Temp] R26/50 Test Acc: 0.5537 | Avg τ: 0.2064 | Time: 12.27s\n[FedGMT+Temp] R27/50 Test Acc: 0.5478 | Avg τ: 0.2212 | Time: 12.25s\n[FedGMT+Temp] R28/50 Test Acc: 0.4691 | Avg τ: 0.1968 | Time: 12.17s\n[FedGMT+Temp] R29/50 Test Acc: 0.5303 | Avg τ: 0.1721 | Time: 12.21s\n[FedGMT+Temp] R30/50 Test Acc: 0.5497 | Avg τ: 0.2738 | Time: 12.21s\n[FedGMT+Temp] R31/50 Test Acc: 0.5502 | Avg τ: 0.2122 | Time: 12.11s\n[FedGMT+Temp] R32/50 Test Acc: 0.5416 | Avg τ: 0.1567 | Time: 12.11s\n[FedGMT+Temp] R33/50 Test Acc: 0.5465 | Avg τ: 0.1472 | Time: 12.15s\n[FedGMT+Temp] R34/50 Test Acc: 0.5293 | Avg τ: 0.1831 | Time: 12.43s\n[FedGMT+Temp] R35/50 Test Acc: 0.5310 | Avg τ: 0.1523 | Time: 12.48s\n[FedGMT+Temp] R36/50 Test Acc: 0.5456 | Avg τ: 0.1466 | Time: 12.39s\n[FedGMT+Temp] R37/50 Test Acc: 0.5392 | Avg τ: 0.2087 | Time: 12.30s\n[FedGMT+Temp] R38/50 Test Acc: 0.5195 | Avg τ: 0.1570 | Time: 12.29s\n[FedGMT+Temp] R39/50 Test Acc: 0.5565 | Avg τ: 0.1395 | Time: 12.30s\n[FedGMT+Temp] R40/50 Test Acc: 0.5090 | Avg τ: 0.1676 | Time: 12.38s\n[FedGMT+Temp] R41/50 Test Acc: 0.5318 | Avg τ: 0.1208 | Time: 12.29s\n[FedGMT+Temp] R42/50 Test Acc: 0.5556 | Avg τ: 0.1566 | Time: 12.40s\n[FedGMT+Temp] R43/50 Test Acc: 0.5316 | Avg τ: 0.1433 | Time: 12.44s\n[FedGMT+Temp] R44/50 Test Acc: 0.5359 | Avg τ: 0.1204 | Time: 12.43s\n[FedGMT+Temp] R45/50 Test Acc: 0.5548 | Avg τ: 0.1167 | Time: 12.37s\n[FedGMT+Temp] R46/50 Test Acc: 0.5431 | Avg τ: 0.1137 | Time: 12.28s\n[FedGMT+Temp] R47/50 Test Acc: 0.5323 | Avg τ: 0.1135 | Time: 12.45s\n[FedGMT+Temp] R48/50 Test Acc: 0.5469 | Avg τ: 0.1160 | Time: 12.42s\n[FedGMT+Temp] R49/50 Test Acc: 0.5464 | Avg τ: 0.1198 | Time: 12.35s\n[FedGMT+Temp] R50/50 Test Acc: 0.5259 | Avg τ: 0.1156 | Time: 12.38s\n################################################################################\nRUNNING EXPERIMENTS WITH ALPHA:  5\n################################################################################\nData partitioning complete.\nClient 0 class dist: ['0:0.02', '1:0.09', '2:0.09', '3:0.07', '4:0.13', '5:0.12', '6:0.07', '7:0.26', '8:0.01', '9:0.14']\nClient 1 class dist: ['0:0.17', '1:0.06', '2:0.20', '3:0.09', '4:0.04', '5:0.08', '6:0.19', '7:0.06', '8:0.11', '9:0.00']\nClient 2 class dist: ['0:0.03', '1:0.06', '2:0.13', '3:0.10', '4:0.11', '5:0.07', '6:0.14', '7:0.05', '8:0.13', '9:0.18']\nClient 3 class dist: ['0:0.13', '1:0.08', '2:0.06', '3:0.11', '4:0.15', '5:0.04', '6:0.07', '7:0.05', '8:0.13', '9:0.17']\nClient 4 class dist: ['0:0.12', '1:0.14', '2:0.07', '3:0.11', '4:0.12', '5:0.14', '6:0.09', '7:0.15', '8:0.07', '9:0.00']\nClient 5 class dist: ['0:0.10', '1:0.15', '2:0.12', '3:0.10', '4:0.06', '5:0.12', '6:0.03', '7:0.10', '8:0.15', '9:0.07']\nClient 6 class dist: ['0:0.07', '1:0.05', '2:0.06', '3:0.18', '4:0.11', '5:0.07', '6:0.18', '7:0.11', '8:0.18', '9:0.00']\nClient 7 class dist: ['0:0.11', '1:0.21', '2:0.09', '3:0.07', '4:0.08', '5:0.09', '6:0.07', '7:0.13', '8:0.03', '9:0.13']\nClient 8 class dist: ['0:0.05', '1:0.10', '2:0.11', '3:0.10', '4:0.13', '5:0.17', '6:0.06', '7:0.07', '8:0.12', '9:0.10']\nClient 9 class dist: ['0:0.16', '1:0.11', '2:0.10', '3:0.07', '4:0.13', '5:0.07', '6:0.10', '7:0.04', '8:0.06', '9:0.15']\n\n[1/12] FedSMOO...\n\n============================================================\nSTARTING FedSMOO TRAINING\n============================================================\n[FedSMOO] R1/50 Test Acc: 0.3557 | Time: 16.84s\n[FedSMOO] R2/50 Test Acc: 0.4326 | Time: 16.96s\n[FedSMOO] R3/50 Test Acc: 0.4936 | Time: 16.82s\n[FedSMOO] R4/50 Test Acc: 0.5421 | Time: 16.88s\n[FedSMOO] R5/50 Test Acc: 0.5737 | Time: 16.83s\n[FedSMOO] R6/50 Test Acc: 0.5877 | Time: 16.78s\n[FedSMOO] R7/50 Test Acc: 0.5968 | Time: 16.76s\n[FedSMOO] R8/50 Test Acc: 0.6132 | Time: 16.85s\n[FedSMOO] R9/50 Test Acc: 0.6213 | Time: 16.86s\n[FedSMOO] R10/50 Test Acc: 0.6501 | Time: 16.90s\n[FedSMOO] R11/50 Test Acc: 0.6503 | Time: 16.95s\n[FedSMOO] R12/50 Test Acc: 0.6457 | Time: 16.79s\n[FedSMOO] R13/50 Test Acc: 0.6633 | Time: 16.79s\n[FedSMOO] R14/50 Test Acc: 0.6522 | Time: 16.97s\n[FedSMOO] R15/50 Test Acc: 0.6561 | Time: 16.76s\n[FedSMOO] R16/50 Test Acc: 0.6593 | Time: 16.70s\n[FedSMOO] R17/50 Test Acc: 0.6709 | Time: 16.72s\n[FedSMOO] R18/50 Test Acc: 0.6719 | Time: 16.70s\n[FedSMOO] R19/50 Test Acc: 0.6659 | Time: 16.78s\n[FedSMOO] R20/50 Test Acc: 0.6760 | Time: 16.70s\n[FedSMOO] R21/50 Test Acc: 0.6757 | Time: 16.65s\n[FedSMOO] R22/50 Test Acc: 0.6767 | Time: 16.78s\n[FedSMOO] R23/50 Test Acc: 0.6787 | Time: 16.72s\n[FedSMOO] R24/50 Test Acc: 0.6852 | Time: 16.71s\n[FedSMOO] R25/50 Test Acc: 0.6804 | Time: 16.74s\n[FedSMOO] R26/50 Test Acc: 0.6836 | Time: 16.74s\n[FedSMOO] R27/50 Test Acc: 0.6810 | Time: 16.78s\n[FedSMOO] R28/50 Test Acc: 0.6864 | Time: 16.75s\n[FedSMOO] R29/50 Test Acc: 0.6895 | Time: 16.79s\n[FedSMOO] R30/50 Test Acc: 0.6884 | Time: 16.76s\n[FedSMOO] R31/50 Test Acc: 0.6870 | Time: 16.77s\n[FedSMOO] R32/50 Test Acc: 0.6884 | Time: 16.75s\n[FedSMOO] R33/50 Test Acc: 0.6879 | Time: 16.82s\n[FedSMOO] R34/50 Test Acc: 0.6881 | Time: 16.78s\n[FedSMOO] R35/50 Test Acc: 0.6852 | Time: 16.73s\n[FedSMOO] R36/50 Test Acc: 0.6893 | Time: 16.79s\n[FedSMOO] R37/50 Test Acc: 0.6894 | Time: 16.72s\n[FedSMOO] R38/50 Test Acc: 0.6919 | Time: 16.75s\n[FedSMOO] R39/50 Test Acc: 0.6943 | Time: 16.76s\n[FedSMOO] R40/50 Test Acc: 0.6936 | Time: 16.76s\n[FedSMOO] R41/50 Test Acc: 0.6928 | Time: 16.74s\n[FedSMOO] R42/50 Test Acc: 0.6937 | Time: 16.75s\n[FedSMOO] R43/50 Test Acc: 0.6940 | Time: 16.73s\n[FedSMOO] R44/50 Test Acc: 0.6959 | Time: 16.77s\n[FedSMOO] R45/50 Test Acc: 0.6942 | Time: 16.81s\n[FedSMOO] R46/50 Test Acc: 0.6939 | Time: 16.69s\n[FedSMOO] R47/50 Test Acc: 0.6944 | Time: 16.78s\n[FedSMOO] R48/50 Test Acc: 0.6967 | Time: 16.77s\n[FedSMOO] R49/50 Test Acc: 0.6966 | Time: 16.75s\n[FedSMOO] R50/50 Test Acc: 0.6967 | Time: 16.81s\n\n[2/12] FedSMOO+Temp...\n\n============================================================\nSTARTING FedSMOO + TEMP TRAINING\n============================================================\n[FedSMOO+Temp] R1/50 Test Acc: 0.3616 | Avg τ: 0.9652 | Time: 16.94s\n[FedSMOO+Temp] R2/50 Test Acc: 0.4287 | Avg τ: 0.8214 | Time: 17.04s\n[FedSMOO+Temp] R3/50 Test Acc: 0.4736 | Avg τ: 0.7526 | Time: 17.03s\n[FedSMOO+Temp] R4/50 Test Acc: 0.5051 | Avg τ: 0.6934 | Time: 17.11s\n[FedSMOO+Temp] R5/50 Test Acc: 0.5459 | Avg τ: 0.6542 | Time: 17.08s\n[FedSMOO+Temp] R6/50 Test Acc: 0.5731 | Avg τ: 0.6153 | Time: 17.22s\n[FedSMOO+Temp] R7/50 Test Acc: 0.6223 | Avg τ: 0.6075 | Time: 17.27s\n[FedSMOO+Temp] R8/50 Test Acc: 0.6250 | Avg τ: 0.6194 | Time: 17.13s\n[FedSMOO+Temp] R9/50 Test Acc: 0.6345 | Avg τ: 0.6288 | Time: 17.15s\n[FedSMOO+Temp] R10/50 Test Acc: 0.6419 | Avg τ: 0.6462 | Time: 17.04s\n[FedSMOO+Temp] R11/50 Test Acc: 0.6428 | Avg τ: 0.6561 | Time: 17.09s\n[FedSMOO+Temp] R12/50 Test Acc: 0.6525 | Avg τ: 0.6662 | Time: 17.27s\n[FedSMOO+Temp] R13/50 Test Acc: 0.6482 | Avg τ: 0.6751 | Time: 17.17s\n[FedSMOO+Temp] R14/50 Test Acc: 0.6564 | Avg τ: 0.6791 | Time: 17.29s\n[FedSMOO+Temp] R15/50 Test Acc: 0.6631 | Avg τ: 0.6789 | Time: 17.09s\n[FedSMOO+Temp] R16/50 Test Acc: 0.6617 | Avg τ: 0.6912 | Time: 17.15s\n[FedSMOO+Temp] R17/50 Test Acc: 0.6573 | Avg τ: 0.6896 | Time: 17.10s\n[FedSMOO+Temp] R18/50 Test Acc: 0.6678 | Avg τ: 0.6894 | Time: 17.23s\n[FedSMOO+Temp] R19/50 Test Acc: 0.6699 | Avg τ: 0.6949 | Time: 17.18s\n[FedSMOO+Temp] R20/50 Test Acc: 0.6699 | Avg τ: 0.7013 | Time: 17.21s\n[FedSMOO+Temp] R21/50 Test Acc: 0.6680 | Avg τ: 0.7036 | Time: 17.12s\n[FedSMOO+Temp] R22/50 Test Acc: 0.6799 | Avg τ: 0.6991 | Time: 17.21s\n[FedSMOO+Temp] R23/50 Test Acc: 0.6748 | Avg τ: 0.6952 | Time: 17.28s\n[FedSMOO+Temp] R24/50 Test Acc: 0.6820 | Avg τ: 0.6954 | Time: 17.20s\n[FedSMOO+Temp] R25/50 Test Acc: 0.6783 | Avg τ: 0.6947 | Time: 17.19s\n[FedSMOO+Temp] R26/50 Test Acc: 0.6821 | Avg τ: 0.6932 | Time: 17.16s\n[FedSMOO+Temp] R27/50 Test Acc: 0.6845 | Avg τ: 0.6918 | Time: 17.17s\n[FedSMOO+Temp] R28/50 Test Acc: 0.6857 | Avg τ: 0.6870 | Time: 17.20s\n[FedSMOO+Temp] R29/50 Test Acc: 0.6856 | Avg τ: 0.6854 | Time: 17.18s\n[FedSMOO+Temp] R30/50 Test Acc: 0.6849 | Avg τ: 0.6890 | Time: 17.12s\n[FedSMOO+Temp] R31/50 Test Acc: 0.6850 | Avg τ: 0.6905 | Time: 16.97s\n[FedSMOO+Temp] R32/50 Test Acc: 0.6818 | Avg τ: 0.6861 | Time: 17.25s\n[FedSMOO+Temp] R33/50 Test Acc: 0.6833 | Avg τ: 0.6899 | Time: 17.27s\n[FedSMOO+Temp] R34/50 Test Acc: 0.6861 | Avg τ: 0.6910 | Time: 17.23s\n[FedSMOO+Temp] R35/50 Test Acc: 0.6870 | Avg τ: 0.6864 | Time: 17.22s\n[FedSMOO+Temp] R36/50 Test Acc: 0.6882 | Avg τ: 0.6888 | Time: 17.23s\n[FedSMOO+Temp] R37/50 Test Acc: 0.6898 | Avg τ: 0.6931 | Time: 17.26s\n[FedSMOO+Temp] R38/50 Test Acc: 0.6863 | Avg τ: 0.6877 | Time: 17.17s\n[FedSMOO+Temp] R39/50 Test Acc: 0.6868 | Avg τ: 0.6900 | Time: 17.25s\n[FedSMOO+Temp] R40/50 Test Acc: 0.6920 | Avg τ: 0.6840 | Time: 17.28s\n[FedSMOO+Temp] R41/50 Test Acc: 0.6917 | Avg τ: 0.6903 | Time: 17.24s\n[FedSMOO+Temp] R42/50 Test Acc: 0.6902 | Avg τ: 0.6853 | Time: 17.22s\n[FedSMOO+Temp] R43/50 Test Acc: 0.6896 | Avg τ: 0.6851 | Time: 17.01s\n[FedSMOO+Temp] R44/50 Test Acc: 0.6902 | Avg τ: 0.6827 | Time: 17.06s\n[FedSMOO+Temp] R45/50 Test Acc: 0.6904 | Avg τ: 0.6872 | Time: 16.99s\n[FedSMOO+Temp] R46/50 Test Acc: 0.6908 | Avg τ: 0.6876 | Time: 16.99s\n[FedSMOO+Temp] R47/50 Test Acc: 0.6902 | Avg τ: 0.6854 | Time: 17.01s\n[FedSMOO+Temp] R48/50 Test Acc: 0.6903 | Avg τ: 0.6832 | Time: 17.01s\n[FedSMOO+Temp] R49/50 Test Acc: 0.6914 | Avg τ: 0.6895 | Time: 16.99s\n[FedSMOO+Temp] R50/50 Test Acc: 0.6908 | Avg τ: 0.6891 | Time: 16.90s\n\n[3/12] FedGMT...\n\n============================================================\nSTARTING FedGMT TRAINING\n============================================================\n[FedGMT] R1/50 Test Acc: 0.1778 | Time: 11.65s\n[FedGMT] R2/50 Test Acc: 0.4672 | Time: 11.74s\n[FedGMT] R3/50 Test Acc: 0.4843 | Time: 11.66s\n[FedGMT] R4/50 Test Acc: 0.4388 | Time: 11.69s\n[FedGMT] R5/50 Test Acc: 0.5024 | Time: 11.68s\n[FedGMT] R6/50 Test Acc: 0.5652 | Time: 11.69s\n[FedGMT] R7/50 Test Acc: 0.5987 | Time: 11.74s\n[FedGMT] R8/50 Test Acc: 0.5973 | Time: 11.70s\n[FedGMT] R9/50 Test Acc: 0.6153 | Time: 11.70s\n[FedGMT] R10/50 Test Acc: 0.6179 | Time: 11.71s\n[FedGMT] R11/50 Test Acc: 0.6239 | Time: 11.83s\n[FedGMT] R12/50 Test Acc: 0.6247 | Time: 11.75s\n[FedGMT] R13/50 Test Acc: 0.6307 | Time: 11.71s\n[FedGMT] R14/50 Test Acc: 0.6271 | Time: 11.65s\n[FedGMT] R15/50 Test Acc: 0.6142 | Time: 11.67s\n[FedGMT] R16/50 Test Acc: 0.6229 | Time: 11.70s\n[FedGMT] R17/50 Test Acc: 0.6240 | Time: 11.75s\n[FedGMT] R18/50 Test Acc: 0.6348 | Time: 11.72s\n[FedGMT] R19/50 Test Acc: 0.6344 | Time: 11.75s\n[FedGMT] R20/50 Test Acc: 0.6341 | Time: 11.96s\n[FedGMT] R21/50 Test Acc: 0.6374 | Time: 11.82s\n[FedGMT] R22/50 Test Acc: 0.6354 | Time: 11.82s\n[FedGMT] R23/50 Test Acc: 0.6370 | Time: 11.80s\n[FedGMT] R24/50 Test Acc: 0.6440 | Time: 11.77s\n[FedGMT] R25/50 Test Acc: 0.6412 | Time: 11.76s\n[FedGMT] R26/50 Test Acc: 0.6420 | Time: 11.75s\n[FedGMT] R27/50 Test Acc: 0.6411 | Time: 12.03s\n[FedGMT] R28/50 Test Acc: 0.6450 | Time: 12.00s\n[FedGMT] R29/50 Test Acc: 0.6357 | Time: 11.94s\n[FedGMT] R30/50 Test Acc: 0.6415 | Time: 11.95s\n[FedGMT] R31/50 Test Acc: 0.6436 | Time: 11.94s\n[FedGMT] R32/50 Test Acc: 0.6442 | Time: 11.74s\n[FedGMT] R33/50 Test Acc: 0.6457 | Time: 11.73s\n[FedGMT] R34/50 Test Acc: 0.6415 | Time: 11.66s\n[FedGMT] R35/50 Test Acc: 0.6470 | Time: 11.81s\n[FedGMT] R36/50 Test Acc: 0.6456 | Time: 11.68s\n[FedGMT] R37/50 Test Acc: 0.6480 | Time: 11.70s\n[FedGMT] R38/50 Test Acc: 0.6464 | Time: 11.81s\n[FedGMT] R39/50 Test Acc: 0.6479 | Time: 11.72s\n[FedGMT] R40/50 Test Acc: 0.6472 | Time: 11.79s\n[FedGMT] R41/50 Test Acc: 0.6495 | Time: 11.74s\n[FedGMT] R42/50 Test Acc: 0.6473 | Time: 11.76s\n[FedGMT] R43/50 Test Acc: 0.6500 | Time: 11.80s\n[FedGMT] R44/50 Test Acc: 0.6495 | Time: 11.77s\n[FedGMT] R45/50 Test Acc: 0.6517 | Time: 11.75s\n[FedGMT] R46/50 Test Acc: 0.6494 | Time: 11.78s\n[FedGMT] R47/50 Test Acc: 0.6533 | Time: 11.73s\n[FedGMT] R48/50 Test Acc: 0.6518 | Time: 11.77s\n[FedGMT] R49/50 Test Acc: 0.6529 | Time: 11.73s\n[FedGMT] R50/50 Test Acc: 0.6519 | Time: 11.77s\n\n[4/12] FedGMT+Temp...\n\n============================================================\nSTARTING FedGMT + TEMP TRAINING\n============================================================\n[FedGMT+Temp] R1/50 Test Acc: 0.2196 | Avg τ: 0.8088 | Time: 11.97s\n[FedGMT+Temp] R2/50 Test Acc: 0.4077 | Avg τ: 0.4735 | Time: 11.96s\n[FedGMT+Temp] R3/50 Test Acc: 0.4743 | Avg τ: 0.3746 | Time: 11.99s\n[FedGMT+Temp] R4/50 Test Acc: 0.4789 | Avg τ: 0.3240 | Time: 11.97s\n[FedGMT+Temp] R5/50 Test Acc: 0.5292 | Avg τ: 0.2874 | Time: 11.99s\n[FedGMT+Temp] R6/50 Test Acc: 0.5458 | Avg τ: 0.2654 | Time: 11.98s\n[FedGMT+Temp] R7/50 Test Acc: 0.5661 | Avg τ: 0.2361 | Time: 11.96s\n[FedGMT+Temp] R8/50 Test Acc: 0.5456 | Avg τ: 0.2196 | Time: 12.01s\n[FedGMT+Temp] R9/50 Test Acc: 0.6026 | Avg τ: 0.2046 | Time: 11.93s\n[FedGMT+Temp] R10/50 Test Acc: 0.6036 | Avg τ: 0.2098 | Time: 12.02s\n[FedGMT+Temp] R11/50 Test Acc: 0.5947 | Avg τ: 0.1874 | Time: 12.00s\n[FedGMT+Temp] R12/50 Test Acc: 0.6071 | Avg τ: 0.1823 | Time: 11.96s\n[FedGMT+Temp] R13/50 Test Acc: 0.6126 | Avg τ: 0.1799 | Time: 12.08s\n[FedGMT+Temp] R14/50 Test Acc: 0.5652 | Avg τ: 0.1782 | Time: 12.01s\n[FedGMT+Temp] R15/50 Test Acc: 0.5978 | Avg τ: 0.1621 | Time: 12.02s\n[FedGMT+Temp] R16/50 Test Acc: 0.5855 | Avg τ: 0.1572 | Time: 11.93s\n[FedGMT+Temp] R17/50 Test Acc: 0.6127 | Avg τ: 0.1663 | Time: 12.03s\n[FedGMT+Temp] R18/50 Test Acc: 0.6117 | Avg τ: 0.1447 | Time: 12.04s\n[FedGMT+Temp] R19/50 Test Acc: 0.5928 | Avg τ: 0.1534 | Time: 12.03s\n[FedGMT+Temp] R20/50 Test Acc: 0.5992 | Avg τ: 0.1402 | Time: 12.06s\n[FedGMT+Temp] R21/50 Test Acc: 0.6138 | Avg τ: 0.1222 | Time: 12.00s\n[FedGMT+Temp] R22/50 Test Acc: 0.6149 | Avg τ: 0.1129 | Time: 12.00s\n[FedGMT+Temp] R23/50 Test Acc: 0.6177 | Avg τ: 0.1283 | Time: 11.96s\n[FedGMT+Temp] R24/50 Test Acc: 0.6096 | Avg τ: 0.1318 | Time: 12.00s\n[FedGMT+Temp] R25/50 Test Acc: 0.6040 | Avg τ: 0.1299 | Time: 12.03s\n[FedGMT+Temp] R26/50 Test Acc: 0.6066 | Avg τ: 0.1397 | Time: 12.01s\n[FedGMT+Temp] R27/50 Test Acc: 0.5938 | Avg τ: 0.1329 | Time: 11.93s\n[FedGMT+Temp] R28/50 Test Acc: 0.6251 | Avg τ: 0.1106 | Time: 12.01s\n[FedGMT+Temp] R29/50 Test Acc: 0.6255 | Avg τ: 0.0976 | Time: 11.95s\n[FedGMT+Temp] R30/50 Test Acc: 0.6021 | Avg τ: 0.1316 | Time: 12.12s\n[FedGMT+Temp] R31/50 Test Acc: 0.5882 | Avg τ: 0.0984 | Time: 11.88s\n[FedGMT+Temp] R32/50 Test Acc: 0.6105 | Avg τ: 0.0937 | Time: 11.89s\n[FedGMT+Temp] R33/50 Test Acc: 0.6187 | Avg τ: 0.1003 | Time: 12.00s\n[FedGMT+Temp] R34/50 Test Acc: 0.6110 | Avg τ: 0.0913 | Time: 11.93s\n[FedGMT+Temp] R35/50 Test Acc: 0.6262 | Avg τ: 0.0915 | Time: 11.97s\n[FedGMT+Temp] R36/50 Test Acc: 0.6149 | Avg τ: 0.0856 | Time: 11.92s\n[FedGMT+Temp] R37/50 Test Acc: 0.6268 | Avg τ: 0.0858 | Time: 11.93s\n[FedGMT+Temp] R38/50 Test Acc: 0.6092 | Avg τ: 0.1105 | Time: 12.00s\n[FedGMT+Temp] R39/50 Test Acc: 0.6145 | Avg τ: 0.1191 | Time: 11.95s\n[FedGMT+Temp] R40/50 Test Acc: 0.6146 | Avg τ: 0.0948 | Time: 11.95s\n[FedGMT+Temp] R41/50 Test Acc: 0.6099 | Avg τ: 0.0878 | Time: 11.94s\n[FedGMT+Temp] R42/50 Test Acc: 0.6201 | Avg τ: 0.0837 | Time: 11.91s\n[FedGMT+Temp] R43/50 Test Acc: 0.6181 | Avg τ: 0.1075 | Time: 11.98s\n[FedGMT+Temp] R44/50 Test Acc: 0.6187 | Avg τ: 0.0876 | Time: 11.93s\n[FedGMT+Temp] R45/50 Test Acc: 0.6147 | Avg τ: 0.0964 | Time: 11.96s\n[FedGMT+Temp] R46/50 Test Acc: 0.6274 | Avg τ: 0.0954 | Time: 11.90s\n[FedGMT+Temp] R47/50 Test Acc: 0.6257 | Avg τ: 0.1271 | Time: 12.01s\n[FedGMT+Temp] R48/50 Test Acc: 0.6243 | Avg τ: 0.1413 | Time: 11.96s\n[FedGMT+Temp] R49/50 Test Acc: 0.6079 | Avg τ: 0.1066 | Time: 11.87s\n[FedGMT+Temp] R50/50 Test Acc: 0.6078 | Avg τ: 0.0945 | Time: 11.92s\n","output_type":"stream"}],"execution_count":6}]}