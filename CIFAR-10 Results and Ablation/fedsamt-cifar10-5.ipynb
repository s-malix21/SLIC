{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport numpy as np\nfrom torch.utils.data import DataLoader, Subset\nimport copy\nimport random\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport time\n\n# Reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\nNUM_CLASSES = 10\n\n# Hyperparameters\nBATCH_SIZE = 64\nLEARNING_RATE = 0.01\nLOCAL_EPOCHS = 5\nNUM_OF_CLIENTS = 10\nCOMM_ROUND = 50\nALPHA = 5\nFRAC = 0.1\nRHO = 0.05  # SAM perturbation radius (from paper)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"execution":{"iopub.status.busy":"2025-12-27T12:38:58.160316Z","iopub.execute_input":"2025-12-27T12:38:58.161129Z","iopub.status.idle":"2025-12-27T12:39:04.899305Z","shell.execute_reply.started":"2025-12-27T12:38:58.161095Z","shell.execute_reply":"2025-12-27T12:39:04.898612Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"class SimpleCNN(nn.Module):\n    def __init__(self, num_classes=NUM_CLASSES):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n        self.bn3 = nn.BatchNorm2d(128)\n        self.conv4 = nn.Conv2d(128, 128, 3, padding=1)\n        self.bn4 = nn.BatchNorm2d(128)\n        self.conv5 = nn.Conv2d(128, 256, 3, padding=1)\n        self.bn5 = nn.BatchNorm2d(256)\n        self.conv6 = nn.Conv2d(256, 256, 3, padding=1)\n        self.bn6 = nn.BatchNorm2d(256)\n        self.conv7 = nn.Conv2d(256, 512, 3, padding=1)\n        self.bn7 = nn.BatchNorm2d(512)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((2, 2))\n        self.fc1 = nn.Linear(512 * 2 * 2, 1024)\n        self.bn_fc1 = nn.BatchNorm1d(1024)\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(1024, 512)\n        self.bn_fc2 = nn.BatchNorm1d(512)\n        self.dropout2 = nn.Dropout(0.3)\n        self.fc3 = nn.Linear(512, num_classes)\n\n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = self.pool(x)\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = F.relu(self.bn4(self.conv4(x)))\n        x = self.pool(x)\n        x = F.relu(self.bn5(self.conv5(x)))\n        x = F.relu(self.bn6(self.conv6(x)))\n        x = self.pool(x)\n        x = F.relu(self.bn7(self.conv7(x)))\n        x = self.adaptive_pool(x)\n        x = x.view(x.size(0), -1)\n        f1 = F.relu(self.bn_fc1(self.fc1(x)))\n        f1 = self.dropout1(f1)\n        f2 = F.relu(self.bn_fc2(self.fc2(f1)))\n        f2 = self.dropout2(f2)\n        logits = self.fc3(f2)\n        return f2, logits  # features, logits\n\n\ndef load_and_partition_data(num_clients=NUM_OF_CLIENTS, alpha=ALPHA, batch_size=BATCH_SIZE, frac=FRAC, rand_seed=42):\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n\n    torch.manual_seed(rand_seed)\n    np.random.seed(rand_seed)\n\n    full_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n    test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n\n    y_train = np.array(full_dataset.targets)\n    y_test = np.array(test_dataset.targets)\n\n    net_dataidx_map = {}\n    net_dataidx_map_test = {}\n\n    min_size = 0\n    while min_size < 10:\n        idx_batch = [[] for _ in range(num_clients)]\n        idx_batch_test = [[] for _ in range(num_clients)]\n        for k in range(10):\n            idx_k = np.where(y_train == k)[0]\n            idx_k_test = np.where(y_test == k)[0]\n            np.random.shuffle(idx_k)\n            np.random.shuffle(idx_k_test)\n            proportions = np.random.dirichlet(np.repeat(alpha, num_clients))\n            proportions_train = np.array([p * (len(idx_j) < len(full_dataset)/num_clients) for p, idx_j in zip(proportions, idx_batch)])\n            proportions_test = np.array([p * (len(idx_j) < len(test_dataset)/num_clients) for p, idx_j in zip(proportions, idx_batch_test)])\n            proportions_train /= proportions_train.sum()\n            proportions_test /= proportions_test.sum()\n            split_train = (np.cumsum(proportions_train) * len(idx_k)).astype(int)[:-1]\n            split_test = (np.cumsum(proportions_test) * len(idx_k_test)).astype(int)[:-1]\n            idx_batch = [idx_j + idx.tolist() for idx_j, idx in zip(idx_batch, np.split(idx_k, split_train))]\n            idx_batch_test = [idx_j + idx.tolist() for idx_j, idx in zip(idx_batch_test, np.split(idx_k_test, split_test))]\n        min_size = min(len(l) for l in idx_batch)\n\n    for j in range(num_clients):\n        np.random.shuffle(idx_batch[j])\n        np.random.shuffle(idx_batch_test[j])\n        net_dataidx_map[j] = idx_batch[j]\n        net_dataidx_map_test[j] = idx_batch_test[j]\n\n    client_train_loaders = []\n    client_val_loaders = []\n    client_class_dist = []\n\n    for i in range(num_clients):\n        np.random.seed(rand_seed + i)\n        train_idx = np.random.choice(net_dataidx_map[i], int(frac * len(net_dataidx_map[i])), replace=False)\n        val_idx = np.random.choice(net_dataidx_map_test[i], int(min(2*frac,1.0)*len(net_dataidx_map_test[i])), replace=False)\n\n        client_labels = [y_train[k] for k in train_idx]\n        dist = {c: client_labels.count(c)/len(client_labels) if client_labels else 0 for c in range(10)}\n        client_class_dist.append(dist)\n\n        train_loader = DataLoader(Subset(full_dataset, train_idx), batch_size=batch_size,\n                                  shuffle=True, generator=torch.Generator().manual_seed(rand_seed+i), drop_last=True)\n        val_loader = DataLoader(Subset(test_dataset, val_idx), batch_size=batch_size,\n                                shuffle=True, generator=torch.Generator().manual_seed(rand_seed+i+num_clients), drop_last=True)\n        client_train_loaders.append(train_loader)\n        client_val_loaders.append(val_loader)\n\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n                             generator=torch.Generator().manual_seed(rand_seed + 2*num_clients + 1))\n\n    print(\"Data partitioning complete.\")\n    for i, d in enumerate(client_class_dist):\n        print(f\"Client {i} class dist: {[f'{c}:{d.get(c,0):.2f}' for c in range(10)]}\")\n\n    # return client_train_loaders, client_val_loaders, test_loader\n    return client_train_loaders, client_val_loaders, test_loader, client_class_dist\n\n# train_loaders, val_loaders, test_loader = load_and_partition_data()\n\ntrain_loaders, val_loaders, test_loader, client_class_dist = load_and_partition_data()\n\n# ========================================\n# 3. TEMPNET\n# ========================================\nclass TempNet(nn.Module):\n    def __init__(self, feature_dim=512, hidden_dim=128, tau_min=0.05, tau_max=2.0):\n        super().__init__()\n        self.fc1 = nn.Linear(feature_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, 1)\n        self.tau_min = tau_min\n        self.tau_max = tau_max\n\n    def forward(self, x):\n        h = F.relu(self.fc1(x))\n        raw = self.fc2(h)\n        tau = torch.sigmoid(raw)\n        tau = tau * (self.tau_max - self.tau_min) + self.tau_min\n        return tau.mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T12:39:06.568045Z","iopub.execute_input":"2025-12-27T12:39:06.568797Z","iopub.status.idle":"2025-12-27T12:39:26.367318Z","shell.execute_reply.started":"2025-12-27T12:39:06.568764Z","shell.execute_reply":"2025-12-27T12:39:26.366627Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 170M/170M [00:15<00:00, 11.2MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Data partitioning complete.\nClient 0 class dist: ['0:0.02', '1:0.09', '2:0.09', '3:0.07', '4:0.13', '5:0.12', '6:0.07', '7:0.26', '8:0.01', '9:0.14']\nClient 1 class dist: ['0:0.17', '1:0.06', '2:0.20', '3:0.09', '4:0.04', '5:0.08', '6:0.19', '7:0.06', '8:0.11', '9:0.00']\nClient 2 class dist: ['0:0.03', '1:0.06', '2:0.13', '3:0.10', '4:0.11', '5:0.07', '6:0.14', '7:0.05', '8:0.13', '9:0.18']\nClient 3 class dist: ['0:0.13', '1:0.08', '2:0.06', '3:0.11', '4:0.15', '5:0.04', '6:0.07', '7:0.05', '8:0.13', '9:0.17']\nClient 4 class dist: ['0:0.12', '1:0.14', '2:0.07', '3:0.11', '4:0.12', '5:0.14', '6:0.09', '7:0.15', '8:0.07', '9:0.00']\nClient 5 class dist: ['0:0.10', '1:0.15', '2:0.12', '3:0.10', '4:0.06', '5:0.12', '6:0.03', '7:0.10', '8:0.15', '9:0.07']\nClient 6 class dist: ['0:0.07', '1:0.05', '2:0.06', '3:0.18', '4:0.11', '5:0.07', '6:0.18', '7:0.11', '8:0.18', '9:0.00']\nClient 7 class dist: ['0:0.11', '1:0.21', '2:0.09', '3:0.07', '4:0.08', '5:0.09', '6:0.07', '7:0.13', '8:0.03', '9:0.13']\nClient 8 class dist: ['0:0.05', '1:0.10', '2:0.11', '3:0.10', '4:0.13', '5:0.17', '6:0.06', '7:0.07', '8:0.12', '9:0.10']\nClient 9 class dist: ['0:0.16', '1:0.11', '2:0.10', '3:0.07', '4:0.13', '5:0.07', '6:0.10', '7:0.04', '8:0.06', '9:0.15']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"class BaseClient:\n    \"\"\"Base class for federated learning clients\"\"\"\n    \n    def __init__(self, client_id, model, train_loader, val_loader, device, \n                 learning_rate=0.01, local_epochs=5, momentum=0.9):\n        self.client_id = client_id\n        self.model = model\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.device = device\n        self.learning_rate = learning_rate\n        self.local_epochs = local_epochs\n        self.momentum = momentum\n        self.loss_fn = nn.CrossEntropyLoss()\n        self.train_samples = 0\n    \n    def train(self):\n        raise NotImplementedError\n    \n    def set_parameters(self, model_state_dict):\n        \"\"\"Load model parameters from server\"\"\"\n        self.model.load_state_dict(model_state_dict)\n    \n    def get_parameters(self):\n        \"\"\"Get model parameters\"\"\"\n        return self.model.state_dict()\n    \n    def get_train_samples(self):\n        \"\"\"Get number of training samples\"\"\"\n        try:\n            return len(self.train_loader.dataset)\n        except:\n            return len(self.train_loader) * BATCH_SIZE\n\n\nclass BaseServer:\n    \"\"\"Base class for federated learning server\"\"\"\n    \n    def __init__(self, global_model, test_loader, num_clients, learning_rate=0.01, \n                 lr_decay=0.998, device='cuda'):\n        self.global_model = copy.deepcopy(global_model)\n        self.test_loader = test_loader\n        self.num_clients = num_clients\n        self.learning_rate = learning_rate\n        self.lr_decay = lr_decay\n        self.device = device\n        self.clients = []\n        self.loss_fn = nn.CrossEntropyLoss()\n    \n    def send_models(self, clients):\n        \"\"\"Send global model to clients\"\"\"\n        model_state = self.global_model.state_dict()\n        for client in clients:\n            client.set_parameters(model_state)\n    \n    def receive_models(self, clients):\n        \"\"\"Receive trained models from clients and aggregate\"\"\"\n        self.client_models = [client.get_parameters() for client in clients]\n        self.client_samples = [client.get_train_samples() for client in clients]\n    \n    def aggregate_parameters(self):\n        \"\"\"Aggregate client models using weighted averaging\n        \n        IMPORTANT: This method handles mixed dtypes in model state dict.\n        Batch normalization layers have both float parameters AND integer buffers \n        (like num_batches_tracked). We must convert to float for weighted averaging,\n        then convert back to original dtype to avoid RuntimeError.\n        \"\"\"\n        total_samples = sum(self.client_samples)\n        avg_state = {}\n        \n        # Get first model as reference\n        for key in self.client_models[0].keys():\n            # Get original dtype (may be float32, float64, or int64)\n            first_tensor = self.client_models[0][key]\n            original_dtype = first_tensor.dtype\n            \n            # Initialize accumulator with zeros, using float32 for safe averaging\n            avg_state[key] = torch.zeros_like(first_tensor, dtype=torch.float32)\n            \n            # Weighted average: convert each tensor to float before summing\n            for model_state, num_samples in zip(self.client_models, self.client_samples):\n                weight = num_samples / total_samples\n                # .float() converts int64 buffers to float32, allows weighted addition\n                avg_state[key] += weight * model_state[key].float()\n            \n            # Convert result back to original dtype\n            # Float params stay float, int buffers become int again\n            avg_state[key] = avg_state[key].to(original_dtype)\n        \n        self.global_model.load_state_dict(avg_state)\n    \n    def evaluate(self):\n        \"\"\"Evaluate global model on test data\"\"\"\n        self.global_model.eval()\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for X, y in self.test_loader:\n                X, y = X.to(self.device), y.to(self.device)\n                output = self.model_forward(X)\n                if isinstance(output, tuple):\n                    _, logits = output\n                else:\n                    logits = output\n                _, predicted = torch.max(logits, 1)\n                total += y.size(0)\n                correct += (predicted == y).sum().item()\n        \n        accuracy = correct / total\n        return accuracy\n    \n    def model_forward(self, X):\n        \"\"\"Forward pass through model\"\"\"\n        return self.global_model(X)\n\n\n# ========================================\n# FEDAVG IMPLEMENTATION\n# ========================================\n\nclass ClientFedAvg(BaseClient):\n    \"\"\"Standard FedAvg client using SGD\"\"\"\n    \n    def train(self):\n        self.model.train()\n        optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate)\n        \n        for epoch in range(self.local_epochs):\n            for X, y in self.train_loader:\n                X, y = X.to(self.device), y.to(self.device)\n                \n                optimizer.zero_grad()\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    _, logits = output\n                else:\n                    logits = output\n                loss = self.loss_fn(logits, y)\n                loss.backward()\n                optimizer.step()\n\n\nclass FedAvgServer(BaseServer):\n    \"\"\"Standard FedAvg server\"\"\"\n    \n    def __init__(self, global_model, test_loader, num_clients, client_loaders, \n                 learning_rate=0.01, lr_decay=0.998, device='cuda', local_epochs=5):\n        super().__init__(global_model, test_loader, num_clients, learning_rate, \n                        lr_decay, device)\n        self.local_epochs = local_epochs\n        \n        for i, (train_loader, val_loader) in enumerate(client_loaders):\n            client = ClientFedAvg(i, copy.deepcopy(global_model), train_loader, \n                                 val_loader, device, learning_rate=learning_rate, \n                                 local_epochs=local_epochs)\n            client.train_samples = client.get_train_samples()\n            self.clients.append(client)\n    \n    def train_round(self):\n        \"\"\"Execute one communication round\"\"\"\n        self.send_models(self.clients)\n        \n        for client in self.clients:\n            client.train()\n        \n        self.receive_models(self.clients)\n        self.aggregate_parameters()\n        \n        acc = self.evaluate()\n        self.learning_rate *= self.lr_decay\n        \n        return acc\n\n\n# ========================================\n# FEDAVG + TEMPNET IMPLEMENTATION\n# ========================================\n\nclass ClientFedAvgTemp(BaseClient):\n    \"\"\"FedAvg + Temperature: Standard SGD with learned temperature scaling\"\"\"\n    \n    def __init__(self, client_id, model, train_loader, val_loader, device, \n                 learning_rate=0.01, local_epochs=5, momentum=0.9, train_samples=None):\n        super().__init__(client_id, model, train_loader, val_loader, device, \n                        learning_rate, local_epochs, momentum)\n        self.train_samples = train_samples or 0\n        self.tempnet = TempNet(feature_dim=512, hidden_dim=128).to(device)\n        self.temp_optimizer = optim.SGD(self.tempnet.parameters(), \n                                       lr=learning_rate)\n    \n    def train(self):\n        self.model.train()\n        self.tempnet.train()\n        optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate)\n        \n        for epoch in range(self.local_epochs):\n            for X, y in self.train_loader:\n                X, y = X.to(self.device), y.to(self.device)\n                \n                optimizer.zero_grad()\n                self.temp_optimizer.zero_grad()\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    features, logits = output\n                else:\n                    features = None\n                    logits = output\n                \n                tau = self.tempnet(features.detach()) if features is not None else torch.tensor(1.0, device=self.device)\n                scaled_logits = logits / tau\n                loss = self.loss_fn(scaled_logits, y)\n                loss.backward()\n                \n                optimizer.step()\n                self.temp_optimizer.step()\n    \n    def get_temperature(self):\n        \"\"\"Get current temperature value\"\"\"\n        self.model.eval()\n        self.tempnet.eval()\n        try:\n            with torch.no_grad():\n                X, _ = next(iter(self.train_loader))\n                X = X.to(self.device)\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    features, _ = output\n                    tau = self.tempnet(features)\n                    return tau.item()\n                else:\n                    return 1.0\n        except:\n            return 1.0\n\n\nclass FedAvgTempServer(BaseServer):\n    \"\"\"FedAvg + Temperature Server\"\"\"\n    \n    def __init__(self, global_model, test_loader, num_clients, client_loaders, \n                 learning_rate=0.01, lr_decay=0.998, device='cuda', local_epochs=5):\n        super().__init__(global_model, test_loader, num_clients, learning_rate, \n                        lr_decay, device)\n        self.local_epochs = local_epochs\n        self.tau_history = []\n        \n        for i, (train_loader, val_loader) in enumerate(client_loaders):\n            client = ClientFedAvgTemp(i, copy.deepcopy(global_model), train_loader, \n                                     val_loader, device, learning_rate=learning_rate, \n                                     local_epochs=local_epochs)\n            client.train_samples = client.get_train_samples()\n            self.clients.append(client)\n    \n    def train_round(self):\n        self.send_models(self.clients)\n        \n        local_taus = []\n        for client in self.clients:\n            client.train()\n            tau = client.get_temperature()\n            local_taus.append(tau)\n        \n        self.receive_models(self.clients)\n        self.aggregate_parameters()\n        \n        acc = self.evaluate()\n        avg_tau = sum(local_taus) / len(local_taus) if local_taus else 1.0\n        self.tau_history.append(avg_tau)\n        \n        self.learning_rate *= self.lr_decay\n        \n        return acc, avg_tau\n\n# ========================================\n# SAM OPTIMIZER (Sharpness Aware Minimization)\n# ========================================\n\nclass SAMOptimizer(optim.Optimizer):\n    \"\"\"SAM optimizer - Sharpness Aware Minimization\"\"\"\n    \n    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n        assert rho >= 0.0, f\"Invalid rho: {rho}\"\n        defaults = dict(rho=rho, adaptive=adaptive)\n        super().__init__(params, defaults)\n        \n        self.base_optimizer = base_optimizer\n        self.param_groups = self.base_optimizer.param_groups\n        for group in self.param_groups:\n            group[\"rho\"] = rho\n            group[\"adaptive\"] = adaptive\n    \n    @torch.no_grad()\n    def first_step(self):\n        \"\"\"Perturbation step: climb to local maximum\"\"\"\n        grad_norm = self._grad_norm()\n        \n        for group in self.param_groups:\n            scale = group[\"rho\"] / (grad_norm + 1e-7)\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n                p.add_(e_w)  # Move to perturbed point θ + ε\n                self.state[p][\"e_w\"] = e_w\n    \n    @torch.no_grad()\n    def second_step(self):\n        \"\"\"Restore to original point (do this BEFORE applying gradients)\"\"\"\n        for group in self.param_groups:\n            for p in group[\"params\"]:\n                if \"e_w\" in self.state[p]:\n                    p.sub_(self.state[p][\"e_w\"])  # Return to original θ\n                    self.state[p][\"e_w\"] = 0\n    \n    def _grad_norm(self):\n        \"\"\"Compute gradient norm across all parameters\"\"\"\n        norm = torch.norm(torch.stack([\n            ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(p=2)\n            for group in self.param_groups for p in group[\"params\"]\n            if p.grad is not None\n        ]), p=2)\n        return norm\n\n\n# ========================================\n# FEDSAM IMPLEMENTATION\n# ========================================\n\nclass ClientFedSAM(BaseClient):\n    \"\"\"FedSAM client using SAM optimizer\"\"\"\n    \n    def __init__(self, client_id, model, train_loader, val_loader, device, \n                 learning_rate=0.01, local_epochs=5, momentum=0.9, rho=0.05, \n                 train_samples=None):\n        super().__init__(client_id, model, train_loader, val_loader, device, \n                        learning_rate, local_epochs, momentum)\n        self.rho = rho\n        self.train_samples = train_samples or 0\n    \n    def train(self):\n        self.model.train()\n        base_optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate)\n        sam_optimizer = SAMOptimizer(self.model.parameters(), base_optimizer, rho=self.rho)\n        \n        for epoch in range(self.local_epochs):\n            for X, y in self.train_loader:\n                X, y = X.to(self.device), y.to(self.device)\n                \n                # First forward-backward: compute gradient at θ\n                base_optimizer.zero_grad()\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    _, logits = output\n                else:\n                    logits = output\n                loss = self.loss_fn(logits, y)\n                loss.backward()\n                \n                # SAM first step: ascent to θ + ε\n                sam_optimizer.first_step()\n                \n                # Second forward-backward: compute gradient at perturbed point θ + ε\n                base_optimizer.zero_grad()\n                output_pert = self.model(X)\n                if isinstance(output_pert, tuple):\n                    _, logits_pert = output_pert\n                else:\n                    logits_pert = output_pert\n                loss_pert = self.loss_fn(logits_pert, y)\n                loss_pert.backward()\n                \n                # SAM second step: restore to original θ (BEFORE gradient descent)\n                sam_optimizer.second_step()\n                \n                # Apply gradient from perturbed point to original θ\n                base_optimizer.step()\n\n\nclass FedSAMServer(BaseServer):\n    \"\"\"FedSAM server\"\"\"\n    \n    def __init__(self, global_model, test_loader, num_clients, client_loaders, \n                 learning_rate=0.01, lr_decay=0.998, device='cuda', local_epochs=5, \n                 rho=0.05):\n        super().__init__(global_model, test_loader, num_clients, learning_rate, \n                        lr_decay, device)\n        self.local_epochs = local_epochs\n        self.rho = rho\n        \n        for i, (train_loader, val_loader) in enumerate(client_loaders):\n            client = ClientFedSAM(i, copy.deepcopy(global_model), train_loader, \n                                 val_loader, device, learning_rate=learning_rate, \n                                 local_epochs=local_epochs, rho=rho)\n            client.train_samples = client.get_train_samples()\n            self.clients.append(client)\n    \n    def train_round(self):\n        self.send_models(self.clients)\n        \n        for client in self.clients:\n            client.train()\n        \n        self.receive_models(self.clients)\n        self.aggregate_parameters()\n        \n        acc = self.evaluate()\n        self.learning_rate *= self.lr_decay\n        \n        return acc\n\n\n# ========================================\n# FEDSAM + TEMPNET IMPLEMENTATION\n# ========================================\n\nclass ClientFedSAMTemp(BaseClient):\n    \"\"\"FedSAM + Temperature: SAM with learned temperature scaling\"\"\"\n    \n    def __init__(self, client_id, model, train_loader, val_loader, device, \n                 learning_rate=0.01, local_epochs=5, momentum=0.9, rho=0.05, \n                 train_samples=None):\n        super().__init__(client_id, model, train_loader, val_loader, device, \n                        learning_rate, local_epochs, momentum)\n        self.rho = rho\n        self.train_samples = train_samples or 0\n        self.tempnet = TempNet(feature_dim=512, hidden_dim=128).to(device)\n        self.temp_optimizer = optim.SGD(self.tempnet.parameters(), \n                                       lr=learning_rate)\n    \n    def train(self):\n        self.model.train()\n        self.tempnet.train()\n        base_optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate)\n        sam_optimizer = SAMOptimizer(self.model.parameters(), base_optimizer, rho=self.rho)\n        \n        for epoch in range(self.local_epochs):\n            for X, y in self.train_loader:\n                X, y = X.to(self.device), y.to(self.device)\n                \n                # First forward-backward: compute gradient at θ with temperature\n                base_optimizer.zero_grad()\n                self.temp_optimizer.zero_grad()\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    features, logits = output\n                else:\n                    features = None\n                    logits = output\n                \n                tau = self.tempnet(features.detach()) if features is not None else torch.tensor(1.0, device=self.device)\n                scaled_logits = logits / tau\n                loss = self.loss_fn(scaled_logits, y)\n                loss.backward()\n                \n                # SAM first step: ascent to θ + ε (model only)\n                sam_optimizer.first_step()\n                \n                # Second forward-backward: at perturbed point θ + ε\n                base_optimizer.zero_grad()\n                self.temp_optimizer.zero_grad()\n                output_pert = self.model(X)\n                if isinstance(output_pert, tuple):\n                    features_pert, logits_pert = output_pert\n                else:\n                    features_pert = None\n                    logits_pert = output_pert\n                \n                tau_pert = self.tempnet(features_pert.detach()) if features_pert is not None else torch.tensor(1.0, device=self.device)\n                scaled_logits_pert = logits_pert / tau_pert\n                loss_pert = self.loss_fn(scaled_logits_pert, y)\n                loss_pert.backward()\n                \n                # SAM second step: restore model to original θ (BEFORE gradient descent)\n                sam_optimizer.second_step()\n                \n                # Apply gradients: model gets gradient from perturbed point, tempnet standard update\n                base_optimizer.step()\n                self.temp_optimizer.step()\n    \n    def get_temperature(self):\n        \"\"\"Get current temperature value\"\"\"\n        self.model.eval()\n        self.tempnet.eval()\n        try:\n            with torch.no_grad():\n                X, _ = next(iter(self.train_loader))\n                X = X.to(self.device)\n                output = self.model(X)\n                if isinstance(output, tuple):\n                    features, _ = output\n                    tau = self.tempnet(features)\n                    return tau.item()\n                else:\n                    return 1.0\n        except:\n            return 1.0\n\n\nclass FedSAMTempServer(BaseServer):\n    \"\"\"FedSAM + Temperature Server\"\"\"\n    \n    def __init__(self, global_model, test_loader, num_clients, client_loaders, \n                 learning_rate=0.01, lr_decay=0.998, device='cuda', local_epochs=5, \n                 rho=0.05):\n        super().__init__(global_model, test_loader, num_clients, learning_rate, \n                        lr_decay, device)\n        self.local_epochs = local_epochs\n        self.rho = rho\n        self.tau_history = []\n        \n        for i, (train_loader, val_loader) in enumerate(client_loaders):\n            client = ClientFedSAMTemp(i, copy.deepcopy(global_model), train_loader, \n                                     val_loader, device, learning_rate=learning_rate, \n                                     local_epochs=local_epochs, rho=rho)\n            client.train_samples = client.get_train_samples()\n            self.clients.append(client)\n    \n    def train_round(self):\n        self.send_models(self.clients)\n        \n        local_taus = []\n        for client in self.clients:\n            client.train()\n            tau = client.get_temperature()\n            local_taus.append(tau)\n        \n        self.receive_models(self.clients)\n        self.aggregate_parameters()\n        \n        acc = self.evaluate()\n        avg_tau = sum(local_taus) / len(local_taus) if local_taus else 1.0\n        self.tau_history.append(avg_tau)\n        \n        self.learning_rate *= self.lr_decay\n        \n        return acc, avg_tau\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T12:39:26.368599Z","iopub.execute_input":"2025-12-27T12:39:26.368835Z","iopub.status.idle":"2025-12-27T12:39:26.414747Z","shell.execute_reply.started":"2025-12-27T12:39:26.368813Z","shell.execute_reply":"2025-12-27T12:39:26.413827Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ========================================\n# TRAINING FUNCTIONS FOR ALL ALGORITHMS\n# ========================================\ndef run_fedavg():\n    \"\"\"Run FedAvg training\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STARTING FedAvg TRAINING\")\n    print(\"=\"*60)\n    \n    client_loaders = [(train_loaders[i], val_loaders[i]) for i in range(NUM_OF_CLIENTS)]\n    \n    server = FedAvgServer(\n        global_model=SimpleCNN().to(device),\n        test_loader=test_loader,\n        num_clients=NUM_OF_CLIENTS,\n        client_loaders=client_loaders,\n        learning_rate=LEARNING_RATE,\n        lr_decay=1,\n        device=device,\n        local_epochs=LOCAL_EPOCHS\n    )\n    \n    test_accs = []\n    for rnd in range(COMM_ROUND):\n        round_start = time.time()\n        acc = server.train_round()\n        test_accs.append(acc)\n        \n        print(f\"[FedAvg] R{rnd+1}/{COMM_ROUND} Test Acc: {acc:.4f} | Time: {time.time()-round_start:.2f}s\")\n    \n    return test_accs, server.global_model\n\n\ndef run_fedavg_temp():\n    \"\"\"Run FedAvg + Temperature training\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STARTING FedAvg + TEMP TRAINING\")\n    print(\"=\"*60)\n    \n    client_loaders = [(train_loaders[i], val_loaders[i]) for i in range(NUM_OF_CLIENTS)]\n    \n    server = FedAvgTempServer(\n        global_model=SimpleCNN().to(device),\n        test_loader=test_loader,\n        num_clients=NUM_OF_CLIENTS,\n        client_loaders=client_loaders,\n        learning_rate=LEARNING_RATE,\n        lr_decay=1,\n        device=device,\n        local_epochs=LOCAL_EPOCHS\n    )\n    \n    test_accs = []\n    tau_history = []\n    for rnd in range(COMM_ROUND):\n        round_start = time.time()\n        acc, avg_tau = server.train_round()\n        test_accs.append(acc)\n        tau_history.append(avg_tau)\n        \n        print(f\"[FedAvg+Temp] R{rnd+1}/{COMM_ROUND} Test Acc: {acc:.4f} | Avg τ: {avg_tau:.4f} | Time: {time.time()-round_start:.2f}s\")\n    \n    return test_accs, tau_history, server.global_model\n\n\ndef run_fedsam():\n    \"\"\"Run FedSAM training\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STARTING FedSAM TRAINING\")\n    print(\"=\"*60)\n    \n    client_loaders = [(train_loaders[i], val_loaders[i]) for i in range(NUM_OF_CLIENTS)]\n    \n    server = FedSAMServer(\n        global_model=SimpleCNN().to(device),\n        test_loader=test_loader,\n        num_clients=NUM_OF_CLIENTS,\n        client_loaders=client_loaders,\n        learning_rate=LEARNING_RATE,\n        lr_decay=1,\n        device=device,\n        local_epochs=LOCAL_EPOCHS,\n        rho=RHO\n    )\n    \n    test_accs = []\n    for rnd in range(COMM_ROUND):\n        round_start = time.time()\n        acc = server.train_round()\n        test_accs.append(acc)\n        \n        print(f\"[FedSAM] R{rnd+1}/{COMM_ROUND} Test Acc: {acc:.4f} | Time: {time.time()-round_start:.2f}s\")\n    \n    return test_accs, server.global_model\n\n\ndef run_fedsam_temp():\n    \"\"\"Run FedSAM + Temperature training\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STARTING FedSAM + TEMP TRAINING\")\n    print(\"=\"*60)\n    \n    client_loaders = [(train_loaders[i], val_loaders[i]) for i in range(NUM_OF_CLIENTS)]\n    \n    server = FedSAMTempServer(\n        global_model=SimpleCNN().to(device),\n        test_loader=test_loader,\n        num_clients=NUM_OF_CLIENTS,\n        client_loaders=client_loaders,\n        learning_rate=LEARNING_RATE,\n        lr_decay=1,\n        device=device,\n        local_epochs=LOCAL_EPOCHS,\n        rho=RHO\n    )\n    \n    test_accs = []\n    tau_history = []\n    for rnd in range(COMM_ROUND):\n        round_start = time.time()\n        acc, avg_tau = server.train_round()\n        test_accs.append(acc)\n        tau_history.append(avg_tau)\n        \n        print(f\"[FedSAM+Temp] R{rnd+1}/{COMM_ROUND} Test Acc: {acc:.4f} | Avg τ: {avg_tau:.4f} | Time: {time.time()-round_start:.2f}s\")\n    \n    return test_accs, tau_history, server.global_model\n\n\n# ========================================\n# RUN ALL EXPERIMENTS\n# ========================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"FEDERATED LEARNING EXPERIMENTS - COMPLETE FL-SAM SUITE\")\nprint(\"=\"*80)\nprint(f\"Configuration:\")\nprint(f\"  - Num Clients: {NUM_OF_CLIENTS}\")\nprint(f\"  - Comm Rounds: {COMM_ROUND}\")\nprint(f\"  - Local Epochs: {LOCAL_EPOCHS}\")\nprint(f\"  - Batch Size: {BATCH_SIZE}\")\nprint(f\"  - Learning Rate: {LEARNING_RATE}\")\nprint(f\"  - RHO (SAM): {RHO}\")\nprint(f\"  - Alpha (Dirichlet): {ALPHA}\")\nprint(\"=\"*80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T12:39:46.389092Z","iopub.execute_input":"2025-12-27T12:39:46.389936Z","iopub.status.idle":"2025-12-27T12:39:46.403504Z","shell.execute_reply.started":"2025-12-27T12:39:46.389905Z","shell.execute_reply":"2025-12-27T12:39:46.402657Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nFEDERATED LEARNING EXPERIMENTS - COMPLETE FL-SAM SUITE\n================================================================================\nConfiguration:\n  - Num Clients: 10\n  - Comm Rounds: 50\n  - Local Epochs: 5\n  - Batch Size: 64\n  - Learning Rate: 0.01\n  - RHO (SAM): 0.05\n  - Alpha (Dirichlet): 5\n================================================================================\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"print(\"\\n[7/12] FedSAM...\")\ntest_accs_fedsam, model_fedsam = run_fedsam()\n\nprint(\"\\n[8/12] FedSAM+Temp...\")\ntest_accs_fedsam_temp, tau_fedsam_temp, model_fedsam_temp = run_fedsam_temp()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T12:39:54.456047Z","iopub.execute_input":"2025-12-27T12:39:54.456338Z","iopub.status.idle":"2025-12-27T13:05:11.058610Z","shell.execute_reply.started":"2025-12-27T12:39:54.456312Z","shell.execute_reply":"2025-12-27T13:05:11.057839Z"}},"outputs":[{"name":"stdout","text":"\n[7/12] FedSAM...\n\n============================================================\nSTARTING FedSAM TRAINING\n============================================================\n[FedSAM] R1/50 Test Acc: 0.2826 | Time: 15.41s\n[FedSAM] R2/50 Test Acc: 0.3922 | Time: 14.34s\n[FedSAM] R3/50 Test Acc: 0.4459 | Time: 14.45s\n[FedSAM] R4/50 Test Acc: 0.4879 | Time: 14.64s\n[FedSAM] R5/50 Test Acc: 0.5131 | Time: 14.67s\n[FedSAM] R6/50 Test Acc: 0.5323 | Time: 14.75s\n[FedSAM] R7/50 Test Acc: 0.5500 | Time: 15.04s\n[FedSAM] R8/50 Test Acc: 0.5627 | Time: 15.20s\n[FedSAM] R9/50 Test Acc: 0.5788 | Time: 15.26s\n[FedSAM] R10/50 Test Acc: 0.5944 | Time: 15.01s\n[FedSAM] R11/50 Test Acc: 0.5940 | Time: 15.19s\n[FedSAM] R12/50 Test Acc: 0.5996 | Time: 14.93s\n[FedSAM] R13/50 Test Acc: 0.6087 | Time: 14.84s\n[FedSAM] R14/50 Test Acc: 0.6184 | Time: 14.94s\n[FedSAM] R15/50 Test Acc: 0.6175 | Time: 15.09s\n[FedSAM] R16/50 Test Acc: 0.6229 | Time: 15.11s\n[FedSAM] R17/50 Test Acc: 0.6274 | Time: 15.21s\n[FedSAM] R18/50 Test Acc: 0.6315 | Time: 14.98s\n[FedSAM] R19/50 Test Acc: 0.6336 | Time: 14.88s\n[FedSAM] R20/50 Test Acc: 0.6325 | Time: 14.97s\n[FedSAM] R21/50 Test Acc: 0.6344 | Time: 14.96s\n[FedSAM] R22/50 Test Acc: 0.6370 | Time: 15.13s\n[FedSAM] R23/50 Test Acc: 0.6363 | Time: 14.96s\n[FedSAM] R24/50 Test Acc: 0.6387 | Time: 15.01s\n[FedSAM] R25/50 Test Acc: 0.6345 | Time: 14.99s\n[FedSAM] R26/50 Test Acc: 0.6424 | Time: 15.04s\n[FedSAM] R27/50 Test Acc: 0.6447 | Time: 14.91s\n[FedSAM] R28/50 Test Acc: 0.6435 | Time: 14.97s\n[FedSAM] R29/50 Test Acc: 0.6426 | Time: 15.05s\n[FedSAM] R30/50 Test Acc: 0.6412 | Time: 15.07s\n[FedSAM] R31/50 Test Acc: 0.6491 | Time: 15.12s\n[FedSAM] R32/50 Test Acc: 0.6454 | Time: 15.23s\n[FedSAM] R33/50 Test Acc: 0.6457 | Time: 15.16s\n[FedSAM] R34/50 Test Acc: 0.6514 | Time: 15.00s\n[FedSAM] R35/50 Test Acc: 0.6528 | Time: 14.93s\n[FedSAM] R36/50 Test Acc: 0.6462 | Time: 15.04s\n[FedSAM] R37/50 Test Acc: 0.6396 | Time: 14.89s\n[FedSAM] R38/50 Test Acc: 0.6454 | Time: 14.99s\n[FedSAM] R39/50 Test Acc: 0.6466 | Time: 15.07s\n[FedSAM] R40/50 Test Acc: 0.6496 | Time: 14.98s\n[FedSAM] R41/50 Test Acc: 0.6505 | Time: 14.93s\n[FedSAM] R42/50 Test Acc: 0.6516 | Time: 15.05s\n[FedSAM] R43/50 Test Acc: 0.6522 | Time: 15.05s\n[FedSAM] R44/50 Test Acc: 0.6507 | Time: 15.13s\n[FedSAM] R45/50 Test Acc: 0.6504 | Time: 14.98s\n[FedSAM] R46/50 Test Acc: 0.6501 | Time: 14.94s\n[FedSAM] R47/50 Test Acc: 0.6521 | Time: 14.95s\n[FedSAM] R48/50 Test Acc: 0.6515 | Time: 15.23s\n[FedSAM] R49/50 Test Acc: 0.6550 | Time: 15.10s\n[FedSAM] R50/50 Test Acc: 0.6507 | Time: 14.96s\n\n[8/12] FedSAM+Temp...\n\n============================================================\nSTARTING FedSAM + TEMP TRAINING\n============================================================\n[FedSAM+Temp] R1/50 Test Acc: 0.2879 | Avg τ: 0.9677 | Time: 15.22s\n[FedSAM+Temp] R2/50 Test Acc: 0.3727 | Avg τ: 0.8141 | Time: 15.33s\n[FedSAM+Temp] R3/50 Test Acc: 0.4403 | Avg τ: 0.6889 | Time: 15.35s\n[FedSAM+Temp] R4/50 Test Acc: 0.4766 | Avg τ: 0.6313 | Time: 15.43s\n[FedSAM+Temp] R5/50 Test Acc: 0.5106 | Avg τ: 0.5947 | Time: 15.36s\n[FedSAM+Temp] R6/50 Test Acc: 0.5318 | Avg τ: 0.5454 | Time: 15.38s\n[FedSAM+Temp] R7/50 Test Acc: 0.5527 | Avg τ: 0.5277 | Time: 15.15s\n[FedSAM+Temp] R8/50 Test Acc: 0.5661 | Avg τ: 0.4938 | Time: 15.43s\n[FedSAM+Temp] R9/50 Test Acc: 0.5760 | Avg τ: 0.4888 | Time: 15.28s\n[FedSAM+Temp] R10/50 Test Acc: 0.5910 | Avg τ: 0.4917 | Time: 15.33s\n[FedSAM+Temp] R11/50 Test Acc: 0.5920 | Avg τ: 0.4721 | Time: 15.35s\n[FedSAM+Temp] R12/50 Test Acc: 0.6025 | Avg τ: 0.4749 | Time: 15.22s\n[FedSAM+Temp] R13/50 Test Acc: 0.6029 | Avg τ: 0.4760 | Time: 15.22s\n[FedSAM+Temp] R14/50 Test Acc: 0.6093 | Avg τ: 0.4733 | Time: 15.31s\n[FedSAM+Temp] R15/50 Test Acc: 0.6165 | Avg τ: 0.4688 | Time: 15.35s\n[FedSAM+Temp] R16/50 Test Acc: 0.6185 | Avg τ: 0.4777 | Time: 15.30s\n[FedSAM+Temp] R17/50 Test Acc: 0.6204 | Avg τ: 0.4713 | Time: 15.33s\n[FedSAM+Temp] R18/50 Test Acc: 0.6241 | Avg τ: 0.4668 | Time: 15.40s\n[FedSAM+Temp] R19/50 Test Acc: 0.6265 | Avg τ: 0.4826 | Time: 15.37s\n[FedSAM+Temp] R20/50 Test Acc: 0.6323 | Avg τ: 0.4822 | Time: 15.42s\n[FedSAM+Temp] R21/50 Test Acc: 0.6301 | Avg τ: 0.4731 | Time: 15.22s\n[FedSAM+Temp] R22/50 Test Acc: 0.6321 | Avg τ: 0.4663 | Time: 15.27s\n[FedSAM+Temp] R23/50 Test Acc: 0.6367 | Avg τ: 0.4773 | Time: 15.53s\n[FedSAM+Temp] R24/50 Test Acc: 0.6408 | Avg τ: 0.4746 | Time: 15.14s\n[FedSAM+Temp] R25/50 Test Acc: 0.6410 | Avg τ: 0.4755 | Time: 15.40s\n[FedSAM+Temp] R26/50 Test Acc: 0.6405 | Avg τ: 0.4751 | Time: 15.27s\n[FedSAM+Temp] R27/50 Test Acc: 0.6405 | Avg τ: 0.4757 | Time: 15.18s\n[FedSAM+Temp] R28/50 Test Acc: 0.6393 | Avg τ: 0.4682 | Time: 15.49s\n[FedSAM+Temp] R29/50 Test Acc: 0.6386 | Avg τ: 0.4689 | Time: 15.33s\n[FedSAM+Temp] R30/50 Test Acc: 0.6446 | Avg τ: 0.4720 | Time: 15.29s\n[FedSAM+Temp] R31/50 Test Acc: 0.6510 | Avg τ: 0.4793 | Time: 15.49s\n[FedSAM+Temp] R32/50 Test Acc: 0.6426 | Avg τ: 0.4746 | Time: 15.34s\n[FedSAM+Temp] R33/50 Test Acc: 0.6468 | Avg τ: 0.4754 | Time: 15.26s\n[FedSAM+Temp] R34/50 Test Acc: 0.6507 | Avg τ: 0.4678 | Time: 15.23s\n[FedSAM+Temp] R35/50 Test Acc: 0.6441 | Avg τ: 0.4619 | Time: 15.25s\n[FedSAM+Temp] R36/50 Test Acc: 0.6477 | Avg τ: 0.4748 | Time: 15.51s\n[FedSAM+Temp] R37/50 Test Acc: 0.6481 | Avg τ: 0.4742 | Time: 15.30s\n[FedSAM+Temp] R38/50 Test Acc: 0.6517 | Avg τ: 0.4689 | Time: 15.47s\n[FedSAM+Temp] R39/50 Test Acc: 0.6509 | Avg τ: 0.4680 | Time: 15.38s\n[FedSAM+Temp] R40/50 Test Acc: 0.6494 | Avg τ: 0.4668 | Time: 15.47s\n[FedSAM+Temp] R41/50 Test Acc: 0.6498 | Avg τ: 0.4722 | Time: 15.43s\n[FedSAM+Temp] R42/50 Test Acc: 0.6507 | Avg τ: 0.4696 | Time: 15.16s\n[FedSAM+Temp] R43/50 Test Acc: 0.6521 | Avg τ: 0.4682 | Time: 15.26s\n[FedSAM+Temp] R44/50 Test Acc: 0.6520 | Avg τ: 0.4589 | Time: 15.36s\n[FedSAM+Temp] R45/50 Test Acc: 0.6522 | Avg τ: 0.4599 | Time: 15.30s\n[FedSAM+Temp] R46/50 Test Acc: 0.6493 | Avg τ: 0.4717 | Time: 15.46s\n[FedSAM+Temp] R47/50 Test Acc: 0.6544 | Avg τ: 0.4664 | Time: 15.30s\n[FedSAM+Temp] R48/50 Test Acc: 0.6531 | Avg τ: 0.4742 | Time: 15.13s\n[FedSAM+Temp] R49/50 Test Acc: 0.6513 | Avg τ: 0.4631 | Time: 15.42s\n[FedSAM+Temp] R50/50 Test Acc: 0.6549 | Avg τ: 0.4646 | Time: 15.25s\n","output_type":"stream"}],"execution_count":5}]}